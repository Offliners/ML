{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAYZVDqSpSzHDztoGVQ/pA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Offliners/NTUML2021_Hung-yi-Lee/blob/main/HW1/homework1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJeAxxvOw4zo"
      },
      "source": [
        "# **Homework 1: COVID-19 Cases Prediction (Regression)**\r\n",
        "\r\n",
        "Source: Delphi group @ CMU : A daily survey since April 2020 via facebook.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItOHwIrDxX6F"
      },
      "source": [
        "## **Download Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaLzrpaLw3uM",
        "outputId": "4bf657bb-2b5e-451b-fdf1-c409e1256a8f"
      },
      "source": [
        "tr_path = 'covid.train.csv'  # path to training data\r\n",
        "tt_path = 'covid.test.csv'   # path to testing data\r\n",
        "\r\n",
        "!gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\r\n",
        "!gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF\n",
            "To: /content/covid.train.csv\n",
            "100% 2.00M/2.00M [00:00<00:00, 31.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O\n",
            "To: /content/covid.test.csv\n",
            "100% 651k/651k [00:00<00:00, 10.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wupnvWGaxsQ3"
      },
      "source": [
        "## **Import Some Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBmUSABexxeZ"
      },
      "source": [
        "# PyTorch\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "# For data preprocess\r\n",
        "import numpy as np\r\n",
        "import csv\r\n",
        "import os\r\n",
        "\r\n",
        "# For plotting\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib.pyplot import figure\r\n",
        "\r\n",
        "myseed = 42069  # set a random seed for reproducibility\r\n",
        "torch.backends.cudnn.deterministic = True\r\n",
        "torch.backends.cudnn.benchmark = False\r\n",
        "np.random.seed(myseed)\r\n",
        "torch.manual_seed(myseed)\r\n",
        "if torch.cuda.is_available():\r\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg_EItEAx1gZ"
      },
      "source": [
        "## **Some Utilities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3jSevQYx47x"
      },
      "source": [
        "def get_device():\r\n",
        "    ''' Get device (if GPU is available, use GPU) '''\r\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "\r\n",
        "def plot_learning_curve(loss_record, title=''):\r\n",
        "    ''' Plot learning curve of your DNN (train & dev loss) '''\r\n",
        "    total_steps = len(loss_record['train'])\r\n",
        "    x_1 = range(total_steps)\r\n",
        "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\r\n",
        "    figure(figsize=(6, 4))\r\n",
        "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\r\n",
        "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\r\n",
        "    plt.ylim(0.0, 5.)\r\n",
        "    plt.xlabel('Training steps')\r\n",
        "    plt.ylabel('MSE loss')\r\n",
        "    plt.title('Learning curve of {}'.format(title))\r\n",
        "    plt.legend()\r\n",
        "    plt.grid()\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\r\n",
        "    ''' Plot prediction of your DNN '''\r\n",
        "    if preds is None or targets is None:\r\n",
        "        model.eval()\r\n",
        "        preds, targets = [], []\r\n",
        "        for x, y in dv_set:\r\n",
        "            x, y = x.to(device), y.to(device)\r\n",
        "            with torch.no_grad():\r\n",
        "                pred = model(x)\r\n",
        "                preds.append(pred.detach().cpu())\r\n",
        "                targets.append(y.detach().cpu())\r\n",
        "        preds = torch.cat(preds, dim=0).numpy()\r\n",
        "        targets = torch.cat(targets, dim=0).numpy()\r\n",
        "\r\n",
        "    figure(figsize=(5, 5))\r\n",
        "    plt.scatter(targets, preds, c='r', alpha=0.5)\r\n",
        "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\r\n",
        "    plt.xlim(-0.2, lim)\r\n",
        "    plt.ylim(-0.2, lim)\r\n",
        "    plt.xlabel('ground truth value')\r\n",
        "    plt.ylabel('predicted value')\r\n",
        "    plt.title('Ground Truth v.s. Prediction')\r\n",
        "    plt.grid()\r\n",
        "    plt.show()"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_bXJgTgx8xt"
      },
      "source": [
        "## **Preprocess**\r\n",
        "\r\n",
        "We have three kinds of datasets:\r\n",
        "* `train`: for training\r\n",
        "* `dev`: for validation\r\n",
        "* `test`: for testing (w/o target value)\r\n",
        "\r\n",
        "## **Dataset**\r\n",
        "\r\n",
        "The `COVID19Dataset` below does:\r\n",
        "* read `.csv` files\r\n",
        "* extract features\r\n",
        "* split `covid.train.csv` into train/dev sets\r\n",
        "* normalize features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEQpQAdNyKU9"
      },
      "source": [
        "class COVID19Dataset(Dataset):\r\n",
        "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\r\n",
        "    def __init__(self,\r\n",
        "                 path,\r\n",
        "                 mode='train',\r\n",
        "                 target_only=False):\r\n",
        "        self.mode = mode\r\n",
        "\r\n",
        "        # Read data into numpy arrays\r\n",
        "        with open(path, 'r') as fp:\r\n",
        "            data = list(csv.reader(fp))\r\n",
        "            data = np.array(data[1:])[:, 1:].astype(float)\r\n",
        "        \r\n",
        "        if not target_only:\r\n",
        "            feats = list(range(1, 93))\r\n",
        "        else:\r\n",
        "            # TODO: Using 40 states & 2 tested_positive features (indices = 57 & 75)\r\n",
        "            feats = list(range(40, 93))\r\n",
        "            pass\r\n",
        "\r\n",
        "        if mode == 'test':\r\n",
        "            # Testing data\r\n",
        "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\r\n",
        "            data = data[:, feats]\r\n",
        "            self.data = torch.FloatTensor(data)\r\n",
        "        else:\r\n",
        "            # Training data (train/dev sets)\r\n",
        "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\r\n",
        "            target = data[:, -1]\r\n",
        "            data = data[:, feats]\r\n",
        "            \r\n",
        "            # Splitting training data into train & dev sets\r\n",
        "            if mode == 'train':\r\n",
        "                indices = [i for i in range(len(data)) if i % 10 != 0]\r\n",
        "            elif mode == 'dev':\r\n",
        "                indices = [i for i in range(len(data)) if i % 10 == 0]\r\n",
        "            \r\n",
        "            # Convert data into PyTorch tensors\r\n",
        "            self.data = torch.FloatTensor(data[indices])\r\n",
        "            self.target = torch.FloatTensor(target[indices])\r\n",
        "\r\n",
        "        # Normalize features (you may remove this part to see what will happen)\r\n",
        "        self.data[:, 40:] = \\\r\n",
        "            (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) \\\r\n",
        "            / self.data[:, 40:].std(dim=0, keepdim=True)\r\n",
        "\r\n",
        "        self.dim = self.data.shape[1]\r\n",
        "\r\n",
        "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'\r\n",
        "              .format(mode, len(self.data), self.dim))\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        # Returns one sample at a time\r\n",
        "        if self.mode in ['train', 'dev']:\r\n",
        "            # For training\r\n",
        "            return self.data[index], self.target[index]\r\n",
        "        else:\r\n",
        "            # For testing (no target)\r\n",
        "            return self.data[index]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        # Returns the size of the dataset\r\n",
        "        return len(self.data)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lohmDpmJyWQZ"
      },
      "source": [
        "## **DataLoader**\r\n",
        "\r\n",
        "A `DataLoader` loads data from a given `Dataset` into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOwjzVYiyY-Y"
      },
      "source": [
        "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\r\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\r\n",
        "    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  # Construct dataset\r\n",
        "    dataloader = DataLoader(\r\n",
        "        dataset, batch_size,\r\n",
        "        shuffle=(mode == 'train'), drop_last=False,\r\n",
        "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\r\n",
        "    return dataloader"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-deK1g2yZu6"
      },
      "source": [
        "## **Deep Neural Network**\r\n",
        "\r\n",
        "`NeuralNet` is an `nn.Module` designed for regression.\r\n",
        "The DNN consists of 2 fully-connected layers with ReLU activation.\r\n",
        "This module also included a function `cal_loss` for calculating loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg-zGj5Wyg85"
      },
      "source": [
        "class NeuralNet(nn.Module):\r\n",
        "    ''' A simple fully-connected deep neural network '''\r\n",
        "    def __init__(self, input_dim):\r\n",
        "        super(NeuralNet, self).__init__()\r\n",
        "\r\n",
        "        # Define your neural network here\r\n",
        "        # TODO: How to modify this model to achieve better performance?\r\n",
        "        self.net = nn.Sequential(\r\n",
        "            nn.Linear(input_dim, 1),\r\n",
        "        )\r\n",
        "\r\n",
        "        # Mean squared error loss\r\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\r\n",
        "        \r\n",
        "        return self.net(x).squeeze(1)\r\n",
        "\r\n",
        "    def cal_loss(self, pred, target):\r\n",
        "        ''' Calculate loss '''\r\n",
        "        # TODO: you may implement L2 regularization here\r\n",
        "\r\n",
        "        return self.criterion(pred, target)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu3iy3HeytMg"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg4f-Smbyu_k"
      },
      "source": [
        "def train(tr_set, dv_set, model, config, device):\r\n",
        "    ''' DNN training '''\r\n",
        "\r\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\r\n",
        "\r\n",
        "    # Setup optimizer\r\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\r\n",
        "        model.parameters(), **config['optim_hparas'])\r\n",
        "\r\n",
        "    min_mse = 1000.\r\n",
        "    loss_record = {'train': [], 'dev': []}      # for recording training loss\r\n",
        "    early_stop_cnt = 0\r\n",
        "    epoch = 0\r\n",
        "    while epoch < n_epochs:\r\n",
        "        model.train()                           # set model to training mode\r\n",
        "        for x, y in tr_set:                     # iterate through the dataloader\r\n",
        "            optimizer.zero_grad()               # set gradient to zero\r\n",
        "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\r\n",
        "            pred = model(x)                     # forward pass (compute output)\r\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\r\n",
        "            mse_loss.backward()                 # compute gradient (backpropagation)\r\n",
        "            optimizer.step()                    # update model with optimizer\r\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item())\r\n",
        "\r\n",
        "        # After each epoch, test your model on the validation (development) set.\r\n",
        "        dev_mse = dev(dv_set, model, device)\r\n",
        "        if dev_mse < min_mse:\r\n",
        "            # Save model if your model improved\r\n",
        "            min_mse = dev_mse\r\n",
        "            print('Saving model (epoch = {:4d}, loss = {:.4f})'\r\n",
        "                .format(epoch + 1, min_mse))\r\n",
        "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\r\n",
        "            early_stop_cnt = 0\r\n",
        "        else:\r\n",
        "            early_stop_cnt += 1\r\n",
        "\r\n",
        "        epoch += 1\r\n",
        "        loss_record['dev'].append(dev_mse)\r\n",
        "        if early_stop_cnt > config['early_stop']:\r\n",
        "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\r\n",
        "            break\r\n",
        "\r\n",
        "    print('Finished training after {} epochs'.format(epoch))\r\n",
        "    return min_mse, loss_record"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQzKQy8vyxIV"
      },
      "source": [
        "## **Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr2NrkEEyzox"
      },
      "source": [
        "def dev(dv_set, model, device):\r\n",
        "    model.eval()                                # set model to evalutation mode\r\n",
        "    total_loss = 0\r\n",
        "    for x, y in dv_set:                         # iterate through the dataloader\r\n",
        "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\r\n",
        "        with torch.no_grad():                   # disable gradient calculation\r\n",
        "            pred = model(x)                     # forward pass (compute output)\r\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\r\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\r\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\r\n",
        "\r\n",
        "    return total_loss"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-gEsuYZy1Vb"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz_VBVHAy3u2"
      },
      "source": [
        "def test(tt_set, model, device):\r\n",
        "    model.eval()                                # set model to evalutation mode\r\n",
        "    preds = []\r\n",
        "    for x in tt_set:                            # iterate through the dataloader\r\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\r\n",
        "        with torch.no_grad():                   # disable gradient calculation\r\n",
        "            pred = model(x)                     # forward pass (compute output)\r\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\r\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\r\n",
        "    return preds"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4Sos50my5RN"
      },
      "source": [
        "## **Setup Hyper-parameters**\r\n",
        "`config` contains hyper-parameters for training and the path to save your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvfemQHcy99g"
      },
      "source": [
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\r\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\r\n",
        "target_only = True                    # TODO: Using 40 states & 2 tested_positive features\r\n",
        "\r\n",
        "# TODO: How to tune these hyper-parameters to improve your model's performance?\r\n",
        "config = {\r\n",
        "    'n_epochs': 100000,               # maximum number of epochs\r\n",
        "    'batch_size': 64,               # mini-batch size for dataloader\r\n",
        "    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)\r\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\r\n",
        "        'lr': 0.000001,                 # learning rate of SGD\r\n",
        "        'momentum': 0.9,             # momentum for SGD\r\n",
        "        'weight_decay': 0.000001\r\n",
        "    },\r\n",
        "    'early_stop': 1000,               # early stopping epochs (the number epochs since your model's last improvement)\r\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\r\n",
        "}"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSZZjxd7y_3U"
      },
      "source": [
        "## **Load data and model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzFLuU50zEio",
        "outputId": "81300da1-a585-4395-b8d2-a6f6d301f18a"
      },
      "source": [
        "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\r\n",
        "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\r\n",
        "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)\r\n",
        "\r\n",
        "model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 53)\n",
            "Finished reading the dev set of COVID19 Dataset (270 samples found, each dim = 53)\n",
            "Finished reading the test set of COVID19 Dataset (893 samples found, each dim = 53)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-S5SrtbzJm5"
      },
      "source": [
        "## **Start Training!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxIA6nP-zO8B",
        "outputId": "3356d0cc-398a-4ba0-af69-020ebb084618"
      },
      "source": [
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model (epoch =    1, loss = 31.5836)\n",
            "Saving model (epoch =    2, loss = 18.5216)\n",
            "Saving model (epoch =    3, loss = 12.5553)\n",
            "Saving model (epoch =    4, loss = 10.5503)\n",
            "Saving model (epoch =    5, loss = 9.6152)\n",
            "Saving model (epoch =    6, loss = 9.2536)\n",
            "Saving model (epoch =    7, loss = 8.8947)\n",
            "Saving model (epoch =    8, loss = 8.3939)\n",
            "Saving model (epoch =    9, loss = 8.1597)\n",
            "Saving model (epoch =   10, loss = 7.8083)\n",
            "Saving model (epoch =   11, loss = 7.5726)\n",
            "Saving model (epoch =   12, loss = 7.3739)\n",
            "Saving model (epoch =   13, loss = 7.1639)\n",
            "Saving model (epoch =   14, loss = 7.0039)\n",
            "Saving model (epoch =   15, loss = 6.8057)\n",
            "Saving model (epoch =   16, loss = 6.6552)\n",
            "Saving model (epoch =   17, loss = 6.5583)\n",
            "Saving model (epoch =   18, loss = 6.2784)\n",
            "Saving model (epoch =   19, loss = 6.0526)\n",
            "Saving model (epoch =   20, loss = 5.8825)\n",
            "Saving model (epoch =   21, loss = 5.7489)\n",
            "Saving model (epoch =   22, loss = 5.6823)\n",
            "Saving model (epoch =   23, loss = 5.4479)\n",
            "Saving model (epoch =   24, loss = 5.3288)\n",
            "Saving model (epoch =   25, loss = 5.1845)\n",
            "Saving model (epoch =   26, loss = 5.1762)\n",
            "Saving model (epoch =   27, loss = 5.0743)\n",
            "Saving model (epoch =   28, loss = 4.8061)\n",
            "Saving model (epoch =   29, loss = 4.7161)\n",
            "Saving model (epoch =   30, loss = 4.5784)\n",
            "Saving model (epoch =   32, loss = 4.5104)\n",
            "Saving model (epoch =   33, loss = 4.3820)\n",
            "Saving model (epoch =   34, loss = 4.1896)\n",
            "Saving model (epoch =   36, loss = 4.0409)\n",
            "Saving model (epoch =   37, loss = 3.9172)\n",
            "Saving model (epoch =   38, loss = 3.9006)\n",
            "Saving model (epoch =   39, loss = 3.7802)\n",
            "Saving model (epoch =   40, loss = 3.6792)\n",
            "Saving model (epoch =   41, loss = 3.6362)\n",
            "Saving model (epoch =   42, loss = 3.5760)\n",
            "Saving model (epoch =   43, loss = 3.4881)\n",
            "Saving model (epoch =   44, loss = 3.4632)\n",
            "Saving model (epoch =   45, loss = 3.3536)\n",
            "Saving model (epoch =   46, loss = 3.2734)\n",
            "Saving model (epoch =   47, loss = 3.2233)\n",
            "Saving model (epoch =   48, loss = 3.1781)\n",
            "Saving model (epoch =   49, loss = 3.1353)\n",
            "Saving model (epoch =   50, loss = 3.1183)\n",
            "Saving model (epoch =   51, loss = 3.0102)\n",
            "Saving model (epoch =   52, loss = 2.9483)\n",
            "Saving model (epoch =   53, loss = 2.9188)\n",
            "Saving model (epoch =   55, loss = 2.8323)\n",
            "Saving model (epoch =   56, loss = 2.7689)\n",
            "Saving model (epoch =   57, loss = 2.7193)\n",
            "Saving model (epoch =   58, loss = 2.7093)\n",
            "Saving model (epoch =   59, loss = 2.6394)\n",
            "Saving model (epoch =   60, loss = 2.6077)\n",
            "Saving model (epoch =   61, loss = 2.5705)\n",
            "Saving model (epoch =   62, loss = 2.5290)\n",
            "Saving model (epoch =   63, loss = 2.4946)\n",
            "Saving model (epoch =   65, loss = 2.4281)\n",
            "Saving model (epoch =   66, loss = 2.3949)\n",
            "Saving model (epoch =   67, loss = 2.3701)\n",
            "Saving model (epoch =   68, loss = 2.3346)\n",
            "Saving model (epoch =   69, loss = 2.3332)\n",
            "Saving model (epoch =   70, loss = 2.2870)\n",
            "Saving model (epoch =   72, loss = 2.2421)\n",
            "Saving model (epoch =   73, loss = 2.2188)\n",
            "Saving model (epoch =   74, loss = 2.1985)\n",
            "Saving model (epoch =   78, loss = 2.1296)\n",
            "Saving model (epoch =   79, loss = 2.0831)\n",
            "Saving model (epoch =   80, loss = 2.0574)\n",
            "Saving model (epoch =   82, loss = 2.0498)\n",
            "Saving model (epoch =   83, loss = 2.0187)\n",
            "Saving model (epoch =   85, loss = 1.9699)\n",
            "Saving model (epoch =   87, loss = 1.9635)\n",
            "Saving model (epoch =   88, loss = 1.9130)\n",
            "Saving model (epoch =   89, loss = 1.8852)\n",
            "Saving model (epoch =   90, loss = 1.8785)\n",
            "Saving model (epoch =   91, loss = 1.8554)\n",
            "Saving model (epoch =   92, loss = 1.8414)\n",
            "Saving model (epoch =   94, loss = 1.8398)\n",
            "Saving model (epoch =   95, loss = 1.8037)\n",
            "Saving model (epoch =   96, loss = 1.8010)\n",
            "Saving model (epoch =   97, loss = 1.7986)\n",
            "Saving model (epoch =   99, loss = 1.7508)\n",
            "Saving model (epoch =  101, loss = 1.7307)\n",
            "Saving model (epoch =  102, loss = 1.7197)\n",
            "Saving model (epoch =  103, loss = 1.7103)\n",
            "Saving model (epoch =  104, loss = 1.6973)\n",
            "Saving model (epoch =  105, loss = 1.6927)\n",
            "Saving model (epoch =  106, loss = 1.6789)\n",
            "Saving model (epoch =  109, loss = 1.6509)\n",
            "Saving model (epoch =  110, loss = 1.6448)\n",
            "Saving model (epoch =  111, loss = 1.6381)\n",
            "Saving model (epoch =  114, loss = 1.6322)\n",
            "Saving model (epoch =  116, loss = 1.6046)\n",
            "Saving model (epoch =  118, loss = 1.5814)\n",
            "Saving model (epoch =  121, loss = 1.5741)\n",
            "Saving model (epoch =  122, loss = 1.5485)\n",
            "Saving model (epoch =  125, loss = 1.5305)\n",
            "Saving model (epoch =  129, loss = 1.5065)\n",
            "Saving model (epoch =  130, loss = 1.5057)\n",
            "Saving model (epoch =  131, loss = 1.4991)\n",
            "Saving model (epoch =  133, loss = 1.4862)\n",
            "Saving model (epoch =  135, loss = 1.4857)\n",
            "Saving model (epoch =  137, loss = 1.4839)\n",
            "Saving model (epoch =  140, loss = 1.4538)\n",
            "Saving model (epoch =  142, loss = 1.4433)\n",
            "Saving model (epoch =  143, loss = 1.4412)\n",
            "Saving model (epoch =  145, loss = 1.4320)\n",
            "Saving model (epoch =  146, loss = 1.4277)\n",
            "Saving model (epoch =  148, loss = 1.4262)\n",
            "Saving model (epoch =  149, loss = 1.4237)\n",
            "Saving model (epoch =  151, loss = 1.4109)\n",
            "Saving model (epoch =  152, loss = 1.4063)\n",
            "Saving model (epoch =  153, loss = 1.4033)\n",
            "Saving model (epoch =  154, loss = 1.3985)\n",
            "Saving model (epoch =  155, loss = 1.3952)\n",
            "Saving model (epoch =  158, loss = 1.3949)\n",
            "Saving model (epoch =  160, loss = 1.3803)\n",
            "Saving model (epoch =  162, loss = 1.3784)\n",
            "Saving model (epoch =  168, loss = 1.3574)\n",
            "Saving model (epoch =  173, loss = 1.3527)\n",
            "Saving model (epoch =  174, loss = 1.3422)\n",
            "Saving model (epoch =  175, loss = 1.3408)\n",
            "Saving model (epoch =  176, loss = 1.3385)\n",
            "Saving model (epoch =  185, loss = 1.3306)\n",
            "Saving model (epoch =  189, loss = 1.3148)\n",
            "Saving model (epoch =  194, loss = 1.3137)\n",
            "Saving model (epoch =  198, loss = 1.2959)\n",
            "Saving model (epoch =  199, loss = 1.2948)\n",
            "Saving model (epoch =  200, loss = 1.2940)\n",
            "Saving model (epoch =  202, loss = 1.2899)\n",
            "Saving model (epoch =  206, loss = 1.2875)\n",
            "Saving model (epoch =  207, loss = 1.2871)\n",
            "Saving model (epoch =  214, loss = 1.2727)\n",
            "Saving model (epoch =  222, loss = 1.2705)\n",
            "Saving model (epoch =  224, loss = 1.2703)\n",
            "Saving model (epoch =  228, loss = 1.2685)\n",
            "Saving model (epoch =  230, loss = 1.2614)\n",
            "Saving model (epoch =  234, loss = 1.2544)\n",
            "Saving model (epoch =  238, loss = 1.2544)\n",
            "Saving model (epoch =  239, loss = 1.2450)\n",
            "Saving model (epoch =  246, loss = 1.2408)\n",
            "Saving model (epoch =  247, loss = 1.2375)\n",
            "Saving model (epoch =  257, loss = 1.2344)\n",
            "Saving model (epoch =  261, loss = 1.2325)\n",
            "Saving model (epoch =  265, loss = 1.2256)\n",
            "Saving model (epoch =  267, loss = 1.2219)\n",
            "Saving model (epoch =  273, loss = 1.2181)\n",
            "Saving model (epoch =  277, loss = 1.2143)\n",
            "Saving model (epoch =  281, loss = 1.2127)\n",
            "Saving model (epoch =  282, loss = 1.2108)\n",
            "Saving model (epoch =  291, loss = 1.2102)\n",
            "Saving model (epoch =  293, loss = 1.2045)\n",
            "Saving model (epoch =  296, loss = 1.2036)\n",
            "Saving model (epoch =  303, loss = 1.2024)\n",
            "Saving model (epoch =  309, loss = 1.2019)\n",
            "Saving model (epoch =  310, loss = 1.1944)\n",
            "Saving model (epoch =  312, loss = 1.1935)\n",
            "Saving model (epoch =  314, loss = 1.1924)\n",
            "Saving model (epoch =  317, loss = 1.1918)\n",
            "Saving model (epoch =  327, loss = 1.1916)\n",
            "Saving model (epoch =  331, loss = 1.1915)\n",
            "Saving model (epoch =  334, loss = 1.1855)\n",
            "Saving model (epoch =  335, loss = 1.1823)\n",
            "Saving model (epoch =  348, loss = 1.1786)\n",
            "Saving model (epoch =  349, loss = 1.1767)\n",
            "Saving model (epoch =  354, loss = 1.1756)\n",
            "Saving model (epoch =  359, loss = 1.1718)\n",
            "Saving model (epoch =  368, loss = 1.1687)\n",
            "Saving model (epoch =  370, loss = 1.1680)\n",
            "Saving model (epoch =  380, loss = 1.1648)\n",
            "Saving model (epoch =  383, loss = 1.1629)\n",
            "Saving model (epoch =  384, loss = 1.1627)\n",
            "Saving model (epoch =  390, loss = 1.1618)\n",
            "Saving model (epoch =  400, loss = 1.1592)\n",
            "Saving model (epoch =  401, loss = 1.1584)\n",
            "Saving model (epoch =  407, loss = 1.1582)\n",
            "Saving model (epoch =  408, loss = 1.1581)\n",
            "Saving model (epoch =  409, loss = 1.1541)\n",
            "Saving model (epoch =  415, loss = 1.1534)\n",
            "Saving model (epoch =  421, loss = 1.1505)\n",
            "Saving model (epoch =  439, loss = 1.1465)\n",
            "Saving model (epoch =  443, loss = 1.1435)\n",
            "Saving model (epoch =  453, loss = 1.1409)\n",
            "Saving model (epoch =  465, loss = 1.1376)\n",
            "Saving model (epoch =  479, loss = 1.1336)\n",
            "Saving model (epoch =  489, loss = 1.1307)\n",
            "Saving model (epoch =  494, loss = 1.1298)\n",
            "Saving model (epoch =  512, loss = 1.1297)\n",
            "Saving model (epoch =  513, loss = 1.1255)\n",
            "Saving model (epoch =  516, loss = 1.1240)\n",
            "Saving model (epoch =  518, loss = 1.1239)\n",
            "Saving model (epoch =  523, loss = 1.1234)\n",
            "Saving model (epoch =  526, loss = 1.1213)\n",
            "Saving model (epoch =  537, loss = 1.1188)\n",
            "Saving model (epoch =  541, loss = 1.1179)\n",
            "Saving model (epoch =  550, loss = 1.1167)\n",
            "Saving model (epoch =  560, loss = 1.1134)\n",
            "Saving model (epoch =  562, loss = 1.1133)\n",
            "Saving model (epoch =  564, loss = 1.1128)\n",
            "Saving model (epoch =  578, loss = 1.1104)\n",
            "Saving model (epoch =  583, loss = 1.1084)\n",
            "Saving model (epoch =  596, loss = 1.1059)\n",
            "Saving model (epoch =  600, loss = 1.1047)\n",
            "Saving model (epoch =  609, loss = 1.1026)\n",
            "Saving model (epoch =  615, loss = 1.1014)\n",
            "Saving model (epoch =  622, loss = 1.1001)\n",
            "Saving model (epoch =  630, loss = 1.0993)\n",
            "Saving model (epoch =  640, loss = 1.0967)\n",
            "Saving model (epoch =  648, loss = 1.0949)\n",
            "Saving model (epoch =  658, loss = 1.0935)\n",
            "Saving model (epoch =  673, loss = 1.0893)\n",
            "Saving model (epoch =  679, loss = 1.0887)\n",
            "Saving model (epoch =  682, loss = 1.0876)\n",
            "Saving model (epoch =  707, loss = 1.0860)\n",
            "Saving model (epoch =  717, loss = 1.0855)\n",
            "Saving model (epoch =  724, loss = 1.0823)\n",
            "Saving model (epoch =  732, loss = 1.0801)\n",
            "Saving model (epoch =  742, loss = 1.0787)\n",
            "Saving model (epoch =  749, loss = 1.0756)\n",
            "Saving model (epoch =  758, loss = 1.0734)\n",
            "Saving model (epoch =  763, loss = 1.0728)\n",
            "Saving model (epoch =  774, loss = 1.0709)\n",
            "Saving model (epoch =  808, loss = 1.0644)\n",
            "Saving model (epoch =  820, loss = 1.0625)\n",
            "Saving model (epoch =  827, loss = 1.0614)\n",
            "Saving model (epoch =  831, loss = 1.0610)\n",
            "Saving model (epoch =  841, loss = 1.0596)\n",
            "Saving model (epoch =  848, loss = 1.0579)\n",
            "Saving model (epoch =  862, loss = 1.0562)\n",
            "Saving model (epoch =  868, loss = 1.0554)\n",
            "Saving model (epoch =  888, loss = 1.0522)\n",
            "Saving model (epoch =  892, loss = 1.0505)\n",
            "Saving model (epoch =  899, loss = 1.0501)\n",
            "Saving model (epoch =  917, loss = 1.0477)\n",
            "Saving model (epoch =  934, loss = 1.0446)\n",
            "Saving model (epoch =  937, loss = 1.0446)\n",
            "Saving model (epoch =  940, loss = 1.0442)\n",
            "Saving model (epoch =  943, loss = 1.0425)\n",
            "Saving model (epoch =  947, loss = 1.0424)\n",
            "Saving model (epoch =  956, loss = 1.0414)\n",
            "Saving model (epoch =  964, loss = 1.0408)\n",
            "Saving model (epoch =  971, loss = 1.0395)\n",
            "Saving model (epoch =  974, loss = 1.0395)\n",
            "Saving model (epoch =  975, loss = 1.0382)\n",
            "Saving model (epoch =  984, loss = 1.0361)\n",
            "Saving model (epoch =  998, loss = 1.0356)\n",
            "Saving model (epoch = 1004, loss = 1.0335)\n",
            "Saving model (epoch = 1010, loss = 1.0334)\n",
            "Saving model (epoch = 1022, loss = 1.0330)\n",
            "Saving model (epoch = 1027, loss = 1.0302)\n",
            "Saving model (epoch = 1035, loss = 1.0295)\n",
            "Saving model (epoch = 1050, loss = 1.0269)\n",
            "Saving model (epoch = 1064, loss = 1.0262)\n",
            "Saving model (epoch = 1065, loss = 1.0251)\n",
            "Saving model (epoch = 1072, loss = 1.0241)\n",
            "Saving model (epoch = 1074, loss = 1.0237)\n",
            "Saving model (epoch = 1083, loss = 1.0229)\n",
            "Saving model (epoch = 1101, loss = 1.0223)\n",
            "Saving model (epoch = 1104, loss = 1.0195)\n",
            "Saving model (epoch = 1112, loss = 1.0188)\n",
            "Saving model (epoch = 1114, loss = 1.0184)\n",
            "Saving model (epoch = 1122, loss = 1.0177)\n",
            "Saving model (epoch = 1132, loss = 1.0157)\n",
            "Saving model (epoch = 1136, loss = 1.0156)\n",
            "Saving model (epoch = 1149, loss = 1.0138)\n",
            "Saving model (epoch = 1154, loss = 1.0128)\n",
            "Saving model (epoch = 1169, loss = 1.0116)\n",
            "Saving model (epoch = 1173, loss = 1.0108)\n",
            "Saving model (epoch = 1185, loss = 1.0088)\n",
            "Saving model (epoch = 1194, loss = 1.0075)\n",
            "Saving model (epoch = 1203, loss = 1.0070)\n",
            "Saving model (epoch = 1219, loss = 1.0055)\n",
            "Saving model (epoch = 1226, loss = 1.0040)\n",
            "Saving model (epoch = 1227, loss = 1.0038)\n",
            "Saving model (epoch = 1232, loss = 1.0034)\n",
            "Saving model (epoch = 1236, loss = 1.0024)\n",
            "Saving model (epoch = 1241, loss = 1.0017)\n",
            "Saving model (epoch = 1251, loss = 1.0011)\n",
            "Saving model (epoch = 1269, loss = 0.9984)\n",
            "Saving model (epoch = 1274, loss = 0.9981)\n",
            "Saving model (epoch = 1289, loss = 0.9961)\n",
            "Saving model (epoch = 1290, loss = 0.9959)\n",
            "Saving model (epoch = 1300, loss = 0.9952)\n",
            "Saving model (epoch = 1303, loss = 0.9951)\n",
            "Saving model (epoch = 1319, loss = 0.9925)\n",
            "Saving model (epoch = 1344, loss = 0.9922)\n",
            "Saving model (epoch = 1346, loss = 0.9902)\n",
            "Saving model (epoch = 1354, loss = 0.9893)\n",
            "Saving model (epoch = 1368, loss = 0.9873)\n",
            "Saving model (epoch = 1373, loss = 0.9872)\n",
            "Saving model (epoch = 1383, loss = 0.9861)\n",
            "Saving model (epoch = 1396, loss = 0.9843)\n",
            "Saving model (epoch = 1413, loss = 0.9826)\n",
            "Saving model (epoch = 1429, loss = 0.9812)\n",
            "Saving model (epoch = 1447, loss = 0.9787)\n",
            "Saving model (epoch = 1455, loss = 0.9785)\n",
            "Saving model (epoch = 1466, loss = 0.9769)\n",
            "Saving model (epoch = 1472, loss = 0.9762)\n",
            "Saving model (epoch = 1481, loss = 0.9757)\n",
            "Saving model (epoch = 1485, loss = 0.9748)\n",
            "Saving model (epoch = 1486, loss = 0.9747)\n",
            "Saving model (epoch = 1500, loss = 0.9739)\n",
            "Saving model (epoch = 1509, loss = 0.9722)\n",
            "Saving model (epoch = 1519, loss = 0.9712)\n",
            "Saving model (epoch = 1526, loss = 0.9705)\n",
            "Saving model (epoch = 1533, loss = 0.9701)\n",
            "Saving model (epoch = 1553, loss = 0.9691)\n",
            "Saving model (epoch = 1554, loss = 0.9676)\n",
            "Saving model (epoch = 1563, loss = 0.9669)\n",
            "Saving model (epoch = 1569, loss = 0.9668)\n",
            "Saving model (epoch = 1575, loss = 0.9655)\n",
            "Saving model (epoch = 1596, loss = 0.9644)\n",
            "Saving model (epoch = 1601, loss = 0.9641)\n",
            "Saving model (epoch = 1602, loss = 0.9632)\n",
            "Saving model (epoch = 1615, loss = 0.9623)\n",
            "Saving model (epoch = 1632, loss = 0.9618)\n",
            "Saving model (epoch = 1640, loss = 0.9618)\n",
            "Saving model (epoch = 1642, loss = 0.9608)\n",
            "Saving model (epoch = 1650, loss = 0.9586)\n",
            "Saving model (epoch = 1654, loss = 0.9584)\n",
            "Saving model (epoch = 1674, loss = 0.9569)\n",
            "Saving model (epoch = 1698, loss = 0.9543)\n",
            "Saving model (epoch = 1702, loss = 0.9542)\n",
            "Saving model (epoch = 1703, loss = 0.9541)\n",
            "Saving model (epoch = 1705, loss = 0.9540)\n",
            "Saving model (epoch = 1726, loss = 0.9522)\n",
            "Saving model (epoch = 1751, loss = 0.9497)\n",
            "Saving model (epoch = 1758, loss = 0.9495)\n",
            "Saving model (epoch = 1780, loss = 0.9485)\n",
            "Saving model (epoch = 1781, loss = 0.9475)\n",
            "Saving model (epoch = 1797, loss = 0.9465)\n",
            "Saving model (epoch = 1809, loss = 0.9457)\n",
            "Saving model (epoch = 1817, loss = 0.9446)\n",
            "Saving model (epoch = 1818, loss = 0.9445)\n",
            "Saving model (epoch = 1825, loss = 0.9441)\n",
            "Saving model (epoch = 1834, loss = 0.9434)\n",
            "Saving model (epoch = 1847, loss = 0.9428)\n",
            "Saving model (epoch = 1855, loss = 0.9427)\n",
            "Saving model (epoch = 1858, loss = 0.9413)\n",
            "Saving model (epoch = 1864, loss = 0.9407)\n",
            "Saving model (epoch = 1873, loss = 0.9403)\n",
            "Saving model (epoch = 1880, loss = 0.9400)\n",
            "Saving model (epoch = 1881, loss = 0.9395)\n",
            "Saving model (epoch = 1895, loss = 0.9389)\n",
            "Saving model (epoch = 1918, loss = 0.9388)\n",
            "Saving model (epoch = 1925, loss = 0.9361)\n",
            "Saving model (epoch = 1952, loss = 0.9352)\n",
            "Saving model (epoch = 1969, loss = 0.9342)\n",
            "Saving model (epoch = 1973, loss = 0.9327)\n",
            "Saving model (epoch = 1983, loss = 0.9317)\n",
            "Saving model (epoch = 2000, loss = 0.9316)\n",
            "Saving model (epoch = 2002, loss = 0.9308)\n",
            "Saving model (epoch = 2012, loss = 0.9300)\n",
            "Saving model (epoch = 2019, loss = 0.9296)\n",
            "Saving model (epoch = 2024, loss = 0.9295)\n",
            "Saving model (epoch = 2025, loss = 0.9291)\n",
            "Saving model (epoch = 2028, loss = 0.9289)\n",
            "Saving model (epoch = 2034, loss = 0.9282)\n",
            "Saving model (epoch = 2056, loss = 0.9274)\n",
            "Saving model (epoch = 2060, loss = 0.9265)\n",
            "Saving model (epoch = 2070, loss = 0.9264)\n",
            "Saving model (epoch = 2073, loss = 0.9258)\n",
            "Saving model (epoch = 2087, loss = 0.9247)\n",
            "Saving model (epoch = 2105, loss = 0.9235)\n",
            "Saving model (epoch = 2117, loss = 0.9226)\n",
            "Saving model (epoch = 2146, loss = 0.9207)\n",
            "Saving model (epoch = 2169, loss = 0.9201)\n",
            "Saving model (epoch = 2177, loss = 0.9197)\n",
            "Saving model (epoch = 2192, loss = 0.9184)\n",
            "Saving model (epoch = 2205, loss = 0.9175)\n",
            "Saving model (epoch = 2213, loss = 0.9167)\n",
            "Saving model (epoch = 2218, loss = 0.9162)\n",
            "Saving model (epoch = 2237, loss = 0.9159)\n",
            "Saving model (epoch = 2255, loss = 0.9143)\n",
            "Saving model (epoch = 2265, loss = 0.9141)\n",
            "Saving model (epoch = 2267, loss = 0.9133)\n",
            "Saving model (epoch = 2279, loss = 0.9127)\n",
            "Saving model (epoch = 2282, loss = 0.9125)\n",
            "Saving model (epoch = 2286, loss = 0.9124)\n",
            "Saving model (epoch = 2300, loss = 0.9121)\n",
            "Saving model (epoch = 2306, loss = 0.9112)\n",
            "Saving model (epoch = 2316, loss = 0.9112)\n",
            "Saving model (epoch = 2320, loss = 0.9102)\n",
            "Saving model (epoch = 2346, loss = 0.9090)\n",
            "Saving model (epoch = 2353, loss = 0.9087)\n",
            "Saving model (epoch = 2356, loss = 0.9084)\n",
            "Saving model (epoch = 2362, loss = 0.9079)\n",
            "Saving model (epoch = 2364, loss = 0.9079)\n",
            "Saving model (epoch = 2373, loss = 0.9074)\n",
            "Saving model (epoch = 2378, loss = 0.9071)\n",
            "Saving model (epoch = 2380, loss = 0.9068)\n",
            "Saving model (epoch = 2392, loss = 0.9062)\n",
            "Saving model (epoch = 2404, loss = 0.9057)\n",
            "Saving model (epoch = 2422, loss = 0.9048)\n",
            "Saving model (epoch = 2435, loss = 0.9042)\n",
            "Saving model (epoch = 2443, loss = 0.9037)\n",
            "Saving model (epoch = 2448, loss = 0.9032)\n",
            "Saving model (epoch = 2468, loss = 0.9029)\n",
            "Saving model (epoch = 2479, loss = 0.9015)\n",
            "Saving model (epoch = 2496, loss = 0.9014)\n",
            "Saving model (epoch = 2518, loss = 0.9007)\n",
            "Saving model (epoch = 2524, loss = 0.9006)\n",
            "Saving model (epoch = 2533, loss = 0.8993)\n",
            "Saving model (epoch = 2537, loss = 0.8986)\n",
            "Saving model (epoch = 2554, loss = 0.8983)\n",
            "Saving model (epoch = 2559, loss = 0.8976)\n",
            "Saving model (epoch = 2589, loss = 0.8974)\n",
            "Saving model (epoch = 2600, loss = 0.8963)\n",
            "Saving model (epoch = 2605, loss = 0.8954)\n",
            "Saving model (epoch = 2617, loss = 0.8952)\n",
            "Saving model (epoch = 2622, loss = 0.8948)\n",
            "Saving model (epoch = 2635, loss = 0.8940)\n",
            "Saving model (epoch = 2639, loss = 0.8940)\n",
            "Saving model (epoch = 2657, loss = 0.8933)\n",
            "Saving model (epoch = 2679, loss = 0.8926)\n",
            "Saving model (epoch = 2681, loss = 0.8923)\n",
            "Saving model (epoch = 2688, loss = 0.8917)\n",
            "Saving model (epoch = 2691, loss = 0.8914)\n",
            "Saving model (epoch = 2716, loss = 0.8905)\n",
            "Saving model (epoch = 2721, loss = 0.8903)\n",
            "Saving model (epoch = 2723, loss = 0.8902)\n",
            "Saving model (epoch = 2732, loss = 0.8901)\n",
            "Saving model (epoch = 2758, loss = 0.8887)\n",
            "Saving model (epoch = 2760, loss = 0.8884)\n",
            "Saving model (epoch = 2761, loss = 0.8883)\n",
            "Saving model (epoch = 2768, loss = 0.8882)\n",
            "Saving model (epoch = 2785, loss = 0.8873)\n",
            "Saving model (epoch = 2786, loss = 0.8873)\n",
            "Saving model (epoch = 2811, loss = 0.8862)\n",
            "Saving model (epoch = 2819, loss = 0.8858)\n",
            "Saving model (epoch = 2825, loss = 0.8857)\n",
            "Saving model (epoch = 2831, loss = 0.8855)\n",
            "Saving model (epoch = 2842, loss = 0.8850)\n",
            "Saving model (epoch = 2888, loss = 0.8843)\n",
            "Saving model (epoch = 2892, loss = 0.8840)\n",
            "Saving model (epoch = 2908, loss = 0.8825)\n",
            "Saving model (epoch = 2912, loss = 0.8822)\n",
            "Saving model (epoch = 2929, loss = 0.8821)\n",
            "Saving model (epoch = 2934, loss = 0.8815)\n",
            "Saving model (epoch = 2953, loss = 0.8810)\n",
            "Saving model (epoch = 2962, loss = 0.8806)\n",
            "Saving model (epoch = 2964, loss = 0.8805)\n",
            "Saving model (epoch = 2978, loss = 0.8795)\n",
            "Saving model (epoch = 2985, loss = 0.8794)\n",
            "Saving model (epoch = 2992, loss = 0.8793)\n",
            "Saving model (epoch = 3000, loss = 0.8789)\n",
            "Saving model (epoch = 3038, loss = 0.8779)\n",
            "Saving model (epoch = 3052, loss = 0.8770)\n",
            "Saving model (epoch = 3062, loss = 0.8767)\n",
            "Saving model (epoch = 3068, loss = 0.8764)\n",
            "Saving model (epoch = 3075, loss = 0.8764)\n",
            "Saving model (epoch = 3090, loss = 0.8760)\n",
            "Saving model (epoch = 3106, loss = 0.8750)\n",
            "Saving model (epoch = 3114, loss = 0.8749)\n",
            "Saving model (epoch = 3127, loss = 0.8746)\n",
            "Saving model (epoch = 3146, loss = 0.8746)\n",
            "Saving model (epoch = 3148, loss = 0.8738)\n",
            "Saving model (epoch = 3152, loss = 0.8738)\n",
            "Saving model (epoch = 3164, loss = 0.8736)\n",
            "Saving model (epoch = 3178, loss = 0.8726)\n",
            "Saving model (epoch = 3187, loss = 0.8725)\n",
            "Saving model (epoch = 3191, loss = 0.8724)\n",
            "Saving model (epoch = 3206, loss = 0.8719)\n",
            "Saving model (epoch = 3209, loss = 0.8717)\n",
            "Saving model (epoch = 3237, loss = 0.8713)\n",
            "Saving model (epoch = 3248, loss = 0.8707)\n",
            "Saving model (epoch = 3258, loss = 0.8702)\n",
            "Saving model (epoch = 3280, loss = 0.8697)\n",
            "Saving model (epoch = 3303, loss = 0.8695)\n",
            "Saving model (epoch = 3312, loss = 0.8685)\n",
            "Saving model (epoch = 3345, loss = 0.8675)\n",
            "Saving model (epoch = 3350, loss = 0.8674)\n",
            "Saving model (epoch = 3358, loss = 0.8672)\n",
            "Saving model (epoch = 3382, loss = 0.8663)\n",
            "Saving model (epoch = 3391, loss = 0.8660)\n",
            "Saving model (epoch = 3401, loss = 0.8660)\n",
            "Saving model (epoch = 3416, loss = 0.8659)\n",
            "Saving model (epoch = 3423, loss = 0.8655)\n",
            "Saving model (epoch = 3431, loss = 0.8653)\n",
            "Saving model (epoch = 3444, loss = 0.8648)\n",
            "Saving model (epoch = 3473, loss = 0.8644)\n",
            "Saving model (epoch = 3484, loss = 0.8637)\n",
            "Saving model (epoch = 3488, loss = 0.8637)\n",
            "Saving model (epoch = 3503, loss = 0.8633)\n",
            "Saving model (epoch = 3522, loss = 0.8633)\n",
            "Saving model (epoch = 3539, loss = 0.8625)\n",
            "Saving model (epoch = 3555, loss = 0.8622)\n",
            "Saving model (epoch = 3558, loss = 0.8619)\n",
            "Saving model (epoch = 3560, loss = 0.8615)\n",
            "Saving model (epoch = 3564, loss = 0.8615)\n",
            "Saving model (epoch = 3578, loss = 0.8612)\n",
            "Saving model (epoch = 3632, loss = 0.8608)\n",
            "Saving model (epoch = 3635, loss = 0.8598)\n",
            "Saving model (epoch = 3638, loss = 0.8598)\n",
            "Saving model (epoch = 3642, loss = 0.8595)\n",
            "Saving model (epoch = 3650, loss = 0.8592)\n",
            "Saving model (epoch = 3659, loss = 0.8591)\n",
            "Saving model (epoch = 3671, loss = 0.8588)\n",
            "Saving model (epoch = 3695, loss = 0.8582)\n",
            "Saving model (epoch = 3696, loss = 0.8581)\n",
            "Saving model (epoch = 3707, loss = 0.8581)\n",
            "Saving model (epoch = 3751, loss = 0.8580)\n",
            "Saving model (epoch = 3754, loss = 0.8570)\n",
            "Saving model (epoch = 3766, loss = 0.8569)\n",
            "Saving model (epoch = 3774, loss = 0.8566)\n",
            "Saving model (epoch = 3782, loss = 0.8565)\n",
            "Saving model (epoch = 3796, loss = 0.8562)\n",
            "Saving model (epoch = 3804, loss = 0.8559)\n",
            "Saving model (epoch = 3813, loss = 0.8557)\n",
            "Saving model (epoch = 3821, loss = 0.8551)\n",
            "Saving model (epoch = 3881, loss = 0.8545)\n",
            "Saving model (epoch = 3889, loss = 0.8541)\n",
            "Saving model (epoch = 3907, loss = 0.8540)\n",
            "Saving model (epoch = 3922, loss = 0.8535)\n",
            "Saving model (epoch = 3924, loss = 0.8532)\n",
            "Saving model (epoch = 3945, loss = 0.8526)\n",
            "Saving model (epoch = 3960, loss = 0.8524)\n",
            "Saving model (epoch = 3975, loss = 0.8522)\n",
            "Saving model (epoch = 3992, loss = 0.8521)\n",
            "Saving model (epoch = 3993, loss = 0.8517)\n",
            "Saving model (epoch = 4000, loss = 0.8515)\n",
            "Saving model (epoch = 4016, loss = 0.8513)\n",
            "Saving model (epoch = 4025, loss = 0.8511)\n",
            "Saving model (epoch = 4035, loss = 0.8508)\n",
            "Saving model (epoch = 4047, loss = 0.8505)\n",
            "Saving model (epoch = 4057, loss = 0.8505)\n",
            "Saving model (epoch = 4085, loss = 0.8499)\n",
            "Saving model (epoch = 4094, loss = 0.8498)\n",
            "Saving model (epoch = 4097, loss = 0.8496)\n",
            "Saving model (epoch = 4107, loss = 0.8494)\n",
            "Saving model (epoch = 4113, loss = 0.8493)\n",
            "Saving model (epoch = 4122, loss = 0.8493)\n",
            "Saving model (epoch = 4128, loss = 0.8491)\n",
            "Saving model (epoch = 4167, loss = 0.8486)\n",
            "Saving model (epoch = 4175, loss = 0.8485)\n",
            "Saving model (epoch = 4192, loss = 0.8478)\n",
            "Saving model (epoch = 4225, loss = 0.8471)\n",
            "Saving model (epoch = 4248, loss = 0.8471)\n",
            "Saving model (epoch = 4254, loss = 0.8468)\n",
            "Saving model (epoch = 4264, loss = 0.8468)\n",
            "Saving model (epoch = 4274, loss = 0.8465)\n",
            "Saving model (epoch = 4306, loss = 0.8461)\n",
            "Saving model (epoch = 4314, loss = 0.8457)\n",
            "Saving model (epoch = 4347, loss = 0.8453)\n",
            "Saving model (epoch = 4350, loss = 0.8452)\n",
            "Saving model (epoch = 4373, loss = 0.8451)\n",
            "Saving model (epoch = 4385, loss = 0.8447)\n",
            "Saving model (epoch = 4389, loss = 0.8445)\n",
            "Saving model (epoch = 4394, loss = 0.8444)\n",
            "Saving model (epoch = 4417, loss = 0.8442)\n",
            "Saving model (epoch = 4443, loss = 0.8439)\n",
            "Saving model (epoch = 4451, loss = 0.8435)\n",
            "Saving model (epoch = 4475, loss = 0.8435)\n",
            "Saving model (epoch = 4476, loss = 0.8429)\n",
            "Saving model (epoch = 4491, loss = 0.8429)\n",
            "Saving model (epoch = 4492, loss = 0.8428)\n",
            "Saving model (epoch = 4499, loss = 0.8427)\n",
            "Saving model (epoch = 4519, loss = 0.8426)\n",
            "Saving model (epoch = 4553, loss = 0.8425)\n",
            "Saving model (epoch = 4576, loss = 0.8416)\n",
            "Saving model (epoch = 4587, loss = 0.8416)\n",
            "Saving model (epoch = 4602, loss = 0.8414)\n",
            "Saving model (epoch = 4609, loss = 0.8411)\n",
            "Saving model (epoch = 4642, loss = 0.8409)\n",
            "Saving model (epoch = 4662, loss = 0.8403)\n",
            "Saving model (epoch = 4684, loss = 0.8402)\n",
            "Saving model (epoch = 4691, loss = 0.8400)\n",
            "Saving model (epoch = 4710, loss = 0.8398)\n",
            "Saving model (epoch = 4717, loss = 0.8398)\n",
            "Saving model (epoch = 4719, loss = 0.8397)\n",
            "Saving model (epoch = 4742, loss = 0.8392)\n",
            "Saving model (epoch = 4778, loss = 0.8388)\n",
            "Saving model (epoch = 4788, loss = 0.8387)\n",
            "Saving model (epoch = 4804, loss = 0.8386)\n",
            "Saving model (epoch = 4817, loss = 0.8383)\n",
            "Saving model (epoch = 4837, loss = 0.8381)\n",
            "Saving model (epoch = 4856, loss = 0.8379)\n",
            "Saving model (epoch = 4857, loss = 0.8378)\n",
            "Saving model (epoch = 4862, loss = 0.8377)\n",
            "Saving model (epoch = 4908, loss = 0.8372)\n",
            "Saving model (epoch = 4950, loss = 0.8368)\n",
            "Saving model (epoch = 4963, loss = 0.8365)\n",
            "Saving model (epoch = 4981, loss = 0.8363)\n",
            "Saving model (epoch = 5012, loss = 0.8361)\n",
            "Saving model (epoch = 5016, loss = 0.8359)\n",
            "Saving model (epoch = 5022, loss = 0.8358)\n",
            "Saving model (epoch = 5076, loss = 0.8357)\n",
            "Saving model (epoch = 5078, loss = 0.8353)\n",
            "Saving model (epoch = 5079, loss = 0.8353)\n",
            "Saving model (epoch = 5091, loss = 0.8352)\n",
            "Saving model (epoch = 5129, loss = 0.8352)\n",
            "Saving model (epoch = 5144, loss = 0.8352)\n",
            "Saving model (epoch = 5146, loss = 0.8347)\n",
            "Saving model (epoch = 5157, loss = 0.8346)\n",
            "Saving model (epoch = 5169, loss = 0.8345)\n",
            "Saving model (epoch = 5184, loss = 0.8344)\n",
            "Saving model (epoch = 5188, loss = 0.8343)\n",
            "Saving model (epoch = 5196, loss = 0.8343)\n",
            "Saving model (epoch = 5203, loss = 0.8343)\n",
            "Saving model (epoch = 5208, loss = 0.8342)\n",
            "Saving model (epoch = 5211, loss = 0.8341)\n",
            "Saving model (epoch = 5225, loss = 0.8341)\n",
            "Saving model (epoch = 5239, loss = 0.8337)\n",
            "Saving model (epoch = 5242, loss = 0.8336)\n",
            "Saving model (epoch = 5260, loss = 0.8336)\n",
            "Saving model (epoch = 5275, loss = 0.8333)\n",
            "Saving model (epoch = 5278, loss = 0.8332)\n",
            "Saving model (epoch = 5287, loss = 0.8331)\n",
            "Saving model (epoch = 5327, loss = 0.8327)\n",
            "Saving model (epoch = 5328, loss = 0.8325)\n",
            "Saving model (epoch = 5349, loss = 0.8324)\n",
            "Saving model (epoch = 5362, loss = 0.8323)\n",
            "Saving model (epoch = 5424, loss = 0.8320)\n",
            "Saving model (epoch = 5431, loss = 0.8316)\n",
            "Saving model (epoch = 5466, loss = 0.8316)\n",
            "Saving model (epoch = 5467, loss = 0.8314)\n",
            "Saving model (epoch = 5472, loss = 0.8314)\n",
            "Saving model (epoch = 5481, loss = 0.8313)\n",
            "Saving model (epoch = 5483, loss = 0.8312)\n",
            "Saving model (epoch = 5500, loss = 0.8311)\n",
            "Saving model (epoch = 5534, loss = 0.8309)\n",
            "Saving model (epoch = 5578, loss = 0.8306)\n",
            "Saving model (epoch = 5579, loss = 0.8305)\n",
            "Saving model (epoch = 5603, loss = 0.8302)\n",
            "Saving model (epoch = 5615, loss = 0.8300)\n",
            "Saving model (epoch = 5628, loss = 0.8300)\n",
            "Saving model (epoch = 5650, loss = 0.8296)\n",
            "Saving model (epoch = 5696, loss = 0.8294)\n",
            "Saving model (epoch = 5714, loss = 0.8294)\n",
            "Saving model (epoch = 5732, loss = 0.8292)\n",
            "Saving model (epoch = 5789, loss = 0.8287)\n",
            "Saving model (epoch = 5834, loss = 0.8287)\n",
            "Saving model (epoch = 5837, loss = 0.8283)\n",
            "Saving model (epoch = 5866, loss = 0.8280)\n",
            "Saving model (epoch = 5877, loss = 0.8280)\n",
            "Saving model (epoch = 5934, loss = 0.8275)\n",
            "Saving model (epoch = 5949, loss = 0.8274)\n",
            "Saving model (epoch = 5969, loss = 0.8274)\n",
            "Saving model (epoch = 5980, loss = 0.8271)\n",
            "Saving model (epoch = 6004, loss = 0.8270)\n",
            "Saving model (epoch = 6064, loss = 0.8269)\n",
            "Saving model (epoch = 6080, loss = 0.8263)\n",
            "Saving model (epoch = 6110, loss = 0.8263)\n",
            "Saving model (epoch = 6140, loss = 0.8263)\n",
            "Saving model (epoch = 6148, loss = 0.8259)\n",
            "Saving model (epoch = 6191, loss = 0.8258)\n",
            "Saving model (epoch = 6206, loss = 0.8258)\n",
            "Saving model (epoch = 6222, loss = 0.8258)\n",
            "Saving model (epoch = 6231, loss = 0.8258)\n",
            "Saving model (epoch = 6235, loss = 0.8255)\n",
            "Saving model (epoch = 6284, loss = 0.8253)\n",
            "Saving model (epoch = 6300, loss = 0.8251)\n",
            "Saving model (epoch = 6358, loss = 0.8248)\n",
            "Saving model (epoch = 6383, loss = 0.8245)\n",
            "Saving model (epoch = 6432, loss = 0.8245)\n",
            "Saving model (epoch = 6441, loss = 0.8244)\n",
            "Saving model (epoch = 6471, loss = 0.8240)\n",
            "Saving model (epoch = 6477, loss = 0.8240)\n",
            "Saving model (epoch = 6528, loss = 0.8238)\n",
            "Saving model (epoch = 6537, loss = 0.8237)\n",
            "Saving model (epoch = 6574, loss = 0.8234)\n",
            "Saving model (epoch = 6603, loss = 0.8233)\n",
            "Saving model (epoch = 6618, loss = 0.8233)\n",
            "Saving model (epoch = 6622, loss = 0.8231)\n",
            "Saving model (epoch = 6632, loss = 0.8230)\n",
            "Saving model (epoch = 6656, loss = 0.8229)\n",
            "Saving model (epoch = 6687, loss = 0.8228)\n",
            "Saving model (epoch = 6695, loss = 0.8228)\n",
            "Saving model (epoch = 6730, loss = 0.8226)\n",
            "Saving model (epoch = 6754, loss = 0.8225)\n",
            "Saving model (epoch = 6759, loss = 0.8224)\n",
            "Saving model (epoch = 6773, loss = 0.8224)\n",
            "Saving model (epoch = 6799, loss = 0.8222)\n",
            "Saving model (epoch = 6844, loss = 0.8220)\n",
            "Saving model (epoch = 6861, loss = 0.8220)\n",
            "Saving model (epoch = 6868, loss = 0.8217)\n",
            "Saving model (epoch = 6943, loss = 0.8214)\n",
            "Saving model (epoch = 6980, loss = 0.8213)\n",
            "Saving model (epoch = 7022, loss = 0.8212)\n",
            "Saving model (epoch = 7044, loss = 0.8211)\n",
            "Saving model (epoch = 7064, loss = 0.8209)\n",
            "Saving model (epoch = 7087, loss = 0.8209)\n",
            "Saving model (epoch = 7114, loss = 0.8208)\n",
            "Saving model (epoch = 7131, loss = 0.8208)\n",
            "Saving model (epoch = 7186, loss = 0.8206)\n",
            "Saving model (epoch = 7197, loss = 0.8205)\n",
            "Saving model (epoch = 7202, loss = 0.8204)\n",
            "Saving model (epoch = 7267, loss = 0.8201)\n",
            "Saving model (epoch = 7284, loss = 0.8199)\n",
            "Saving model (epoch = 7379, loss = 0.8197)\n",
            "Saving model (epoch = 7382, loss = 0.8196)\n",
            "Saving model (epoch = 7407, loss = 0.8196)\n",
            "Saving model (epoch = 7432, loss = 0.8193)\n",
            "Saving model (epoch = 7459, loss = 0.8193)\n",
            "Saving model (epoch = 7476, loss = 0.8192)\n",
            "Saving model (epoch = 7490, loss = 0.8192)\n",
            "Saving model (epoch = 7514, loss = 0.8191)\n",
            "Saving model (epoch = 7525, loss = 0.8191)\n",
            "Saving model (epoch = 7579, loss = 0.8190)\n",
            "Saving model (epoch = 7587, loss = 0.8188)\n",
            "Saving model (epoch = 7674, loss = 0.8184)\n",
            "Saving model (epoch = 7683, loss = 0.8184)\n",
            "Saving model (epoch = 7729, loss = 0.8183)\n",
            "Saving model (epoch = 7744, loss = 0.8182)\n",
            "Saving model (epoch = 7751, loss = 0.8181)\n",
            "Saving model (epoch = 7795, loss = 0.8180)\n",
            "Saving model (epoch = 7882, loss = 0.8180)\n",
            "Saving model (epoch = 7917, loss = 0.8178)\n",
            "Saving model (epoch = 7934, loss = 0.8177)\n",
            "Saving model (epoch = 7960, loss = 0.8177)\n",
            "Saving model (epoch = 8019, loss = 0.8174)\n",
            "Saving model (epoch = 8039, loss = 0.8174)\n",
            "Saving model (epoch = 8047, loss = 0.8173)\n",
            "Saving model (epoch = 8064, loss = 0.8173)\n",
            "Saving model (epoch = 8071, loss = 0.8171)\n",
            "Saving model (epoch = 8159, loss = 0.8171)\n",
            "Saving model (epoch = 8170, loss = 0.8169)\n",
            "Saving model (epoch = 8198, loss = 0.8169)\n",
            "Saving model (epoch = 8237, loss = 0.8168)\n",
            "Saving model (epoch = 8240, loss = 0.8168)\n",
            "Saving model (epoch = 8246, loss = 0.8167)\n",
            "Saving model (epoch = 8286, loss = 0.8167)\n",
            "Saving model (epoch = 8313, loss = 0.8167)\n",
            "Saving model (epoch = 8324, loss = 0.8164)\n",
            "Saving model (epoch = 8356, loss = 0.8164)\n",
            "Saving model (epoch = 8378, loss = 0.8164)\n",
            "Saving model (epoch = 8397, loss = 0.8162)\n",
            "Saving model (epoch = 8488, loss = 0.8159)\n",
            "Saving model (epoch = 8602, loss = 0.8157)\n",
            "Saving model (epoch = 8659, loss = 0.8157)\n",
            "Saving model (epoch = 8666, loss = 0.8156)\n",
            "Saving model (epoch = 8677, loss = 0.8156)\n",
            "Saving model (epoch = 8721, loss = 0.8156)\n",
            "Saving model (epoch = 8722, loss = 0.8155)\n",
            "Saving model (epoch = 8734, loss = 0.8154)\n",
            "Saving model (epoch = 8758, loss = 0.8153)\n",
            "Saving model (epoch = 8763, loss = 0.8153)\n",
            "Saving model (epoch = 8771, loss = 0.8153)\n",
            "Saving model (epoch = 8782, loss = 0.8153)\n",
            "Saving model (epoch = 8793, loss = 0.8152)\n",
            "Saving model (epoch = 8797, loss = 0.8151)\n",
            "Saving model (epoch = 8843, loss = 0.8150)\n",
            "Saving model (epoch = 8980, loss = 0.8150)\n",
            "Saving model (epoch = 9012, loss = 0.8148)\n",
            "Saving model (epoch = 9063, loss = 0.8146)\n",
            "Saving model (epoch = 9118, loss = 0.8146)\n",
            "Saving model (epoch = 9153, loss = 0.8145)\n",
            "Saving model (epoch = 9160, loss = 0.8145)\n",
            "Saving model (epoch = 9176, loss = 0.8145)\n",
            "Saving model (epoch = 9178, loss = 0.8145)\n",
            "Saving model (epoch = 9213, loss = 0.8143)\n",
            "Saving model (epoch = 9234, loss = 0.8143)\n",
            "Saving model (epoch = 9245, loss = 0.8141)\n",
            "Saving model (epoch = 9270, loss = 0.8140)\n",
            "Saving model (epoch = 9318, loss = 0.8138)\n",
            "Saving model (epoch = 9489, loss = 0.8138)\n",
            "Saving model (epoch = 9503, loss = 0.8137)\n",
            "Saving model (epoch = 9512, loss = 0.8136)\n",
            "Saving model (epoch = 9526, loss = 0.8136)\n",
            "Saving model (epoch = 9603, loss = 0.8133)\n",
            "Saving model (epoch = 9619, loss = 0.8132)\n",
            "Saving model (epoch = 9676, loss = 0.8132)\n",
            "Saving model (epoch = 9772, loss = 0.8131)\n",
            "Saving model (epoch = 9822, loss = 0.8130)\n",
            "Saving model (epoch = 9829, loss = 0.8129)\n",
            "Saving model (epoch = 9896, loss = 0.8128)\n",
            "Saving model (epoch = 9945, loss = 0.8127)\n",
            "Saving model (epoch = 10010, loss = 0.8125)\n",
            "Saving model (epoch = 10087, loss = 0.8124)\n",
            "Saving model (epoch = 10128, loss = 0.8124)\n",
            "Saving model (epoch = 10190, loss = 0.8123)\n",
            "Saving model (epoch = 10210, loss = 0.8123)\n",
            "Saving model (epoch = 10279, loss = 0.8122)\n",
            "Saving model (epoch = 10295, loss = 0.8122)\n",
            "Saving model (epoch = 10331, loss = 0.8121)\n",
            "Saving model (epoch = 10358, loss = 0.8121)\n",
            "Saving model (epoch = 10380, loss = 0.8119)\n",
            "Saving model (epoch = 10460, loss = 0.8118)\n",
            "Saving model (epoch = 10508, loss = 0.8117)\n",
            "Saving model (epoch = 10549, loss = 0.8116)\n",
            "Saving model (epoch = 10564, loss = 0.8116)\n",
            "Saving model (epoch = 10587, loss = 0.8115)\n",
            "Saving model (epoch = 10673, loss = 0.8114)\n",
            "Saving model (epoch = 10696, loss = 0.8113)\n",
            "Saving model (epoch = 10769, loss = 0.8113)\n",
            "Saving model (epoch = 10773, loss = 0.8112)\n",
            "Saving model (epoch = 10850, loss = 0.8111)\n",
            "Saving model (epoch = 10863, loss = 0.8111)\n",
            "Saving model (epoch = 10876, loss = 0.8110)\n",
            "Saving model (epoch = 10912, loss = 0.8110)\n",
            "Saving model (epoch = 10938, loss = 0.8110)\n",
            "Saving model (epoch = 10968, loss = 0.8109)\n",
            "Saving model (epoch = 10975, loss = 0.8108)\n",
            "Saving model (epoch = 11143, loss = 0.8108)\n",
            "Saving model (epoch = 11172, loss = 0.8107)\n",
            "Saving model (epoch = 11208, loss = 0.8105)\n",
            "Saving model (epoch = 11307, loss = 0.8104)\n",
            "Saving model (epoch = 11337, loss = 0.8103)\n",
            "Saving model (epoch = 11357, loss = 0.8103)\n",
            "Saving model (epoch = 11401, loss = 0.8103)\n",
            "Saving model (epoch = 11488, loss = 0.8103)\n",
            "Saving model (epoch = 11500, loss = 0.8102)\n",
            "Saving model (epoch = 11503, loss = 0.8100)\n",
            "Saving model (epoch = 11529, loss = 0.8100)\n",
            "Saving model (epoch = 11645, loss = 0.8099)\n",
            "Saving model (epoch = 11673, loss = 0.8099)\n",
            "Saving model (epoch = 11718, loss = 0.8099)\n",
            "Saving model (epoch = 11738, loss = 0.8098)\n",
            "Saving model (epoch = 11759, loss = 0.8096)\n",
            "Saving model (epoch = 11874, loss = 0.8096)\n",
            "Saving model (epoch = 11964, loss = 0.8094)\n",
            "Saving model (epoch = 12097, loss = 0.8094)\n",
            "Saving model (epoch = 12103, loss = 0.8093)\n",
            "Saving model (epoch = 12174, loss = 0.8092)\n",
            "Saving model (epoch = 12233, loss = 0.8092)\n",
            "Saving model (epoch = 12262, loss = 0.8090)\n",
            "Saving model (epoch = 12406, loss = 0.8090)\n",
            "Saving model (epoch = 12427, loss = 0.8089)\n",
            "Saving model (epoch = 12553, loss = 0.8087)\n",
            "Saving model (epoch = 12585, loss = 0.8087)\n",
            "Saving model (epoch = 12684, loss = 0.8086)\n",
            "Saving model (epoch = 12761, loss = 0.8085)\n",
            "Saving model (epoch = 12767, loss = 0.8084)\n",
            "Saving model (epoch = 12940, loss = 0.8084)\n",
            "Saving model (epoch = 12959, loss = 0.8082)\n",
            "Saving model (epoch = 12973, loss = 0.8082)\n",
            "Saving model (epoch = 13019, loss = 0.8082)\n",
            "Saving model (epoch = 13068, loss = 0.8081)\n",
            "Saving model (epoch = 13193, loss = 0.8080)\n",
            "Saving model (epoch = 13287, loss = 0.8080)\n",
            "Saving model (epoch = 13429, loss = 0.8078)\n",
            "Saving model (epoch = 13435, loss = 0.8077)\n",
            "Saving model (epoch = 13436, loss = 0.8076)\n",
            "Saving model (epoch = 13616, loss = 0.8076)\n",
            "Saving model (epoch = 13631, loss = 0.8075)\n",
            "Saving model (epoch = 13673, loss = 0.8074)\n",
            "Saving model (epoch = 13700, loss = 0.8074)\n",
            "Saving model (epoch = 13810, loss = 0.8073)\n",
            "Saving model (epoch = 13828, loss = 0.8072)\n",
            "Saving model (epoch = 13878, loss = 0.8072)\n",
            "Saving model (epoch = 13949, loss = 0.8071)\n",
            "Saving model (epoch = 14041, loss = 0.8071)\n",
            "Saving model (epoch = 14086, loss = 0.8070)\n",
            "Saving model (epoch = 14099, loss = 0.8068)\n",
            "Saving model (epoch = 14201, loss = 0.8068)\n",
            "Saving model (epoch = 14219, loss = 0.8067)\n",
            "Saving model (epoch = 14233, loss = 0.8067)\n",
            "Saving model (epoch = 14242, loss = 0.8067)\n",
            "Saving model (epoch = 14268, loss = 0.8067)\n",
            "Saving model (epoch = 14312, loss = 0.8066)\n",
            "Saving model (epoch = 14382, loss = 0.8066)\n",
            "Saving model (epoch = 14556, loss = 0.8066)\n",
            "Saving model (epoch = 14593, loss = 0.8065)\n",
            "Saving model (epoch = 14627, loss = 0.8063)\n",
            "Saving model (epoch = 14828, loss = 0.8062)\n",
            "Saving model (epoch = 14886, loss = 0.8060)\n",
            "Saving model (epoch = 15026, loss = 0.8060)\n",
            "Saving model (epoch = 15056, loss = 0.8059)\n",
            "Saving model (epoch = 15108, loss = 0.8057)\n",
            "Saving model (epoch = 15343, loss = 0.8056)\n",
            "Saving model (epoch = 15458, loss = 0.8056)\n",
            "Saving model (epoch = 15489, loss = 0.8056)\n",
            "Saving model (epoch = 15543, loss = 0.8054)\n",
            "Saving model (epoch = 15631, loss = 0.8054)\n",
            "Saving model (epoch = 15737, loss = 0.8054)\n",
            "Saving model (epoch = 15786, loss = 0.8053)\n",
            "Saving model (epoch = 15877, loss = 0.8052)\n",
            "Saving model (epoch = 15899, loss = 0.8050)\n",
            "Saving model (epoch = 16032, loss = 0.8050)\n",
            "Saving model (epoch = 16153, loss = 0.8050)\n",
            "Saving model (epoch = 16215, loss = 0.8050)\n",
            "Saving model (epoch = 16226, loss = 0.8049)\n",
            "Saving model (epoch = 16307, loss = 0.8049)\n",
            "Saving model (epoch = 16380, loss = 0.8048)\n",
            "Saving model (epoch = 16387, loss = 0.8047)\n",
            "Saving model (epoch = 16463, loss = 0.8046)\n",
            "Saving model (epoch = 16501, loss = 0.8045)\n",
            "Saving model (epoch = 16608, loss = 0.8045)\n",
            "Saving model (epoch = 16668, loss = 0.8045)\n",
            "Saving model (epoch = 16701, loss = 0.8044)\n",
            "Saving model (epoch = 16796, loss = 0.8044)\n",
            "Saving model (epoch = 16831, loss = 0.8044)\n",
            "Saving model (epoch = 16872, loss = 0.8044)\n",
            "Saving model (epoch = 16899, loss = 0.8043)\n",
            "Saving model (epoch = 16904, loss = 0.8041)\n",
            "Saving model (epoch = 16940, loss = 0.8041)\n",
            "Saving model (epoch = 17121, loss = 0.8040)\n",
            "Saving model (epoch = 17146, loss = 0.8039)\n",
            "Saving model (epoch = 17159, loss = 0.8039)\n",
            "Saving model (epoch = 17199, loss = 0.8038)\n",
            "Saving model (epoch = 17280, loss = 0.8038)\n",
            "Saving model (epoch = 17348, loss = 0.8037)\n",
            "Saving model (epoch = 17448, loss = 0.8037)\n",
            "Saving model (epoch = 17513, loss = 0.8035)\n",
            "Saving model (epoch = 17808, loss = 0.8034)\n",
            "Saving model (epoch = 17876, loss = 0.8033)\n",
            "Saving model (epoch = 17965, loss = 0.8032)\n",
            "Saving model (epoch = 18025, loss = 0.8030)\n",
            "Saving model (epoch = 18030, loss = 0.8030)\n",
            "Saving model (epoch = 18263, loss = 0.8029)\n",
            "Saving model (epoch = 18295, loss = 0.8029)\n",
            "Saving model (epoch = 18344, loss = 0.8027)\n",
            "Saving model (epoch = 18486, loss = 0.8025)\n",
            "Saving model (epoch = 18761, loss = 0.8025)\n",
            "Saving model (epoch = 18788, loss = 0.8024)\n",
            "Saving model (epoch = 18813, loss = 0.8024)\n",
            "Saving model (epoch = 18872, loss = 0.8024)\n",
            "Saving model (epoch = 18986, loss = 0.8024)\n",
            "Saving model (epoch = 18995, loss = 0.8023)\n",
            "Saving model (epoch = 19155, loss = 0.8023)\n",
            "Saving model (epoch = 19158, loss = 0.8021)\n",
            "Saving model (epoch = 19221, loss = 0.8021)\n",
            "Saving model (epoch = 19282, loss = 0.8021)\n",
            "Saving model (epoch = 19407, loss = 0.8019)\n",
            "Saving model (epoch = 19498, loss = 0.8017)\n",
            "Saving model (epoch = 19579, loss = 0.8016)\n",
            "Saving model (epoch = 19734, loss = 0.8016)\n",
            "Saving model (epoch = 19837, loss = 0.8015)\n",
            "Saving model (epoch = 19920, loss = 0.8014)\n",
            "Saving model (epoch = 19953, loss = 0.8014)\n",
            "Saving model (epoch = 20064, loss = 0.8013)\n",
            "Saving model (epoch = 20087, loss = 0.8012)\n",
            "Saving model (epoch = 20179, loss = 0.8012)\n",
            "Saving model (epoch = 20200, loss = 0.8011)\n",
            "Saving model (epoch = 20330, loss = 0.8011)\n",
            "Saving model (epoch = 20421, loss = 0.8010)\n",
            "Saving model (epoch = 20423, loss = 0.8009)\n",
            "Saving model (epoch = 20636, loss = 0.8009)\n",
            "Saving model (epoch = 20696, loss = 0.8008)\n",
            "Saving model (epoch = 20805, loss = 0.8007)\n",
            "Saving model (epoch = 20825, loss = 0.8006)\n",
            "Saving model (epoch = 20876, loss = 0.8006)\n",
            "Saving model (epoch = 20909, loss = 0.8006)\n",
            "Saving model (epoch = 20943, loss = 0.8005)\n",
            "Saving model (epoch = 21060, loss = 0.8005)\n",
            "Saving model (epoch = 21216, loss = 0.8005)\n",
            "Saving model (epoch = 21250, loss = 0.8004)\n",
            "Saving model (epoch = 21287, loss = 0.8004)\n",
            "Saving model (epoch = 21336, loss = 0.8002)\n",
            "Saving model (epoch = 21348, loss = 0.8002)\n",
            "Saving model (epoch = 21413, loss = 0.8002)\n",
            "Saving model (epoch = 21442, loss = 0.8001)\n",
            "Saving model (epoch = 21452, loss = 0.8000)\n",
            "Saving model (epoch = 21455, loss = 0.8000)\n",
            "Saving model (epoch = 21564, loss = 0.7999)\n",
            "Saving model (epoch = 21625, loss = 0.7999)\n",
            "Saving model (epoch = 21901, loss = 0.7998)\n",
            "Saving model (epoch = 21927, loss = 0.7997)\n",
            "Saving model (epoch = 22013, loss = 0.7996)\n",
            "Saving model (epoch = 22066, loss = 0.7996)\n",
            "Saving model (epoch = 22079, loss = 0.7995)\n",
            "Saving model (epoch = 22132, loss = 0.7995)\n",
            "Saving model (epoch = 22275, loss = 0.7994)\n",
            "Saving model (epoch = 22364, loss = 0.7993)\n",
            "Saving model (epoch = 22435, loss = 0.7992)\n",
            "Saving model (epoch = 22604, loss = 0.7991)\n",
            "Saving model (epoch = 22748, loss = 0.7991)\n",
            "Saving model (epoch = 22749, loss = 0.7990)\n",
            "Saving model (epoch = 22867, loss = 0.7989)\n",
            "Saving model (epoch = 22876, loss = 0.7989)\n",
            "Saving model (epoch = 23052, loss = 0.7986)\n",
            "Saving model (epoch = 23204, loss = 0.7984)\n",
            "Saving model (epoch = 23322, loss = 0.7984)\n",
            "Saving model (epoch = 23327, loss = 0.7984)\n",
            "Saving model (epoch = 23371, loss = 0.7984)\n",
            "Saving model (epoch = 23756, loss = 0.7983)\n",
            "Saving model (epoch = 23791, loss = 0.7982)\n",
            "Saving model (epoch = 23811, loss = 0.7982)\n",
            "Saving model (epoch = 24001, loss = 0.7981)\n",
            "Saving model (epoch = 24107, loss = 0.7980)\n",
            "Saving model (epoch = 24140, loss = 0.7980)\n",
            "Saving model (epoch = 24186, loss = 0.7978)\n",
            "Saving model (epoch = 24188, loss = 0.7977)\n",
            "Saving model (epoch = 24411, loss = 0.7977)\n",
            "Saving model (epoch = 24535, loss = 0.7977)\n",
            "Saving model (epoch = 24580, loss = 0.7976)\n",
            "Saving model (epoch = 24703, loss = 0.7975)\n",
            "Saving model (epoch = 24812, loss = 0.7975)\n",
            "Saving model (epoch = 25016, loss = 0.7974)\n",
            "Saving model (epoch = 25080, loss = 0.7974)\n",
            "Saving model (epoch = 25105, loss = 0.7973)\n",
            "Saving model (epoch = 25107, loss = 0.7973)\n",
            "Saving model (epoch = 25190, loss = 0.7971)\n",
            "Saving model (epoch = 25386, loss = 0.7970)\n",
            "Saving model (epoch = 25551, loss = 0.7970)\n",
            "Saving model (epoch = 25593, loss = 0.7970)\n",
            "Saving model (epoch = 25652, loss = 0.7969)\n",
            "Saving model (epoch = 25776, loss = 0.7968)\n",
            "Saving model (epoch = 25855, loss = 0.7967)\n",
            "Saving model (epoch = 25947, loss = 0.7965)\n",
            "Saving model (epoch = 26124, loss = 0.7964)\n",
            "Saving model (epoch = 26170, loss = 0.7963)\n",
            "Saving model (epoch = 26244, loss = 0.7963)\n",
            "Saving model (epoch = 26300, loss = 0.7962)\n",
            "Saving model (epoch = 26435, loss = 0.7962)\n",
            "Saving model (epoch = 26491, loss = 0.7961)\n",
            "Saving model (epoch = 26496, loss = 0.7960)\n",
            "Saving model (epoch = 26704, loss = 0.7960)\n",
            "Saving model (epoch = 26825, loss = 0.7960)\n",
            "Saving model (epoch = 26835, loss = 0.7959)\n",
            "Saving model (epoch = 26842, loss = 0.7957)\n",
            "Saving model (epoch = 26868, loss = 0.7957)\n",
            "Saving model (epoch = 26872, loss = 0.7956)\n",
            "Saving model (epoch = 26891, loss = 0.7956)\n",
            "Saving model (epoch = 26918, loss = 0.7956)\n",
            "Saving model (epoch = 27189, loss = 0.7956)\n",
            "Saving model (epoch = 27321, loss = 0.7955)\n",
            "Saving model (epoch = 27329, loss = 0.7955)\n",
            "Saving model (epoch = 27498, loss = 0.7954)\n",
            "Saving model (epoch = 27575, loss = 0.7953)\n",
            "Saving model (epoch = 27650, loss = 0.7953)\n",
            "Saving model (epoch = 27700, loss = 0.7951)\n",
            "Saving model (epoch = 27861, loss = 0.7951)\n",
            "Saving model (epoch = 27906, loss = 0.7950)\n",
            "Saving model (epoch = 27957, loss = 0.7950)\n",
            "Saving model (epoch = 28118, loss = 0.7949)\n",
            "Saving model (epoch = 28237, loss = 0.7948)\n",
            "Saving model (epoch = 28414, loss = 0.7947)\n",
            "Saving model (epoch = 28591, loss = 0.7947)\n",
            "Saving model (epoch = 28615, loss = 0.7947)\n",
            "Saving model (epoch = 28616, loss = 0.7946)\n",
            "Saving model (epoch = 28737, loss = 0.7946)\n",
            "Saving model (epoch = 28762, loss = 0.7945)\n",
            "Saving model (epoch = 28874, loss = 0.7944)\n",
            "Saving model (epoch = 28922, loss = 0.7944)\n",
            "Saving model (epoch = 28923, loss = 0.7943)\n",
            "Saving model (epoch = 29042, loss = 0.7942)\n",
            "Saving model (epoch = 29159, loss = 0.7942)\n",
            "Saving model (epoch = 29165, loss = 0.7941)\n",
            "Saving model (epoch = 29338, loss = 0.7939)\n",
            "Saving model (epoch = 29587, loss = 0.7938)\n",
            "Saving model (epoch = 29696, loss = 0.7936)\n",
            "Saving model (epoch = 29820, loss = 0.7936)\n",
            "Saving model (epoch = 30038, loss = 0.7935)\n",
            "Saving model (epoch = 30090, loss = 0.7935)\n",
            "Saving model (epoch = 30248, loss = 0.7934)\n",
            "Saving model (epoch = 30276, loss = 0.7933)\n",
            "Saving model (epoch = 30291, loss = 0.7933)\n",
            "Saving model (epoch = 30341, loss = 0.7932)\n",
            "Saving model (epoch = 30540, loss = 0.7931)\n",
            "Saving model (epoch = 30697, loss = 0.7930)\n",
            "Saving model (epoch = 30830, loss = 0.7929)\n",
            "Saving model (epoch = 30962, loss = 0.7929)\n",
            "Saving model (epoch = 31045, loss = 0.7929)\n",
            "Saving model (epoch = 31059, loss = 0.7928)\n",
            "Saving model (epoch = 31247, loss = 0.7926)\n",
            "Saving model (epoch = 31304, loss = 0.7925)\n",
            "Saving model (epoch = 31565, loss = 0.7925)\n",
            "Saving model (epoch = 31609, loss = 0.7923)\n",
            "Saving model (epoch = 31717, loss = 0.7923)\n",
            "Saving model (epoch = 31725, loss = 0.7922)\n",
            "Saving model (epoch = 31867, loss = 0.7922)\n",
            "Saving model (epoch = 31900, loss = 0.7921)\n",
            "Saving model (epoch = 32027, loss = 0.7921)\n",
            "Saving model (epoch = 32137, loss = 0.7920)\n",
            "Saving model (epoch = 32294, loss = 0.7920)\n",
            "Saving model (epoch = 32365, loss = 0.7919)\n",
            "Saving model (epoch = 32412, loss = 0.7918)\n",
            "Saving model (epoch = 32481, loss = 0.7918)\n",
            "Saving model (epoch = 32558, loss = 0.7917)\n",
            "Saving model (epoch = 32668, loss = 0.7916)\n",
            "Saving model (epoch = 32777, loss = 0.7916)\n",
            "Saving model (epoch = 32807, loss = 0.7916)\n",
            "Saving model (epoch = 32953, loss = 0.7916)\n",
            "Saving model (epoch = 32997, loss = 0.7915)\n",
            "Saving model (epoch = 33045, loss = 0.7914)\n",
            "Saving model (epoch = 33093, loss = 0.7913)\n",
            "Saving model (epoch = 33151, loss = 0.7912)\n",
            "Saving model (epoch = 33231, loss = 0.7911)\n",
            "Saving model (epoch = 33469, loss = 0.7911)\n",
            "Saving model (epoch = 33484, loss = 0.7910)\n",
            "Saving model (epoch = 33568, loss = 0.7910)\n",
            "Saving model (epoch = 33615, loss = 0.7910)\n",
            "Saving model (epoch = 33708, loss = 0.7909)\n",
            "Saving model (epoch = 33720, loss = 0.7909)\n",
            "Saving model (epoch = 33839, loss = 0.7907)\n",
            "Saving model (epoch = 34002, loss = 0.7907)\n",
            "Saving model (epoch = 34162, loss = 0.7906)\n",
            "Saving model (epoch = 34176, loss = 0.7906)\n",
            "Saving model (epoch = 34205, loss = 0.7906)\n",
            "Saving model (epoch = 34208, loss = 0.7906)\n",
            "Saving model (epoch = 34222, loss = 0.7904)\n",
            "Saving model (epoch = 34422, loss = 0.7903)\n",
            "Saving model (epoch = 34671, loss = 0.7902)\n",
            "Saving model (epoch = 34691, loss = 0.7902)\n",
            "Saving model (epoch = 34832, loss = 0.7901)\n",
            "Saving model (epoch = 34940, loss = 0.7900)\n",
            "Saving model (epoch = 35104, loss = 0.7898)\n",
            "Saving model (epoch = 35406, loss = 0.7897)\n",
            "Saving model (epoch = 35513, loss = 0.7897)\n",
            "Saving model (epoch = 35728, loss = 0.7897)\n",
            "Saving model (epoch = 35772, loss = 0.7896)\n",
            "Saving model (epoch = 35777, loss = 0.7896)\n",
            "Saving model (epoch = 35822, loss = 0.7895)\n",
            "Saving model (epoch = 35970, loss = 0.7894)\n",
            "Saving model (epoch = 36054, loss = 0.7893)\n",
            "Saving model (epoch = 36190, loss = 0.7893)\n",
            "Saving model (epoch = 36212, loss = 0.7892)\n",
            "Saving model (epoch = 36442, loss = 0.7891)\n",
            "Saving model (epoch = 36553, loss = 0.7891)\n",
            "Saving model (epoch = 36723, loss = 0.7889)\n",
            "Saving model (epoch = 36800, loss = 0.7889)\n",
            "Saving model (epoch = 36926, loss = 0.7888)\n",
            "Saving model (epoch = 36941, loss = 0.7887)\n",
            "Saving model (epoch = 37120, loss = 0.7886)\n",
            "Saving model (epoch = 37126, loss = 0.7886)\n",
            "Saving model (epoch = 37174, loss = 0.7885)\n",
            "Saving model (epoch = 37347, loss = 0.7885)\n",
            "Saving model (epoch = 37379, loss = 0.7885)\n",
            "Saving model (epoch = 37401, loss = 0.7884)\n",
            "Saving model (epoch = 37580, loss = 0.7884)\n",
            "Saving model (epoch = 37659, loss = 0.7883)\n",
            "Saving model (epoch = 37783, loss = 0.7882)\n",
            "Saving model (epoch = 37877, loss = 0.7882)\n",
            "Saving model (epoch = 37992, loss = 0.7881)\n",
            "Saving model (epoch = 38122, loss = 0.7881)\n",
            "Saving model (epoch = 38125, loss = 0.7881)\n",
            "Saving model (epoch = 38410, loss = 0.7880)\n",
            "Saving model (epoch = 38455, loss = 0.7877)\n",
            "Saving model (epoch = 38797, loss = 0.7875)\n",
            "Saving model (epoch = 38807, loss = 0.7874)\n",
            "Saving model (epoch = 39098, loss = 0.7872)\n",
            "Saving model (epoch = 39529, loss = 0.7871)\n",
            "Saving model (epoch = 39684, loss = 0.7870)\n",
            "Saving model (epoch = 39733, loss = 0.7870)\n",
            "Saving model (epoch = 39838, loss = 0.7870)\n",
            "Saving model (epoch = 39866, loss = 0.7869)\n",
            "Saving model (epoch = 39918, loss = 0.7868)\n",
            "Saving model (epoch = 40146, loss = 0.7868)\n",
            "Saving model (epoch = 40185, loss = 0.7867)\n",
            "Saving model (epoch = 40206, loss = 0.7867)\n",
            "Saving model (epoch = 40278, loss = 0.7867)\n",
            "Saving model (epoch = 40355, loss = 0.7865)\n",
            "Saving model (epoch = 40362, loss = 0.7864)\n",
            "Saving model (epoch = 40727, loss = 0.7863)\n",
            "Saving model (epoch = 40835, loss = 0.7863)\n",
            "Saving model (epoch = 40940, loss = 0.7863)\n",
            "Saving model (epoch = 41023, loss = 0.7862)\n",
            "Saving model (epoch = 41072, loss = 0.7860)\n",
            "Saving model (epoch = 41105, loss = 0.7859)\n",
            "Saving model (epoch = 41235, loss = 0.7859)\n",
            "Saving model (epoch = 41583, loss = 0.7859)\n",
            "Saving model (epoch = 41652, loss = 0.7858)\n",
            "Saving model (epoch = 41708, loss = 0.7857)\n",
            "Saving model (epoch = 41816, loss = 0.7856)\n",
            "Saving model (epoch = 42024, loss = 0.7854)\n",
            "Saving model (epoch = 42114, loss = 0.7854)\n",
            "Saving model (epoch = 42372, loss = 0.7854)\n",
            "Saving model (epoch = 42431, loss = 0.7853)\n",
            "Saving model (epoch = 42699, loss = 0.7850)\n",
            "Saving model (epoch = 43042, loss = 0.7849)\n",
            "Saving model (epoch = 43096, loss = 0.7849)\n",
            "Saving model (epoch = 43164, loss = 0.7849)\n",
            "Saving model (epoch = 43374, loss = 0.7847)\n",
            "Saving model (epoch = 43470, loss = 0.7847)\n",
            "Saving model (epoch = 43545, loss = 0.7847)\n",
            "Saving model (epoch = 43624, loss = 0.7847)\n",
            "Saving model (epoch = 43698, loss = 0.7844)\n",
            "Saving model (epoch = 44185, loss = 0.7843)\n",
            "Saving model (epoch = 44208, loss = 0.7843)\n",
            "Saving model (epoch = 44284, loss = 0.7842)\n",
            "Saving model (epoch = 44389, loss = 0.7841)\n",
            "Saving model (epoch = 44474, loss = 0.7841)\n",
            "Saving model (epoch = 44753, loss = 0.7841)\n",
            "Saving model (epoch = 44787, loss = 0.7840)\n",
            "Saving model (epoch = 44846, loss = 0.7839)\n",
            "Saving model (epoch = 44891, loss = 0.7839)\n",
            "Saving model (epoch = 44896, loss = 0.7838)\n",
            "Saving model (epoch = 45052, loss = 0.7838)\n",
            "Saving model (epoch = 45168, loss = 0.7837)\n",
            "Saving model (epoch = 45191, loss = 0.7837)\n",
            "Saving model (epoch = 45356, loss = 0.7834)\n",
            "Saving model (epoch = 45673, loss = 0.7834)\n",
            "Saving model (epoch = 45825, loss = 0.7833)\n",
            "Saving model (epoch = 45876, loss = 0.7833)\n",
            "Saving model (epoch = 45894, loss = 0.7833)\n",
            "Saving model (epoch = 46061, loss = 0.7833)\n",
            "Saving model (epoch = 46066, loss = 0.7832)\n",
            "Saving model (epoch = 46110, loss = 0.7831)\n",
            "Saving model (epoch = 46564, loss = 0.7830)\n",
            "Saving model (epoch = 46581, loss = 0.7829)\n",
            "Saving model (epoch = 46650, loss = 0.7829)\n",
            "Saving model (epoch = 46735, loss = 0.7829)\n",
            "Saving model (epoch = 46894, loss = 0.7829)\n",
            "Saving model (epoch = 46897, loss = 0.7828)\n",
            "Saving model (epoch = 46920, loss = 0.7828)\n",
            "Saving model (epoch = 47012, loss = 0.7827)\n",
            "Saving model (epoch = 47129, loss = 0.7827)\n",
            "Saving model (epoch = 47142, loss = 0.7827)\n",
            "Saving model (epoch = 47173, loss = 0.7826)\n",
            "Saving model (epoch = 47226, loss = 0.7825)\n",
            "Saving model (epoch = 47484, loss = 0.7825)\n",
            "Saving model (epoch = 47518, loss = 0.7823)\n",
            "Saving model (epoch = 47521, loss = 0.7823)\n",
            "Saving model (epoch = 47754, loss = 0.7822)\n",
            "Saving model (epoch = 47817, loss = 0.7822)\n",
            "Saving model (epoch = 48108, loss = 0.7822)\n",
            "Saving model (epoch = 48127, loss = 0.7820)\n",
            "Saving model (epoch = 48172, loss = 0.7820)\n",
            "Saving model (epoch = 48208, loss = 0.7820)\n",
            "Saving model (epoch = 48232, loss = 0.7819)\n",
            "Saving model (epoch = 48506, loss = 0.7819)\n",
            "Saving model (epoch = 48509, loss = 0.7818)\n",
            "Saving model (epoch = 48824, loss = 0.7818)\n",
            "Saving model (epoch = 48970, loss = 0.7817)\n",
            "Saving model (epoch = 48981, loss = 0.7816)\n",
            "Saving model (epoch = 49149, loss = 0.7816)\n",
            "Saving model (epoch = 49226, loss = 0.7815)\n",
            "Saving model (epoch = 49255, loss = 0.7815)\n",
            "Saving model (epoch = 49369, loss = 0.7813)\n",
            "Saving model (epoch = 49403, loss = 0.7813)\n",
            "Saving model (epoch = 49411, loss = 0.7812)\n",
            "Saving model (epoch = 50097, loss = 0.7811)\n",
            "Saving model (epoch = 50160, loss = 0.7809)\n",
            "Saving model (epoch = 50318, loss = 0.7808)\n",
            "Saving model (epoch = 50701, loss = 0.7807)\n",
            "Saving model (epoch = 50874, loss = 0.7805)\n",
            "Saving model (epoch = 51031, loss = 0.7805)\n",
            "Saving model (epoch = 51331, loss = 0.7804)\n",
            "Saving model (epoch = 51375, loss = 0.7803)\n",
            "Saving model (epoch = 51679, loss = 0.7802)\n",
            "Saving model (epoch = 51697, loss = 0.7801)\n",
            "Saving model (epoch = 52031, loss = 0.7799)\n",
            "Saving model (epoch = 52225, loss = 0.7799)\n",
            "Saving model (epoch = 52322, loss = 0.7798)\n",
            "Saving model (epoch = 52578, loss = 0.7797)\n",
            "Saving model (epoch = 52595, loss = 0.7796)\n",
            "Saving model (epoch = 52674, loss = 0.7796)\n",
            "Saving model (epoch = 52762, loss = 0.7796)\n",
            "Saving model (epoch = 52931, loss = 0.7795)\n",
            "Saving model (epoch = 53090, loss = 0.7795)\n",
            "Saving model (epoch = 53118, loss = 0.7794)\n",
            "Saving model (epoch = 53228, loss = 0.7792)\n",
            "Saving model (epoch = 53605, loss = 0.7790)\n",
            "Saving model (epoch = 54007, loss = 0.7789)\n",
            "Saving model (epoch = 54557, loss = 0.7788)\n",
            "Saving model (epoch = 54604, loss = 0.7788)\n",
            "Saving model (epoch = 54715, loss = 0.7787)\n",
            "Saving model (epoch = 54725, loss = 0.7786)\n",
            "Saving model (epoch = 54964, loss = 0.7785)\n",
            "Saving model (epoch = 55137, loss = 0.7784)\n",
            "Saving model (epoch = 55386, loss = 0.7783)\n",
            "Saving model (epoch = 55585, loss = 0.7782)\n",
            "Saving model (epoch = 55730, loss = 0.7782)\n",
            "Saving model (epoch = 55734, loss = 0.7782)\n",
            "Saving model (epoch = 55789, loss = 0.7782)\n",
            "Saving model (epoch = 55854, loss = 0.7781)\n",
            "Saving model (epoch = 56025, loss = 0.7780)\n",
            "Saving model (epoch = 56039, loss = 0.7779)\n",
            "Saving model (epoch = 56083, loss = 0.7779)\n",
            "Saving model (epoch = 56420, loss = 0.7778)\n",
            "Saving model (epoch = 56630, loss = 0.7778)\n",
            "Saving model (epoch = 56651, loss = 0.7777)\n",
            "Saving model (epoch = 56761, loss = 0.7775)\n",
            "Saving model (epoch = 57153, loss = 0.7775)\n",
            "Saving model (epoch = 57164, loss = 0.7775)\n",
            "Saving model (epoch = 57208, loss = 0.7774)\n",
            "Saving model (epoch = 57389, loss = 0.7774)\n",
            "Saving model (epoch = 57487, loss = 0.7774)\n",
            "Saving model (epoch = 57522, loss = 0.7773)\n",
            "Saving model (epoch = 57565, loss = 0.7773)\n",
            "Saving model (epoch = 57633, loss = 0.7771)\n",
            "Saving model (epoch = 57999, loss = 0.7771)\n",
            "Saving model (epoch = 58097, loss = 0.7769)\n",
            "Saving model (epoch = 58472, loss = 0.7769)\n",
            "Saving model (epoch = 58493, loss = 0.7768)\n",
            "Saving model (epoch = 58755, loss = 0.7768)\n",
            "Saving model (epoch = 58857, loss = 0.7768)\n",
            "Saving model (epoch = 58880, loss = 0.7767)\n",
            "Saving model (epoch = 58907, loss = 0.7766)\n",
            "Saving model (epoch = 59183, loss = 0.7765)\n",
            "Saving model (epoch = 59373, loss = 0.7765)\n",
            "Saving model (epoch = 59436, loss = 0.7765)\n",
            "Saving model (epoch = 59528, loss = 0.7762)\n",
            "Saving model (epoch = 60143, loss = 0.7762)\n",
            "Saving model (epoch = 60307, loss = 0.7760)\n",
            "Saving model (epoch = 60402, loss = 0.7760)\n",
            "Saving model (epoch = 60573, loss = 0.7760)\n",
            "Saving model (epoch = 60680, loss = 0.7760)\n",
            "Saving model (epoch = 60763, loss = 0.7759)\n",
            "Saving model (epoch = 60785, loss = 0.7759)\n",
            "Saving model (epoch = 60888, loss = 0.7757)\n",
            "Saving model (epoch = 61290, loss = 0.7757)\n",
            "Saving model (epoch = 61351, loss = 0.7756)\n",
            "Saving model (epoch = 61673, loss = 0.7755)\n",
            "Saving model (epoch = 61961, loss = 0.7754)\n",
            "Saving model (epoch = 62036, loss = 0.7754)\n",
            "Saving model (epoch = 62256, loss = 0.7751)\n",
            "Saving model (epoch = 62314, loss = 0.7751)\n",
            "Saving model (epoch = 62692, loss = 0.7750)\n",
            "Saving model (epoch = 62988, loss = 0.7750)\n",
            "Saving model (epoch = 63007, loss = 0.7750)\n",
            "Saving model (epoch = 63199, loss = 0.7749)\n",
            "Saving model (epoch = 63204, loss = 0.7748)\n",
            "Saving model (epoch = 63397, loss = 0.7748)\n",
            "Saving model (epoch = 63491, loss = 0.7748)\n",
            "Saving model (epoch = 63615, loss = 0.7747)\n",
            "Saving model (epoch = 63629, loss = 0.7746)\n",
            "Saving model (epoch = 63743, loss = 0.7746)\n",
            "Saving model (epoch = 64073, loss = 0.7745)\n",
            "Saving model (epoch = 64200, loss = 0.7744)\n",
            "Saving model (epoch = 64389, loss = 0.7743)\n",
            "Saving model (epoch = 64419, loss = 0.7743)\n",
            "Saving model (epoch = 64459, loss = 0.7743)\n",
            "Saving model (epoch = 64544, loss = 0.7743)\n",
            "Saving model (epoch = 64563, loss = 0.7743)\n",
            "Saving model (epoch = 64747, loss = 0.7742)\n",
            "Saving model (epoch = 64769, loss = 0.7742)\n",
            "Saving model (epoch = 65070, loss = 0.7739)\n",
            "Saving model (epoch = 65633, loss = 0.7737)\n",
            "Saving model (epoch = 65935, loss = 0.7736)\n",
            "Saving model (epoch = 66436, loss = 0.7736)\n",
            "Saving model (epoch = 66530, loss = 0.7735)\n",
            "Saving model (epoch = 66542, loss = 0.7734)\n",
            "Saving model (epoch = 66715, loss = 0.7734)\n",
            "Saving model (epoch = 66768, loss = 0.7734)\n",
            "Saving model (epoch = 66971, loss = 0.7733)\n",
            "Saving model (epoch = 67033, loss = 0.7733)\n",
            "Saving model (epoch = 67218, loss = 0.7732)\n",
            "Saving model (epoch = 67237, loss = 0.7730)\n",
            "Saving model (epoch = 67885, loss = 0.7729)\n",
            "Saving model (epoch = 68085, loss = 0.7728)\n",
            "Saving model (epoch = 68373, loss = 0.7728)\n",
            "Saving model (epoch = 68390, loss = 0.7727)\n",
            "Saving model (epoch = 68422, loss = 0.7726)\n",
            "Saving model (epoch = 68666, loss = 0.7726)\n",
            "Saving model (epoch = 68912, loss = 0.7725)\n",
            "Saving model (epoch = 68968, loss = 0.7725)\n",
            "Saving model (epoch = 69016, loss = 0.7724)\n",
            "Saving model (epoch = 69078, loss = 0.7723)\n",
            "Saving model (epoch = 69173, loss = 0.7722)\n",
            "Saving model (epoch = 69489, loss = 0.7721)\n",
            "Saving model (epoch = 69957, loss = 0.7720)\n",
            "Saving model (epoch = 70095, loss = 0.7720)\n",
            "Saving model (epoch = 70138, loss = 0.7720)\n",
            "Saving model (epoch = 70588, loss = 0.7720)\n",
            "Saving model (epoch = 70635, loss = 0.7719)\n",
            "Saving model (epoch = 70755, loss = 0.7718)\n",
            "Saving model (epoch = 70921, loss = 0.7718)\n",
            "Saving model (epoch = 70961, loss = 0.7717)\n",
            "Saving model (epoch = 70976, loss = 0.7717)\n",
            "Saving model (epoch = 71054, loss = 0.7716)\n",
            "Saving model (epoch = 71189, loss = 0.7716)\n",
            "Saving model (epoch = 71390, loss = 0.7716)\n",
            "Saving model (epoch = 71517, loss = 0.7716)\n",
            "Saving model (epoch = 71542, loss = 0.7715)\n",
            "Saving model (epoch = 71639, loss = 0.7712)\n",
            "Saving model (epoch = 72327, loss = 0.7712)\n",
            "Saving model (epoch = 72395, loss = 0.7712)\n",
            "Saving model (epoch = 72636, loss = 0.7710)\n",
            "Saving model (epoch = 72883, loss = 0.7710)\n",
            "Saving model (epoch = 73270, loss = 0.7709)\n",
            "Saving model (epoch = 73437, loss = 0.7709)\n",
            "Saving model (epoch = 73449, loss = 0.7708)\n",
            "Saving model (epoch = 73655, loss = 0.7705)\n",
            "Saving model (epoch = 74578, loss = 0.7705)\n",
            "Saving model (epoch = 74769, loss = 0.7704)\n",
            "Saving model (epoch = 74836, loss = 0.7704)\n",
            "Saving model (epoch = 75084, loss = 0.7703)\n",
            "Saving model (epoch = 75092, loss = 0.7702)\n",
            "Saving model (epoch = 75186, loss = 0.7702)\n",
            "Saving model (epoch = 75489, loss = 0.7701)\n",
            "Saving model (epoch = 75564, loss = 0.7700)\n",
            "Saving model (epoch = 76047, loss = 0.7700)\n",
            "Saving model (epoch = 76071, loss = 0.7700)\n",
            "Saving model (epoch = 76522, loss = 0.7699)\n",
            "Saving model (epoch = 76856, loss = 0.7699)\n",
            "Saving model (epoch = 76860, loss = 0.7699)\n",
            "Saving model (epoch = 76870, loss = 0.7698)\n",
            "Saving model (epoch = 77114, loss = 0.7698)\n",
            "Saving model (epoch = 77138, loss = 0.7697)\n",
            "Saving model (epoch = 77150, loss = 0.7696)\n",
            "Saving model (epoch = 77223, loss = 0.7695)\n",
            "Saving model (epoch = 77300, loss = 0.7695)\n",
            "Saving model (epoch = 77512, loss = 0.7695)\n",
            "Saving model (epoch = 77653, loss = 0.7694)\n",
            "Saving model (epoch = 77822, loss = 0.7694)\n",
            "Saving model (epoch = 78031, loss = 0.7694)\n",
            "Saving model (epoch = 78101, loss = 0.7693)\n",
            "Saving model (epoch = 78168, loss = 0.7693)\n",
            "Saving model (epoch = 78183, loss = 0.7693)\n",
            "Saving model (epoch = 78201, loss = 0.7693)\n",
            "Saving model (epoch = 78308, loss = 0.7693)\n",
            "Saving model (epoch = 78345, loss = 0.7692)\n",
            "Saving model (epoch = 78568, loss = 0.7692)\n",
            "Saving model (epoch = 78783, loss = 0.7692)\n",
            "Saving model (epoch = 78965, loss = 0.7690)\n",
            "Saving model (epoch = 79168, loss = 0.7689)\n",
            "Saving model (epoch = 79213, loss = 0.7689)\n",
            "Saving model (epoch = 79290, loss = 0.7689)\n",
            "Saving model (epoch = 79347, loss = 0.7688)\n",
            "Saving model (epoch = 79439, loss = 0.7687)\n",
            "Saving model (epoch = 79506, loss = 0.7687)\n",
            "Saving model (epoch = 79661, loss = 0.7686)\n",
            "Saving model (epoch = 80325, loss = 0.7685)\n",
            "Saving model (epoch = 80824, loss = 0.7684)\n",
            "Saving model (epoch = 80829, loss = 0.7684)\n",
            "Saving model (epoch = 80849, loss = 0.7684)\n",
            "Saving model (epoch = 81117, loss = 0.7683)\n",
            "Saving model (epoch = 81436, loss = 0.7683)\n",
            "Saving model (epoch = 81570, loss = 0.7682)\n",
            "Saving model (epoch = 81638, loss = 0.7681)\n",
            "Saving model (epoch = 82052, loss = 0.7680)\n",
            "Saving model (epoch = 82232, loss = 0.7680)\n",
            "Saving model (epoch = 82403, loss = 0.7680)\n",
            "Saving model (epoch = 82433, loss = 0.7680)\n",
            "Saving model (epoch = 82507, loss = 0.7679)\n",
            "Saving model (epoch = 82645, loss = 0.7678)\n",
            "Saving model (epoch = 82878, loss = 0.7678)\n",
            "Saving model (epoch = 83229, loss = 0.7676)\n",
            "Saving model (epoch = 83288, loss = 0.7676)\n",
            "Saving model (epoch = 83362, loss = 0.7676)\n",
            "Saving model (epoch = 83786, loss = 0.7675)\n",
            "Saving model (epoch = 84084, loss = 0.7674)\n",
            "Saving model (epoch = 84604, loss = 0.7672)\n",
            "Saving model (epoch = 84635, loss = 0.7672)\n",
            "Saving model (epoch = 85046, loss = 0.7672)\n",
            "Saving model (epoch = 85260, loss = 0.7671)\n",
            "Saving model (epoch = 85348, loss = 0.7670)\n",
            "Saving model (epoch = 85659, loss = 0.7669)\n",
            "Saving model (epoch = 86001, loss = 0.7669)\n",
            "Saving model (epoch = 86193, loss = 0.7668)\n",
            "Saving model (epoch = 86322, loss = 0.7668)\n",
            "Saving model (epoch = 86412, loss = 0.7668)\n",
            "Saving model (epoch = 86448, loss = 0.7668)\n",
            "Saving model (epoch = 86621, loss = 0.7668)\n",
            "Saving model (epoch = 86697, loss = 0.7668)\n",
            "Saving model (epoch = 86833, loss = 0.7667)\n",
            "Saving model (epoch = 86997, loss = 0.7666)\n",
            "Saving model (epoch = 87092, loss = 0.7665)\n",
            "Saving model (epoch = 87359, loss = 0.7665)\n",
            "Saving model (epoch = 87520, loss = 0.7664)\n",
            "Saving model (epoch = 87971, loss = 0.7663)\n",
            "Saving model (epoch = 88147, loss = 0.7663)\n",
            "Saving model (epoch = 88283, loss = 0.7662)\n",
            "Saving model (epoch = 88456, loss = 0.7662)\n",
            "Saving model (epoch = 88502, loss = 0.7661)\n",
            "Saving model (epoch = 89169, loss = 0.7659)\n",
            "Saving model (epoch = 89793, loss = 0.7659)\n",
            "Saving model (epoch = 89804, loss = 0.7658)\n",
            "Saving model (epoch = 89813, loss = 0.7658)\n",
            "Saving model (epoch = 90207, loss = 0.7657)\n",
            "Saving model (epoch = 90255, loss = 0.7657)\n",
            "Saving model (epoch = 90876, loss = 0.7657)\n",
            "Saving model (epoch = 90982, loss = 0.7655)\n",
            "Saving model (epoch = 91226, loss = 0.7654)\n",
            "Saving model (epoch = 91435, loss = 0.7654)\n",
            "Saving model (epoch = 91481, loss = 0.7653)\n",
            "Saving model (epoch = 91924, loss = 0.7651)\n",
            "Saving model (epoch = 92731, loss = 0.7650)\n",
            "Saving model (epoch = 93240, loss = 0.7649)\n",
            "Saving model (epoch = 93586, loss = 0.7648)\n",
            "Saving model (epoch = 93972, loss = 0.7648)\n",
            "Saving model (epoch = 94297, loss = 0.7647)\n",
            "Saving model (epoch = 94683, loss = 0.7645)\n",
            "Saving model (epoch = 94826, loss = 0.7645)\n",
            "Saving model (epoch = 95347, loss = 0.7645)\n",
            "Saving model (epoch = 95656, loss = 0.7643)\n",
            "Saving model (epoch = 96276, loss = 0.7643)\n",
            "Saving model (epoch = 96313, loss = 0.7641)\n",
            "Saving model (epoch = 97100, loss = 0.7640)\n",
            "Saving model (epoch = 97219, loss = 0.7638)\n",
            "Saving model (epoch = 97830, loss = 0.7638)\n",
            "Saving model (epoch = 97976, loss = 0.7637)\n",
            "Saving model (epoch = 97995, loss = 0.7637)\n",
            "Saving model (epoch = 98040, loss = 0.7637)\n",
            "Saving model (epoch = 98620, loss = 0.7636)\n",
            "Saving model (epoch = 98767, loss = 0.7636)\n",
            "Saving model (epoch = 98853, loss = 0.7635)\n",
            "Saving model (epoch = 99173, loss = 0.7634)\n",
            "Saving model (epoch = 99261, loss = 0.7634)\n",
            "Saving model (epoch = 99657, loss = 0.7634)\n",
            "Saving model (epoch = 99715, loss = 0.7633)\n",
            "Saving model (epoch = 99810, loss = 0.7633)\n",
            "Saving model (epoch = 99986, loss = 0.7632)\n",
            "Finished training after 100000 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ml3n_r2g0HMp",
        "outputId": "04f5a99b-a0e1-43a8-eb8f-426bcad59153"
      },
      "source": [
        "plot_learning_curve(model_loss_record, title='deep model')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3wU1RbA8d9JISGEJihKEdCnBEFBQAWxADYsT58VC/p8Fp5iwV6w4bN3RRTE8lBUUFREEFCfgA1QQREpoQcSOimkt937/tjZZXs2ZbPJeL6fTz7Znbkzc3Z29+zMvXfuiDEGpZRS9hMX6wCUUkpFhyZ4pZSyKU3wSillU5rglVLKpjTBK6WUTWmCV0opm9IEr+qEiJwoImtiHUdDISIDRWSdiBSKyD8iKD9JRB6vj9jqi4gsEJHrIixrRORv0Y7pr0YTvA2ISIaInBrLGIwxPxhjusUyhgbmP8A4Y0yqMebzWAej/po0wauIiEh8rGOorXp+DZ2BlfW4PaUCaIK3MRGJE5H7RGSDiGSLyMcisp/X/GkiskNE9orI9yLSw2veJBEZLyKzRaQIGGydKdwlIsutZT4SkWSr/CARyfJaPmRZa/49IrJdRLaJyHXhTtFFZD8R+a9VNldEPremXy0iP/qV9awnyGu4y3q98V7lzxeR5ZHsryBxXS8i60UkR0S+EJH21vQNwCHATKuKJinIskeLyG8iUiAiHwHJfvPPEZFlIpInIgtF5Civee1F5FMR2S0im0TkVq95Y0TkE2t/F1jb6BXmNRgRGWlVJxWIyGMicqi1zXxrHzSp6jVb804TkXTr/R4HiN+2rhGR1dZ7+JWIdA4Vl6ojxhj9a+R/QAZwapDpo4DFQEcgCXgDmOI1/xqguTXvZWCZ17xJwF5gIK4DgWRrO78A7YH9gNXADVb5QUCWX0yhyg4FdgA9gBTgfcAAfwvx+r4EPgJaA4nAydb0q4Ef/cp61hPiNWwATvMqPw24L5L95bedIcAeoI9V9lXg+6reE2teE2AzcLv1ei4CKoDHrflHA7uA44B44J/W+pKs17EUeNhazyHARuAMa9kx1roustZ9F7AJSAwRiwFmAC2s96MM+NZab0tgFfDPql4z0BYo8Nru7UAlcJ01/zxgPdAdSAAeBBYGe9/0rw5zQ6wD0L86eBNDJ/jVwClezw+yvvwJQcq2sr5kLa3nk4D3gmxnuNfzZ4EJ1uNBBCb4UGXfAZ7ymve3UF9wK2Yn0DrIvKupOsH7v4bHgXesx82BIqBzDfbX28CzXs9TrbJdwr0n1ryTgG2AeE1byL4EPx54zG+ZNcDJuJL+Fr959wP/tR6PARZ7zYsDtgMnhojFAAO9ni8F7vV6/gLwclWvGbjKb7sCZLEvwc8BrvWLq9hr32uCj8KfVtHYW2dgunWan4crgTmAdiISLyJPW9UR+bgSEriOxNwyg6xzh9fjYlxf8lBClW3vt+5g23HrBOQYY3LDlAnHf90fAhdY1SYXAL8ZYzZb80LuryDrbY/rKBwAY0whkA10iCCm9sBWY2U2y2avx52BO91xWLF0spbrDLT3mzfaL0bPazbGOHEl2vaEttPrcUmQ597vW6jX7POeWq/Ne993Bl7xijkH149AJPtL1VBCrANQUZUJXGOM+cl/hohcieu0+VRcyb0lkItvvWm0hhrdjqsaxK1TmLKZwH4i0soYk+c3rwhXFQ8AInJgkOV9XoMxZpWIbAbOBC7HlfC9txV0fwWxDVfScm+7GdAG2BrBstuBDiIiXkn+YFzVR+44njDGPOG/oIgMADYZYw4Ls/5OXuXjcO3rbRHEVZVwr3m733YF3/fV/Zo+qIM4VIT0CN4+EkUk2esvAZgAPOFuzBKR/UXkPKt8c1z1rdm4kuST9Rjrx8C/RKS7iKQAD4UqaIzZjuv0/nURaS0iiSJykjX7D6CHiPS2GnDHRLj9D3HVt5+Eqw7eLdz+8jfFeg29rbOBJ4GfjTEZEWx/Ea766Vut13MBcKzX/DeBG0TkOHFpJiJni0hzXO0aBSJyr4g0tc7EeorIMV7L9xWRC6zPwG243ufFEcRVlXCv+Utc74V7u7cC3j+4E4D7xWrIF5GWInJxHcSkwtAEbx+zcZ1Ou//GAK8AXwBfi0gBri/5cVb593Cdbm/F1ZBWFwkgIsaYOcBYYD6uhjf3tstCLHIlrrredFyNj7dZ61mLq7/5/4B1wI8hlvc3BVd99jxjzB6v6eH2l/9r+B+uH6ZPcR29HgpcGsnGjTHluKqHrsZVVTEM+Mxr/hLgemAcrrOq9VZZjDEO4BygN67G0z3AW7jOwNxmWOvMxbXvLjDGVEQSWxVxh3zN1n68GHga10HDYcBPXstOB54BplpVgitwnUWpKBLfakCl6p+IdMf1hU8yxlTGOp7GTETG4GqsHB7rWFTs6RG8iglx9T9PEpHWuI7sZmpyV6puRTXBi+tilz+tCzaWRHNbqtH5N67qlg24eqrcGNtwlLKfqFbRiEgG0M+vnlMppVQ90CoapZSyqWgfwW/C1ZJvgDeMMRODlBkBjABo2rRp306dwnWJDi6z0tC0rJS2zZrWMuLocTqdxMU13N9Tja92NL7a0fhqbu3atXuMMfsHnRnNy2SBDtb/A3D1WT4pXPm+ffuamuj98Vwz/MlXarRsfZk/f36sQwhL46sdja92NL6aA5aYWAxVYIzZav3fBUzH92IOpZRSURS1BG9dfdfc/Rg4HVdf52htMGqrVkqpxiiaY9G0wzVwk3s7Hxpj5kZjQ6LXaimlVICoJXhjzEYg5I0GlFKqLlRUVJCVlUVpaWnUttGyZUtWr14dtfVHIjk5mY4dO5KYmBjxMjqapFKqUcvKyqJ58+Z06dIFiVJVbUFBAc2bN4/KuiNhjCE7O5usrCy6du0a8XINs99PDRi0Dl6pv6LS0lLatGkTteTeEIgIbdq0qfZZij0SvA6YptRfmp2Tu1tNXqM9ErxSSqkAmuCVUqoW8vLyeP3116u93FlnnUVenv9NyuqWbRK8sf8ZmlKqAQqV4Csrw49+PXv2bFq1ahWtsACb9KKRqN06VCmlwrvvvvvYsGEDvXv3JjExkeTkZFq3bk16ejpr167lH//4B5mZmZSWljJq1ChGjBgBQJcuXViyZAmFhYWceeaZnHDCCSxcuJAOHTowY8YMmjat/dhatkjwSikFsOPJJylbnV6n60zqnkazW24JOf/pp59mxYoVLFu2jAULFnD22WezYsUKT3fGd955h/3224+SkhKOOeYYLrzwQtq0aeOzjnXr1jFlyhTefPNNLrnkEj799FOGD6/9Tblsk+DNX6AVXSnV8B177LE+fdXHjh3L9OnTAcjMzGTdunUBCb5r16707t0bgL59+5KRkVEnsdgiwetQBUopgANHj47KegsKCiIu26xZM8/jBQsW8L///Y9FixaRkpLCoEGDgvZlT0pK8jyOj4+npKSkdgFbbNPIqpRSsdC8efOQPwB79+6ldevWpKSkkJ6ezuLFi+s1NlscwSulVKy0adOGgQMH0rNnT5o2bUq7du0884YOHcqECRPo3r073bp1o3///vUam40SvNbBK6Vi48MPPww6PSkpiTlz5gSd565nb9u2LStW7BtJ/a677qqzuGxRRaPdJJVSKpAtErxSSqlAmuCVUsqmbJPgdagCpZTyZY8Er8MFK6VUAHskeKWUUgFsk+D1jk5KqYZizJgxPP/887EOwx4JXocqUEqpQLZI8EopFWtPPPEEhx9+OCeccAJr1qwBYMOGDQwdOpS+ffty4oknkp6ezt69e+ncuTNOpxOAoqIiOnXqREVFRZ3HZKMrWZVSf3UPrctiRWHdDNTl1jO1Kfcc2DJsmaVLlzJ16lSWLVtGZWUlffr0oW/fvowYMYIJEyZw2GGH8fPPPzNy5EjmzZtH7969+e677xg8eDCzZs3ijDPOIDExsU7jBhsleB0uWCkVKz/88APnn38+KSkpAJx77rmUlpaycOFCLr74Yk+5srIyAIYNG8ZHH33E4MGDmTp1KiNHjoxKXLZI8DpUgVIK4LHDOkZlvdUZLtjN6XTSqlUrli1bFjDv3HPPZfTo0eTk5LB06VKGDBlSF2EG0Dp4pZSqpZNOOonPP/+ckpISCgoKmDlzJikpKXTt2pVp06YBYIzhjz/+ACA1NZVjjjmGUaNGcc455xAfHx+VuDTBK6VULfXp04dhw4bRq1cvzjzzTI455hgAPvjgA95++2169epFjx49mDFjhmeZYcOG8f777zNs2LCoxWWLKhqllIq1Bx54gAceeCBg+ty5c4OWv+iiizBRvgrfHkfwWgWvlFIB7JHglVJKBbBNgtdukkr9dUW7qqMhqMlrtEWC126SSv11JScnk52dbeskb4whOzub5OTkai2njaxKqUatY8eOZGVlsXv37qhto7S0tNrJta4lJyfTsWP1+vlrgldKNWqJiYl07do1qttYsGABRx99dFS3EQ22qKIBvaOTUkr5i3qCF5F4EfldRGZFbRs2rntTSqmaqo8j+FHA6nrYjlJKKS9RTfAi0hE4G3grmttRSikVSKLZtUhEPgGeApoDdxljzglSZgQwAqBdu3Z9p06dWu3t3J+VT8ddO7ipz+G1jDh6CgsLSU1NjXUYIWl8taPx1Y7GV3ODBw9eaozpF2xe1HrRiMg5wC5jzFIRGRSqnDFmIjARoF+/fmbQoJBFQ29r8hcA1GTZ+rJgwQKNrxY0vtrR+GqnoccXSjSraAYC54pIBjAVGCIi70dxe0oppbxELcEbY+43xnQ0xnQBLgXmGWOGR217OlSBUkr5sEk/eO0mqZRS/urlSlZjzAJgQX1sSymllItNjuCVUkr5s02C10oapZTyZYsEr0MVKKVUIFskeKWUUoE0wSullE3ZJsFrP3illPJliwSvqV0ppQLZIsErpZQKZJ8Er1U0SinlwxYJXrtJKqVUIFskeKWUUoE0wSullE3ZJsFrJY1SSvmySYLX9K6UUv5skuCVUkr50wSvlFI2ZZsEr0MVKKWUL1skeNEqeKWUCmCLBK+UUiqQbRK80SHHlFLKhy0SvA5VoJRSgWyR4JVSSgXSBK+UUjZlnwSvVfBKKeXDFgledKgCpZQKYIsE7+bIz491CEop1WDYKsGb8vJYh6CUUg2GbRK8DlWglFK+7JHgtQpeKaUC2CPBu+kFT0op5WGbBK9DFSillC9bJHjtJqmUUoFskeDdjFbRKKWUh60SvFJKqX2iluBFJFlEfhGRP0RkpYg8Gq1tARitgldKKR8JUVx3GTDEGFMoIonAjyIyxxizuK43pMMFK6VUoKgleOOqEC+0niZaf9HNxJrnlVLKQ6LZMCki8cBS4G/Aa8aYe4OUGQGMAGjXrl3fqVOnVns7/9mwi5TSEh4s3IGJi6PsmGNqGXndKywsJDU1NdZhhKTx1Y7GVzsaX80NHjx4qTGmX9CZxpio/wGtgPlAz3Dl+vbta2pi8FsfmbPHvWtWdUszq7ql1Wgd0TZ//vxYhxCWxlc7Gl/taHw1BywxIXJqtRpZRSRORFpU9xfGGJNnJfih1V02EqJVM0opFaDKBC8iH4pICxFpBqwAVonI3REst7+ItLIeNwVOA9JrG7BSSqnIRHIEf4QxJh/4BzAH6ApcGcFyBwHzRWQ58CvwjTFmVo0jrYKOJqmUUr4i6UWTaHVz/AcwzhhTIVJ1pYgxZjlwdG0DjIzW0SillL9IjuDfADKAZsD3ItIZ0FsnKaVUA1flEbwxZiww1mvSZhEZHL2Qas84HABIfHyMI1FKqdiJpJF1lNXIKiLytoj8Bgyph9iqxXuognUnD2LtscfFLhillGoAIqmiucZqZD0daI2rgfXpqEZVTf5DFTj27MFZVBSjaJRSqmGIJMG7j43PAiYbY1Z6TVNKKdVARZLgl4rI17gS/Fci0hxwRjcspZRStRVJgr8WuA84xhhTDDQB/hXVqGpAb9lXf4p++YWtd9ypN1hRqoGrMsEbY5xAR+BBEXkeON7q495gVJXajTFk/3cSFTt31Us8dpd5/QjyZ8/GlJXFOhSlVBiR9KJ5GhgFrLL+bhWRJ6MdWF2q2LyZXc88Q9Ytt8Q6FKVqrDI3l8rc3FiHoRqRSK5kPQvobR3JIyLvAr8Do6MZWLWFGarA3S/eWVgYsoxSDd26AccD0D19dYwjUY1FpKNJtvJ63DIagdRGqDs6rU7rTsmyZfUcjVKxkT93LpsuuFDbRpRHJAn+KeB3EZlkHb0vBZ6Iblh1J2fy+0GnO/buredIlIqurXfcSemqVeDUTm7KJZJG1ilAf+Az4FNggDHmo2gHFk1Fv/zC2uP6UzBvXqxDUUqpqAlZBy8iffwmZVn/24tIe2PMb9ELq/rCnpT6nbKW/vknAMVLltJ8SPRGXdg7cxaO/L006dgRjME4HFTuySax3QFR26ZSSrmFa2R9Icw8Q0MajybSKscojRnvyMuDhETiU5v5TN929777oiT/8yp2/fwLOZMmcdjCn0jYb7+oxKKUUm4hE7wxpkGPGBmxemhwWtt/AHEpKXT7bWnIMnF5eRT+4bp8wJG3VxO8UirqqnVP1saovgYdcxYX1/k6jdNJ0S+/RFy+YudONl0yjMrs7DqPRTUeFdu3Y7ShVWGjBB/qln2F333H3pkh7hRYx0f32x8ZU6fry3nnHbZc9U8Kf/gxovK5kydTunw5eZ99VqdxqMZlw6mnsWf8+HrbXsmff5L/9df1tr3Gpjwzk4pdsbmK3hYJXqqohM9+442AJQCKFi0CXEMZOMvKyJ02LWgf4rzpn1NiNcyuH3IKuVOnBt1O3kcfuT7sc78KEmT16//LNm4CoHLXzmovG1XWPqrcvTviH5/6ljP5fbJG3RaTbRuHIzZ90b0+Y8WLf663zWZcfAlbbx1Vb9trbDacdjrrTzo5JtsOmeBFZLjX44F+826OZlD1pSw9HVNZSXr3I1jTqzc7HnqYgq+/8cx3lpZSvmUL2++/n4yLLwGgYts2dox5FGeIcVgyLr6ErbeFTywbzzqLrFtupWTFyrp7MXWsdNUqVqd1p+jn0FVEGZcMI/P66+sxqsjtfOIJCr4K8kMbZRU7d5Heoyd5H0+LeJmyjRtxlpZGMSr1VxXuCP4Or8ev+s27Jgqx1Eoko0mWb9kSMC13iu/RuPdwBltH3caG08/Ytw2ves01vXpjKiqqF6TXEVbBN9+QcdFFlK1bF7q8dRS4/YEHPZPKs7JCtisU/7rEWs71Q7TtwQerH6Mlf85cAAqDXCtgyssBcOi4KAHKMzIAyJ8VolrQj7O4mI1nnc22u++JYlSxsfWuu8l5991Yh9HgFcybR+H330dl3eESvIR4HOx5TIUaqiBAkGS384nQF+UW/vCDz/PdL77o87x6yTP4Ltv59DPVWIerfnXLtdf5TCv84QcK5s2j5I8/PNO2PfAAez/5lOJff8VZXIyjsJDdY8ey8e9/p2T5ctafcQaOwtAN0Nlvvgm4XqOprPQk9WjZO3MmlTk5Ud1GpJxRfq3e3Pu1Oo3pEYlSl+DqyJ81i51PNaibvzVIWSNvInPEv6Oy7nD94E2Ix8GeNy6Rfvj9eiJkv/V27bYb5Ieo6Kefqr0a//F1Mq8fEbb82oEnYEpKPM93vfgSFZu3UPrncpoNGFDF1gwZwy6ldOXKqA1yVbFzp+cINm3lipA3SzeVleRNm0Zix04k9+xBQuvWdR5LybJlZFx6GZ3enEjqiSdWfwV+77EpL8dZXkHxkl9JaNOGpkceWUeRqkjtfnUc5Zs30+H552IdSr0LdwSfJiLLReRPr8fu593qKb46V7J8ORVZmSHn73oh3PVdkauTxkevZLF35kwqd++u9nKAT3KvidKVddNW4MjPD3rWYyoqPY+dJaHronMmv8+OR/9D5vXXk3XDjXUSk7/ipa4LtIsWLgpbzllURPpRvShYsMBvjrXvrYOIzVf9k7X9+pF1w42edhwVHUULFwYde2rPa69FXGXmtnfGDLbaoNosXILvDvwdOMfrsfv5EdEPrZoiPCjPuGQYuR9OCTnfkZPDzqeeZnVa9yrXVbFtG1tCHD0HND4KVZ45VGZnYyorg87bdvc9IbcVVD2eY+168SUAytato/DH4GckzvJy1h57HFvvvKta612d1p2ihQsBcOTvGyCubOPGWkTsK/+rr6s9LlHhDz9iysvZ+eRTwQtY73W9jmbq/flqAFU09W3LNdeGrXKtjm333kf+zJl1sq5YCpngjTGbvf+AQqAP0NZ63mDU9Uc50oahjef8nSK/evpQpNJB+aZNQecVLVrEhjOGsm7gCazp0zdkg0ulV1/a/Llfsf3hR0L+EBUvXmxtOHDvVO6s226X7v218e/nknnddQHzjTGsOaoXAAVB+kv7h1ixcxc5H3xA9sSJAGS/89+Q2y7PyMBRUFDT0Cn8/nu2jhpF1sibqrXcntfGuWL1b7i3yVC9xuFg9+uve0Zd3fPmm3V+nUckHAUF7Hzq6XptF7GTcN0kZ4lIT+vxQcAKXL1nJotIbDoYh1CWmEhpk6RYhxFWapijge0PPUz5ZtdvpikvJ3PEvynbuJG9n3/uU87h1Qi59bbbyPv446Dr8+5lU/DN/wLme35oQhzl7R47Nuj0nc89F/QKSVNWFvLMIyJ+cWTdfDM7H3s87CLOggLyPv2UDUPPJOOyy8KWNcaw8+lnKArSNzxajVsxP4Ku5fbzZ81iz9hXWXtcfwB2v/AieR8FDiLrvj6kpnI++CBo7za33a+MJefdd9k7/fOQZYIxTieOvDycNaierO01DDnvvRf0THbXiy/V7ntSA+GqaLoaY1ZYj/8FfGOM+TtwHA2sm+SqQw5nbedDYh1Gndp41tk1XtZ95AuQ++GH1V5+z+vBr4LMefsd9rz2etB52x9+xPM479PP2HLNtTjy84OWLfRrWN56x50+zx15eT7Pi378EQnSNdTdfbR8/QbA1Qgb7MuZPfFNciZNYsvVVweNx1ctj8BrmByce/eyOq075VlZVReORC1/XyJNjDVpV3AWF1OyYiXOkhJ2PvY4m4dfGVCmZNkySv5csa/NxgQeWJSuWUtJiPah3a++ytr+A1hztP+guFXz//wBNJs9O+QFjv52PvlU0DPZ7IkTya/nazPCJXjv1rBTgNkAxpgCQAe6qEMVdfWlrqa8Tz+r9tHKntdeCzo9f84cz+PtDzxA0cKFrD32OMCVvLxl+nXz9K6nNuVlVGQGNoLv91SIum7L2uMHkt7zSNK7HxFwVJk/e7bnsftMqUo1PAL27E+pxra8eHd1dSvPzKQsRPVefXOWl1OybBnZkyYFvdgvkq6uW++6m4yLLsKR76paC1bFlnHpZWRcfHHQ5Xe98CI7n36GTeedR8aFFwUtUxDsavJaSP1iJjvGPFr7FVm3D60v4RJ8pojcIiLn46p7nwsgIk2BxPoILlLHrPyDtE3rYx1Go5M/cya5kyezOq07yVG8tN198U8ksm4OfmP0hD3hB1Dzrr7adte+YZqNMT5VVrte2Hctg3fiB3CWlWEcoY9dyrOyXLeBDJKE3dw/XsWLFrPhjKEhyxX+9FPIo09/G047nY1nnhVR2UiVLFvG6rTulK6uXtfXNUf1IuPSy9j19DOs6dXbZ17R4sWsO34gBfPm+0z3v4bCvf9MefCrwauS/abrjKxRquc2mnAJ/lqgB3A1MMwY4z5v6Q+EbvWKgTjjbGCXXjUe7l4gzb78EoAtI6rRU8dLdbti7n7dVdWz65VXfKaX/Bb6PjKmLLKGNu8j591jx/qcIRV8/bXnAi//qqE1vXoHXMzmlnXrKLbdex/gOvPx5m7ors7ZUOa114U8+qyJPW++CV71u1LFGUj+N64hOWpyHUYoJctdZ04lv/u+h+lWA7ubu9rFu7FfCgrY/tBDgWcFNUyIoTo0VJepxhF34U8/4fC6Ej4StekgEIlwvWh2GWNuMMacZ4z52mv6fGPM81GNqprEGJxii3HTYq7oe99eQeG6lEYqWIPTnrGvYowhe/yEiNeTO6X6sQRbf8ZFVSfWsvXrfPrsF3z9NSVLvcf7D0ygez+fUeV68z6bTsbw4aELWPmsfPNmCubPD1nMUVjk6SrqLC1l9wvBf5iMMZ6hrJ3FxZSuXcuul172DEaW+/G0gB4qzrIy9ni149Q1p9U2s+PRfVUezadPJ2/aJ+TP+tKnbN4095g+kR/B+Z+ZuVVs21btK7L9R2YNVe1WsWsXmddex7YqugFvu/c+cqftG6fIGasELyJfhPuLalTVJMaEHC5YRciYqBxNpPfpy55x44LO237f/dVaV20v2HKLpMqo6Psf2Pmc68pH/wHXXBec+R5ZJmRlRdT9dPvo0ZQs2fdDEdAQbR2xbjhjKFk3jgy5ni3XXuNpiHf3dPHmyC+gYvt2cj/8kDV9+tJ03jzW9OnLpnPPI/uNNzwXr1Vs2cKe11+nbONG0o88ivKsLLLfmEjltu2edWVcfkWVr8tn23uDN677K1vnVa3q9L1AzMPvCL5yz56A9ex+dRx7vXqp+Z+Zua0fckq1u3rmTPLtMr3p/Au8QtsXm7EGi/O+PmNNv2OCrjPvk0+qFUNthBuqYACQCUwBfqYBV4Jogq+9hD17WHvMsXW+XhPmRih7Z1R9xBstG84+p8oyJb8vI//rrwOGwi0McmTd5vEncNagest7MDuAkj+X0/Lv4WMrWbaMUuvuYOWbN2OCNHaWrljB+sFDSDnO1dCd8n3o6zXypn5E6cpVmIoK8mfPCRjMLly1mTd343jexx8HHPk68vKIa9myyqqjqrgvevMWquE/mMLvvgs6vfjXX9l85VV0mTaNuJSmIZd3nw1VbN3K+lNOJblnT7p+EnzkUGc1q2uiIVyCPxA4DbgMuBz4EphijGlwY9y6qmg0wavIlW/YUGWZ0j//rNY459l1UK2R+95k4pJDJxhw9TBxC9eQC1D8c9WN5468PM8Fe6HaICLhLPMaZsKvv/fa/gM44J57aHPNvwKWi+TMbMeYMSR3T6txbPs2FrxOf4fVS6v458XkTt3X3798wwY2/8s3ZmMMZdbnp3TFirD9+KsMJ8p33gpXB+3NS5UAAB7OSURBVO8wxsw1xvwTV8PqemBBpGPBi0gnEZkvIqtEZKWIRO2OAHHG0IBPMJSqlrr4ofCXsGNHna/TX8XWbWHn73r2WUrXrA06TyIYDz+zmlcbB+PIzSXvs+mULF/uM71slas3kbO8PKDbcvGixT7PC77yvRo7XENszvsfBEwr/WM5O59+htypU9lw6mnVir+6wh3BIyJJwNm4juK7AGOB6RGuuxK40xjzm4g0B5aKyDfGmFW1iDd4nMbgjNMEr1Qs+TZCB7fpvPOCTk+2roWo3LUz9NjoddTFcPvo0QB0T19N9qRJJOy/v2fenrH+t74I5MjNIS6lg+d55e7dxDVr5nrsd2u+nY8HvyK7vrp5hkzwIvIe0BPXBU6Pel3VGhFjzHZgu/W4QERWAx2AqCR4rYNXdpRXzUv0G7vdL78Scp4jJ4dt99xbZ9ty5OWxq5r3YwDY8eh/6DRx321Ac6dM8dwJLlh7SCyFO4IfDhQBo4BbvRpHBDDGmBaRbkREugBH42qs9Z83AhgB0K5dOxYEDL8awfqNieiOTko1Ntvvr15PIxW5tf2ruhdCaMuXL8d9N4IC6+5ntVWT3FeVkAneGFMnHctFJBX4FLjNGBPQf8oYMxGYCNCvXz8zaNCgam/jjV9XYbSKRilVTzou+4PQ90OrmZrkvqpE9eogEUnEldw/MMZ8VlX5Gm9Hj+CVUvWo6Mc6uKFPPYhaghdXnc7bwGpjTM37XkWyLe0mqZRSAaJ5BD8QuBIYIiLLrL+6HTHJEmdM7MffVkqpBiZsN8naMMb8SD11TtcjeKWUCmSLEbq0m6RSSgXSBK+UUjZlmwSvVTRKKeXLFgleG1mVUiqQLRK8GKcewSullB+bJHgwekcnpZTyYYus6GpkjXUUSinVsNgjwaP3ZFVKKX+2yIpxzroZJ1oppezEFglejFNv+KGUUn7skeDRRlallPJni6woTidaSaOUUr7skeBBG1mVUsqPLbJinHHW07iVSinVeNgiwWP0CF4ppfzZIivGGaeOJqmUUn5skeBzWrSiIjEx1mEopVSDYosE/9WAkwEobJoS40iUUqrhsEWCd9NqGqWU2scWCf6yr2YA4IizxctRSqk6YYuMeOCe3QBUxkftHuJKKdXo2CLBlzdp4vqvDa1KKeVhiwQ//sLhAFzx2Cs6ZIFSSllskeCdXnXv/zv2hBhGopRSDYctEry33OYtYx2CUko1CLZL8HHGGesQlFKqQbBFgj/t5x88j3/ofUwMI1FKqYbDFgn+uBXLPI+XH9Y9hpEopVTDYYsEf8yqP3yeDx4/hWeHj4hRNEop1TDYIsG3KC4KmDZn4OAYRKKUUg2HLRK8UkqpQJrglVLKpmyT4NMy1gdMGzx+Cm+de0kMolFKqdizTYJ//ZmHgk7/4Mzz+fPQbvUcjVJKxV7UEryIvCMiu0RkRbS24bO9MPNuvWsMJUlJ9RGGUko1GNE8gp8EDI3i+gM8+dqzIeed99xEdrfarx6jUUqp2IpagjfGfA/kRGv9wQxY8XvIeRWJTbjkqdcYPH4KALtbtia7Rasq1/nwiNs9yyilVGMS8ztkiMgIYARAu3btWLBgQbXX0c7r8UX/+5JPTj07bPkH/30HP1lDGkwdfTOXPjmO0e+M47Rffwoo+8PRx1Y7HqWUqq6a5L6qiDHRG0FdRLoAs4wxPSMp369fP7NkyZJqb2d1mu/wBDU54j54+1YOyM0ms91BvPX4vUw8/3Iu/WYmVzz2CgDzb7wMgK+PPYFmpSUMXL6UwuSmfNe3P2f9ND9kG0BOi5aIMbQuyPdMK0hpRmJFBckV5dWOsyYccXFMOf1cLpw3h6blZfWyTaVU9XRPX12j5URkqTGmX9B5dkzwjrg4Tn3tg2qvx23QkkUs6DcgYHpKSTHFTVMA+Pj+m7jkqdcA6LFhDeOeHxNQ/vF/3cS3XuPTv/jSY0w5/Vx+7dGLpPIy5o662qd8Xmpzdrdqw/552Zz/3ERa783jsTde5IhN68I2IpcnJPL+mf/girmfk1RRETB/9oBBPHfVv7no29kUJyez/G/dmTzmDgAcIlQmJARdrjwhkcKmKexXsNczbdNBHUkuL+OgbNdtEg2wp9V+7J9Xr7VxQeW0aElu85YcunVLxMuUJySQ4HAQZ30PDDD19L9z+uIfaJOfF6VIfeW0aEnT0tKQP747W7ehdUE+TSoD36PGxBEXR2mTJJqVlsQ6lKja2boNlz45jglPPUC3LRsjXi4aCd423SS9xTudPDXumRovHyy5A57kDniSO8DKQ7sxePwU/n3fEwweP4VXL76Kyrh4n+QOcMftD/Frj14AlDVJYku79hjgu6OPpaRJEuc/N5ERDzzF+c9NBCC3ZStuvuc/DBk/heevuB6AouSmlDRJ4rrRT3HquMkMHj+FM159j8lnXcCzV97AjJNO5frRTzF4/BQGj5/CsCdeZWPHgwH45JSzmD1wCFntDmL6yafxW7cenPr6hwwd+17Aa13f4WDuu+keLnx2gs/0ax5+jssfH+t5PmfAIC556jVWdz7UM21Rz6MptPZVZVw8ZYmJPDt8BD8e1Zexl1zts770zofw/tB/UOZ1u8Wc5i1Z2fUwn3JliYlUxsWzqX1HBo+fwvY2+wfEfMWjL3Pdg89QnpDouQG7Aeb36U95QvDayDNencz9N93jKf9V/5OYeP7lPHr9KAqbpvBzj15UxMcDsLdZcx699laKkpsGXZe/je07Mf3k08ht3gKAivj4gNcFcOEzE7j57keDrsMh4qpCHHk3nww5s8o7luWnNPPEC7Cr9X7s2K9tRPECbGt7APP69o+4fHU8c9UNnPPSO2HLZLdoRaHf/p1w/uU++y3zgAO54JkJ7GodutOEgaCfkfrwc4/eAMw6wXe4lMU9ejN4/BQ2te+IQ4TyhOjfYjRqR/AiMgUYBLQFdgKPGGPeDrdMXR3Bu3113Ik8ffXIaq/vr+imj9/ltUv+GXRe7zUruePDt0guL/f5YQNok5dLdqvWAcv02LCGlSGuP3j7sXu49qFnuf2Dt3jpius806+fPoUBK37jmoeeA+CNJ+/n8MwM7ht5Dz8feTQ9NqzhiE3rmWa1sQxespD5/Y7n+ZcfpzClGWNG3O77mqa9x2sXX+V5PunROyltkkTLwgLinU4whkuefh2A8777mmu++JjzXngLgMSKCo7ckM5vaUcC8MibL/Pi5ddR0CyVMxZ9x0HZu+m8PYtHr7+Nrlu38M7j91IZF88fh3cnL7UFpyxZGLKq8NKvvuDfn0/hprsf5dRffmTspdf4zHdXBwIs7daDu2570PP81qn/pdvmDRyYvcfnzOqbYwbye7ceIcdgGjntPS6cPxcDxBnjc0ZYnpDAlgM7sKrL33j7vGHkpzbng4dG0X7PrqDr8leU3JSU0hIEV4JuUlnBE/+6iXWduvDx6Jv5vvex/Of6UUFfn9sng4fSYfdORt90D01LS7jr/Ymc8MdSmlRWePbjjZ9M5sJ5c5hwwRV8curZ3PjJ+1zy7ZeedZQkJbGzdVu67NjKrIFDeGH49bz63MP03LjOZ1vThpzJ0WtW8jevMz33D3y8s/r3kyhPSCDjoI4csjWTBKeDL044hZeuuI6zf/iWuz58y1PuxnsfI73L3xj29Uw+H3Q6ZU2SfPZFo6uiqa66TvDgOhr0PwpVqq6NmfhSwA9MTe23N5eclq35W2YG6zt1qZN1AiSXlVKalOx5/vS4p3nzvEvZEGIb186YyvC5M3jqnzfydf+TOOeH/zHrxFM98x9582Xm9TueH44+loO3b0UwbD6oY5Vx3DfpdZ6+eiS3fDSJT4ecyX3vjufWu8YELXvjJ5MZf9GVnuf/+uJj/mtdnX7cit8ZtHQx+c1SGfjHEoZb7WXNiwopaJYKwO0fvs3QRQu4+9bR9NywlhZFBUyw7uF869R3iHcayhMS+ODMf5DXvCVPvvYsh2Vm0HZvLumdD2FV18M4eMc2Om/PojwxkeGPvcKEp0bTdVsmcwcM4qXLr/WJ94s7r+PjU8/m/TPPB1yfi/EXDmdniLOJafeNZMuB7SlLbMI1n3xY5b4L5i+d4N2uGvMCme3a1yQspZSKuh2De9douXAJPubdJOvLe2PuBAJPeZVSyq7+Mgnere+alT71Xn8ecji3hmjgUkqpxuwvl+D9HblxbdBGn2AMUNokiY0dDmbZ4UfwW1oPTyOcUko1NH/5BF8dAjQtL6PHpnX02LSOK76aEeuQ6o13S40RQYzBiFAZn0CCo5KyxCY44uOpSEggu0Ur9rTaj72pzclp2Yq81BbktmiJU4QdbfanrEkS29seQJFXt1OlVN3TBK8i4t2tTqyGeTHGc/GN90U6rQvyfbqgKXty/+iL3zT3AYAjLp444+p2KMbgjIujMj6ehMpKjMRRkZBAvNNJRUIClfEJlCcmsrdZKoiQ27wFCQ4HTSoq2NOqNUaEioREyhNc5Xa3bkNpk2TijJOSpCSaVFSwN7W5VX4/KuITiDNONh/UkeLkZBIcDsQ4KUxJDXgdCZWVVIa4TqK+DF24AGrYyBqOJnilVI0Eu7pa2HcAkOB0+MyLdzr39TM3DhLKXfO9r9BtCFdEx8wDt9X5Km15JatSSilN8EopZVua4JVSyqY0wSullE1pgldKKZvSBK+UUjZliwTf9fPpsQ5BKaUaHFsk+MQOHWIdglJKNTi2SPBKKaUCaYJXSimb0gSvlFI2pQleKaVsyhYJPi41leKTTox1GEop1aDYIsGLCAWXX17ju5IrpZQd2SLBe0s99RQAWl9xRYwjUUqp2LLdePCdxo3DGAMOB8THkfveZDq+/hrJPXuy/qSTa73+pn36UPLbb3UQqfqrSj3lFAq//TbWYUSs09tvgYHM666LdSiqmmx3BA+uKhtJSODA0aPpnr6a5kOGkHjAAbR78EFPmQ6vjuXQb76GxERSBvT3Wb7l+ef7lHNrcc45dPnwAw75chYH3HuvzzLd01cHVBEd9MTjIWPsMPYVEvbfP+T8ZieeyEFPPM6hc+fUSdVT89NPr9Fy3Zb/Qff01Ry+eBEAra+8kg4vvxS0bPtnn6HJIYd4nh9w9120f/55z3Pv15GUluZ5nLZqZY1iCya+Vat9T+LiOHzJErpO/8ynTMvzz6fr59Ppnr6atrfcHLCOuGbNgq47+YgjIoqh3YMPcuAjD4ec3/6ZZ4JOP2TWTDqOe5W0P5cHzNv/zjs4ZM7siLYvSUk+z8N9DuNatqxyfXFNm5J6wkAO/u87pJ58Mp3enOiZ13bkyIhiCmDdQalpv74kH3VU0CJtrg//g9LywguCTt//9tuDTm81bJjP+9Lk0EODlqtqn7QdeWPY+f66Tv+M/e+8w2dawoEH+jxPOvzwaq0zUmKMqbpUPenXr59ZsmRJjZZdsGABgwYNqrJcxfbtlG/JpNlxx/pMNxUV5H02naTDDiOlz9Hsev55KrZto8OLL1K+eTPG4SDJK3kBGGNI7+760nsnr9VHHgUVFaStXEF6j54AlB92GE3WrfOU6Z6+mu2PPkrelKm0vOACmp9+Glk3uD44+99+O23/PcJnW/lz5rD1dt8PycGTJpE98Q2KFi6i7cgbyf14GhIXR5MuXSj+5RdXofh4cDjonr6a8owMKnfvZvOVVwHQ/oXnKVq0iOZDTiHL+qK2uvhiUo47jm133RXwuhz5+cSlplKWns6mCy6E+HiS09IoXbmSDi+/TIuhZ2AqK9n+0MO0uf56kg7p6tofad0968qfO5fSFSvY79prWTfgeM/0PW++SUqfPp73Ztfzz1O+JZPSFStI6taNrZdfRttHxtDmxhvIHj/BtZ9G3Ur+N99QtmpfjGl/LkcSEylbt46Eg9oTn+pK1uVZWWw49bSA1+T9fnm/N+UZGVTm5uIsKmbnk09SvnEjacv/wBjDml6uW6sd9OSTFC1aRHK3w/mzUyfajbotYP3Fv/+OJCSS0GY/1g85xTN/04UXUbpyJZ2nfEhS166UZ2TQtPe+W7a59xlA89NOpcPLLyPx8fs+D7Nns/u112naqxcJbfajYvsO8mfNos2NNxCXlMzul1/mwMf+Q1xKCi3PPpvvP/2U/R9wHeDsf9soTHk5TXv3JuXYY1nT+2hCaffgg7S+4nJEfO/fVLT4Z1KO6YfEx+PIy2Nt/wFBl2/SpQvlGRme521vvYXCBd9x0JhHME5D8uGHIU2a+Lxe92vu+OqrOMvKyJ85k+0PPgRA1xkzcBYVEdesGcndDg9Yzr1/cz/6mB2PPLJvu7fczP433eSzb9vdfx+7x71GswED6PDiC+yZOJE9Y1+l8/uT2Tz8yn37YPT9LD/4YDq/Oo7SlSvp9OZEMq/f9/3c/847aD54MLlTppL7wQekDOhP8aLFABz0xBO0sn6IvGP1XkfHca+SevLJSGJi0H1YFRFZaozpF3SmMabB/PXt29fU1Pz582u8bG2U79xpCr7/wWda2ebNJv/bb40xxlTm55vN/7rGfPfZdOMsLzeruqWZjRdcaIwxxllWZkrWrPEst6pbmlnVLc0ULf0t6LZ2j59gipYsMZW5uaYyL88YY0zpho1mVbc0U7punaeco6jIrOqWZnaNG2cqdu82pRs2+qwn56OPTFlGhs8097bdin77zZSsXh00jsr8fLOqW5rJmzHDOB0OU5GTE3Yfbbr8Cp91u5Vt3mycZWVhly344UdTkZ3t8/76x7qqW5pZP/RMU7FnT8j1lO/caVZ1SzNrTjghYJ6zvNw4KyoC1huMs6zM9R5edLHP9Pnz51e5vPd8Z0WFz3sWrGzGVf8MG4tPXE6n6/0oKzNOpzPgPZk/f77Z9cpYs6pbmsn/dp7PvIIFC0zhTz+Z4j9XmLLNm03W7XeYVd3SzN45cyLevlvJypXGUVrqea1Oh8NU7Nlj1g4a7Frn11+HfL3h9l32u+95vjc+y6V1d33+N2wwhYsWm6JffjHGGJM7bZpZ1S3NbL3vfuOsqAi6LafD4TPd6XCY4uV/GmOM2fHss2ZVtzRTvnWrMca1/yp27/bsk+IVK8LGXPTLL2b1kUf5fL5L1641+fPnm7JNmzz7qvCnn0K+5kgBS0yInBrzpO791xgTfKTc8VXk5BhHSUnQMqu6pZk1A46vk+05HQ7jdDojLh9JcqtxLGVlprKgsFbr8H5/i1esMHv++1/P87ItW4yjMPz6wyV4t6JffzXZk9+vMpaiJUs8P7De8RUtXWqK/1wRcrmcKVPNmuMHVrl+Y4ypzMszjip+/Kpj/vz5xllebvZ+9VWVnwtnRYWpLCio1fbKt2/3OYjIHHWb68flm2+Cll9y7XVm6wMPVHs7juJi4yguDpxeWmq2PfhQ0IOPgh9/NNnvTa7WdoLll/xv55n8efMCC9ezcAnedo2sDV1C69Yh5x0yZzYJbdvWyXYkrnrNK6W9etGhf/+qC9YkliZNiG/SpM7W17RHD5r26OF53qRTpyqXSWjThmbHH0+bf/87ZJmUfv1I6Rf8TNenXN++waf36RN2udaXDqP1pcOqXD9AfAR149UliYm0iKAtRhISiE9NrdW2Ev3qmA986EES2x1Aaohq1ILhV9A3gipWf3FNmwafnpTEQY/9J+i81IEDYeDAam/LX/Mhg2u9jmjTBN+AJHXtGrNt773xBo6uwRessZD4eA5+5+1Yh/GXldCmDe3uvz/WYfzl2LIXjVJKKU3wSillW5rglVLKpjTBK6WUTWmCV0opm9IEr5RSNhXVBC8iQ0VkjYisF5H7orktpZRSvqKW4EUkHngNOBM4ArhMRCIbrUkppVStRfMI/lhgvTFmozGmHJgKnBfF7SmllPISzStZOwCZXs+zgOP8C4nICMA9NFuhiKyp4fbaAntquGx90PhqR+OrHY2vdhpyfJ1DzYj5UAXGmInAxCoLVkFElphQQ2Y2ABpf7Wh8taPx1U5Djy+UaFbRbAW8R4HqaE1TSilVD6KZ4H8FDhORriLSBLgU+CKK21NKKeUlalU0xphKEbkZ+AqIB94xxtTdvdkC1bqaJ8o0vtrR+GpH46udhh5fUA3qln1KKaXqjl7JqpRSNqUJXimlbKrRJfiqhj8QkSQR+cia/7OIdGlg8V0tIrtFZJn1d109xvaOiOwSkRUh5ouIjLViXy4i4e9BV//xDRKRvV777uF6jq+TiMwXkVUislJERgUpE7N9GGF8MduHIpIsIr+IyB9WfI8GKROz72+E8cXs+1sjoW7W2hD/cDXWbgAOAZoAfwBH+JUZCUywHl8KfNTA4rsaGBej/XcS0AdYEWL+WcAcQID+wM8NLL5BwKxY7Dtr+wcBfazHzYG1Qd7fmO3DCOOL2T609kmq9TgR+Bno71cmlt/fSOKL2fe3Jn+N7Qg+kuEPzgPetR5/ApwiItKA4osZY8z3QE6YIucB7xmXxUArETmofqKLKL6YMsZsN8b8Zj0uAFbjumLbW8z2YYTxxYy1Twqtp4nWn38vj5h9fyOMr1FpbAk+2PAH/h9gTxljTCWwF2hTL9FFFh/Ahdbp+yci0inI/FiJNP5YGmCdQs8RkR6xCsKqOjga11GetwaxD8PEBzHchyISLyLLgF3AN8aYkPsvBt/fSOKDhvv9DdDYErwdzAS6GGOOAr5h39GKqtpvQGdjTC/gVeDzWAQhIqnAp8Btxpj8WMQQThXxxXQfGmMcxpjeuK5sP1ZEetbn9qsSQXyN6vvb2BJ8JMMfeMqISALQEsiul+giiM8Yk22MKbOevgX0rafYItGgh5cwxuS7T6GNMbOBRBFpW58xiEgiruT5gTHmsyBFYroPq4qvIexDa9t5wHxgqN+sWH5/PULF18C/vwEaW4KPZPiDL4B/Wo8vAuYZq3WkIcTnVx97Lq560obiC+AqqydIf2CvMWZ7rINyE5ED3fWxInIsrs9vvX35rW2/Daw2xrwYoljM9mEk8cVyH4rI/iLSynrcFDgNSPcrFrPvbyTxNfDvb4CYjyZZHSbE8Aci8h9giTHmC1wf8Mkish5Xg92lDSy+W0XkXKDSiu/q+opPRKbg6kXRVkSygEdwNSRhjJkAzMbVC2Q9UAz8q75iizC+i4AbRaQSKAEurccfb4CBwJXAn1Y9LcBo4GCvGGO5DyOJL5b78CDgXXHdDCgO+NgYM6uhfH8jjC9m39+a0KEKlFLKphpbFY1SSqkIaYJXSimb0gSvlFI2pQleKaVsShO8UkrFiFQxwF6Q8pd4DSb3YVXlNcGrBktE2niN2rdDRLZ6PW9SxbL9RGRsBNtYWHcRB6y7lYiMjNb6lS1MIvBir6BE5DDgfmCgMaYHcFuVy2g3SdUYiMgYoNAY87zXtARrvJIGyRoPZpYxpkFdjq8aFv/PiYgcCrwG7I/rWorrjTHpIvIssNYY81ak69YjeNWoiMgkEZkgIj8Dz4rIsSKySER+F5GFItLNKjdIRGZZj8dYp8ILRGSjiNzqtb5Cr/ILrAGk0kXkA68rPs+ypi0V11jvs4LE1UNcY4kvE9dAVIcBTwOHWtOes8rdLSK/WmUetaZ18drmaiuGFGve09Yp+XIRed5/u8qWJgK3GGP6AncBr1vTDwcOF5GfRGSxiFR55N+ormRVytIRON4Y4xCRFsCJ1lXEpwJPAhcGWSYNGIxrnPQ1IjLeGFPhV+ZooAewDfgJGCgiS4A3gJOMMZusq22DuQF4xRjzgVV9FA/cB/S0Bq9CRE4HDsM1rLQAX4jIScAWoBtwrTHmJxF5BxgpIv8FzgfSjDHGfRm9si9xDRR3PDBN9o2SnGT9T8D1+RmE6zvwvYgcaY2bE5QmeNUYTTPGOKzHLXFdXn4YrrG7E0Ms86U1SFSZiOwC2uEaytfbL8aYLADrUv8uQCGw0RizySozBRgRZP2LgAdEpCPwmTFmnQQOY3669fe79TwV1xd2C5BpjPnJmv4+cCvwMlAKvG2dNQScOSjbiQPy3AcFfrJw3UCmAtgkImtxfX5+DbcypRqbIq/HjwHzrfrLvwPJIZYp83rsIPjBTSRlgjLGfIhr8KkSYLaIDAlSTICnjDG9rb+/GWPedq8icJWmEtfR/ifAOcDcSONRjZM1vPMmEbkYPLeA7GXN/hzX0TviGgH0cGBjuPVpgleNXUv2Dcd7dRTWvwY4RPbdG3RYsEIicgiuI/2xwAzgKKAAV5WQ21fANdZpOCLSQUQOsOYdLCIDrMeXAz9a5Vpaw/reDvRC2YpV5bcI6CYiWSJyLXAFcK2I/AGsZN9d4b4CskVkFa6hjO82xoQdCVSraFRj9yyuKpoHgS/reuXGmBKrq+NcESki9OnwJcCVIlIB7ACeNMbkWA1iK4A5xpi7RaQ7sMiqvikEhuM6W1gD3GTVv68CxuP68ZohIsm4jv7vqOvXp2LLGHNZiFkBDajWqJ93UI3PgXaTVKoKIpJqjCm0etW8BqwzxrxUh+vvgnanVFGgVTRKVe16q9F1Ja6j6jdiHI9SEdEjeKWUsik9gldKKZvSBK+UUjalCV4ppWxKE7xSStmUJnillLKp/wM9ba4Y5fPmDwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "dNYdctmK0CtP",
        "outputId": "3a9980eb-811f-43c2-f256-2c3e57859048"
      },
      "source": [
        "del model\r\n",
        "model = NeuralNet(tr_set.dataset.dim).to(device)\r\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\r\n",
        "model.load_state_dict(ckpt)\r\n",
        "plot_pred(dv_set, model, device)  # Show prediction on the validation set"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZdOe6jUzQvO"
      },
      "source": [
        "## **Testing**\r\n",
        "The predictions of your model on testing set will be stored at `pred.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNk5sDLjzaMp",
        "outputId": "c4dfb11f-dfd5-49fc-bfe6-000718ca9e10"
      },
      "source": [
        "def save_pred(preds, file):\r\n",
        "    ''' Save predictions to specified file '''\r\n",
        "    print('Saving results to {}'.format(file))\r\n",
        "    with open(file, 'w') as fp:\r\n",
        "        writer = csv.writer(fp)\r\n",
        "        writer.writerow(['id', 'tested_positive'])\r\n",
        "        for i, p in enumerate(preds):\r\n",
        "            writer.writerow([i, p])\r\n",
        "\r\n",
        "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\r\n",
        "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to pred.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7llbZOw0XsX"
      },
      "source": [
        "## **Reference**\r\n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)"
      ]
    }
  ]
}