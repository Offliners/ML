{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNkT1fUKGjSiNZWGLwMsJKF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Offliners/NTUML2021_Hung-yi-Lee/blob/main/HW2/homewrok2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghYxMSxsmAYm"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**\r\n",
        "\r\n",
        "The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\r\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\r\n",
        "\r\n",
        "This homework is a multiclass classification task, we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\r\n",
        "\r\n",
        "## **Download Data**\r\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3\r\n",
        "\r\n",
        "timit_11/\r\n",
        "\r\n",
        "* train_11.npy: training data\r\n",
        "* train_label_11.npy: training label\r\n",
        "* test_11.npy: testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N98vZGvSl-Fq",
        "outputId": "046bd499-6a22-4f15-bb20-9e17e3ce470d"
      },
      "source": [
        "!gdown --id '1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG' --output data.zip\r\n",
        "!unzip data.zip\r\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG\n",
            "To: /content/data.zip\n",
            "376MB [00:08, 42.6MB/s]\n",
            "Archive:  data.zip\n",
            "  inflating: sampleSubmission.csv    \n",
            "  inflating: timit_11/timit_11/test_11.npy  \n",
            "  inflating: timit_11/timit_11/train_11.npy  \n",
            "  inflating: timit_11/timit_11/train_label_11.npy  \n",
            "data.zip  sample_data  sampleSubmission.csv  timit_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffxBMT3mU5g"
      },
      "source": [
        "# **Preparing Data**\r\n",
        "\r\n",
        "Load the training and testing data from the .npy file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRTYp6bemeRk",
        "outputId": "62668d06-4731-4205-8478-053674ac7d5a"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "print('Loading data ...')\r\n",
        "\r\n",
        "data_root='./timit_11/timit_11/'\r\n",
        "train = np.load(data_root + 'train_11.npy')\r\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\r\n",
        "test = np.load(data_root + 'test_11.npy')\r\n",
        "\r\n",
        "print('Size of training data: {}'.format(train.shape))\r\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpQ84yh8me6A"
      },
      "source": [
        "# **Create Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPvM4GoFmhzo"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import Dataset\r\n",
        "\r\n",
        "class TIMITDataset(Dataset):\r\n",
        "    def __init__(self, X, y=None):\r\n",
        "        self.data = torch.from_numpy(X).float()\r\n",
        "        if y is not None:\r\n",
        "            y = y.astype(np.int)\r\n",
        "            self.label = torch.LongTensor(y)\r\n",
        "        else:\r\n",
        "            self.label = None\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if self.label is not None:\r\n",
        "            return self.data[idx], self.label[idx]\r\n",
        "        else:\r\n",
        "            return self.data[idx]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmKia2rfmmk9",
        "outputId": "f59dd40e-9446-4173-9d26-c2160a00421e"
      },
      "source": [
        "VAL_RATIO = 0.2\r\n",
        "\r\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\r\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\r\n",
        "print('Size of training set: {}'.format(train_x.shape))\r\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: (983945, 429)\n",
            "Size of validation set: (245987, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1fNcNamqKa"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "train_set = TIMITDataset(train_x, train_y)\r\n",
        "val_set = TIMITDataset(val_x, val_y)\r\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\r\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kqycVinmj11"
      },
      "source": [
        "#### **notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7dGNUuGmtDD",
        "outputId": "26980a04-2a37-41fd-ebe7-2f426f301911"
      },
      "source": [
        "import gc\r\n",
        "\r\n",
        "del train, train_label, train_x, train_y, val_x, val_y\r\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFtiY94m6X-"
      },
      "source": [
        "# **Create Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfFT6XJ8nBCS"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "class Classifier(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Classifier, self).__init__()\r\n",
        "        self.layer1 = nn.Linear(429, 1024)\r\n",
        "        self.layer2 = nn.Linear(1024, 512)\r\n",
        "        self.layer3 = nn.Linear(512, 128)\r\n",
        "        self.out = nn.Linear(128, 39) \r\n",
        "\r\n",
        "        self.act_fn = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.layer1(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "\r\n",
        "        x = self.layer2(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "\r\n",
        "        x = self.layer3(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "\r\n",
        "        x = self.out(x)\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLq4t_knLUL"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBsChEconOOV",
        "outputId": "46cb0106-a060-49ff-dcab-9e8d2911d7ec"
      },
      "source": [
        "#check device\r\n",
        "def get_device():\r\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "\r\n",
        "# fix random seed\r\n",
        "def same_seeds(seed):\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        torch.cuda.manual_seed(seed)\r\n",
        "        torch.cuda.manual_seed_all(seed)  \r\n",
        "    np.random.seed(seed)  \r\n",
        "    torch.backends.cudnn.benchmark = False\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "# fix random seed for reproducibility\r\n",
        "same_seeds(0)\r\n",
        "\r\n",
        "# get device \r\n",
        "device = get_device()\r\n",
        "print(f'DEVICE: {device}')\r\n",
        "\r\n",
        "# training parameters\r\n",
        "num_epoch = 20               # number of training epoch\r\n",
        "learning_rate = 1e-4         # learning rate\r\n",
        "l2 = 1e-3                    # L2 regularization\r\n",
        "\r\n",
        "# the path where checkpoint saved\r\n",
        "model_path = './model.ckpt'\r\n",
        "\r\n",
        "# create model, define a loss function, and optimizer\r\n",
        "model = Classifier().to(device)\r\n",
        "criterion = nn.CrossEntropyLoss() \r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyjl3itJnWwL",
        "outputId": "044a1e5d-bbde-4a63-c81c-d6e46ad20358"
      },
      "source": [
        "# start training\r\n",
        "\r\n",
        "best_acc = 0.0\r\n",
        "for epoch in range(num_epoch):\r\n",
        "    train_acc = 0.0\r\n",
        "    train_loss = 0.0\r\n",
        "    val_acc = 0.0\r\n",
        "    val_loss = 0.0\r\n",
        "\r\n",
        "    # training\r\n",
        "    model.train() # set the model to training mode\r\n",
        "    for i, data in enumerate(train_loader):\r\n",
        "        inputs, labels = data\r\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "        optimizer.zero_grad() \r\n",
        "        outputs = model(inputs) \r\n",
        "        batch_loss = criterion(outputs, labels)\r\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\r\n",
        "        batch_loss.backward() \r\n",
        "        optimizer.step() \r\n",
        "\r\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\r\n",
        "        train_loss += batch_loss.item()\r\n",
        "\r\n",
        "    # validation\r\n",
        "    if len(val_set) > 0:\r\n",
        "        model.eval() # set the model to evaluation mode\r\n",
        "        with torch.no_grad():\r\n",
        "            for i, data in enumerate(val_loader):\r\n",
        "                inputs, labels = data\r\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "                outputs = model(inputs)\r\n",
        "                batch_loss = criterion(outputs, labels) \r\n",
        "                _, val_pred = torch.max(outputs, 1) \r\n",
        "            \r\n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\r\n",
        "                val_loss += batch_loss.item()\r\n",
        "\r\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\r\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\r\n",
        "            ))\r\n",
        "\r\n",
        "            # if the model improves, save a checkpoint at this epoch\r\n",
        "            if val_acc > best_acc:\r\n",
        "                best_acc = val_acc\r\n",
        "                torch.save(model.state_dict(), model_path)\r\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\r\n",
        "    else:\r\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\r\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\r\n",
        "        ))\r\n",
        "\r\n",
        "# if not validating, save the last epoch\r\n",
        "if len(val_set) == 0:\r\n",
        "    torch.save(model.state_dict(), model_path)\r\n",
        "    print('saving model at last epoch')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/020] Train Acc: 0.610734 Loss: 1.267194 | Val Acc: 0.662189 loss: 1.078465\n",
            "saving model with acc 0.662\n",
            "[002/020] Train Acc: 0.670508 Loss: 1.042772 | Val Acc: 0.683459 loss: 0.998771\n",
            "saving model with acc 0.683\n",
            "[003/020] Train Acc: 0.688731 Loss: 0.978801 | Val Acc: 0.688821 loss: 0.968805\n",
            "saving model with acc 0.689\n",
            "[004/020] Train Acc: 0.699398 Loss: 0.938747 | Val Acc: 0.695695 loss: 0.948611\n",
            "saving model with acc 0.696\n",
            "[005/020] Train Acc: 0.707704 Loss: 0.909336 | Val Acc: 0.701253 loss: 0.929879\n",
            "saving model with acc 0.701\n",
            "[006/020] Train Acc: 0.713737 Loss: 0.887337 | Val Acc: 0.704562 loss: 0.913815\n",
            "saving model with acc 0.705\n",
            "[007/020] Train Acc: 0.719107 Loss: 0.869160 | Val Acc: 0.705769 loss: 0.909557\n",
            "saving model with acc 0.706\n",
            "[008/020] Train Acc: 0.723298 Loss: 0.854309 | Val Acc: 0.708135 loss: 0.898911\n",
            "saving model with acc 0.708\n",
            "[009/020] Train Acc: 0.726461 Loss: 0.841218 | Val Acc: 0.712286 loss: 0.886429\n",
            "saving model with acc 0.712\n",
            "[010/020] Train Acc: 0.730115 Loss: 0.829919 | Val Acc: 0.711257 loss: 0.890309\n",
            "[011/020] Train Acc: 0.733100 Loss: 0.820394 | Val Acc: 0.714440 loss: 0.878264\n",
            "saving model with acc 0.714\n",
            "[012/020] Train Acc: 0.735957 Loss: 0.811388 | Val Acc: 0.712318 loss: 0.884146\n",
            "[013/020] Train Acc: 0.738115 Loss: 0.804185 | Val Acc: 0.711769 loss: 0.886208\n",
            "[014/020] Train Acc: 0.740097 Loss: 0.797328 | Val Acc: 0.717762 loss: 0.868284\n",
            "saving model with acc 0.718\n",
            "[015/020] Train Acc: 0.742348 Loss: 0.790602 | Val Acc: 0.715948 loss: 0.872893\n",
            "[016/020] Train Acc: 0.743811 Loss: 0.784368 | Val Acc: 0.716095 loss: 0.870856\n",
            "[017/020] Train Acc: 0.745089 Loss: 0.779657 | Val Acc: 0.717997 loss: 0.862334\n",
            "saving model with acc 0.718\n",
            "[018/020] Train Acc: 0.746835 Loss: 0.774225 | Val Acc: 0.716042 loss: 0.868663\n",
            "[019/020] Train Acc: 0.748180 Loss: 0.769595 | Val Acc: 0.721705 loss: 0.853316\n",
            "saving model with acc 0.722\n",
            "[020/020] Train Acc: 0.749439 Loss: 0.765597 | Val Acc: 0.720152 loss: 0.858827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTr80Panaq8"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiBILuUEncLl",
        "outputId": "f9ed7856-820c-49d7-975d-5b53c7bb3da1"
      },
      "source": [
        "# create testing dataset\r\n",
        "test_set = TIMITDataset(test, None)\r\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\r\n",
        "\r\n",
        "# create model and load weights from checkpoint\r\n",
        "model = Classifier().to(device)\r\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP_tUJeLnd94"
      },
      "source": [
        "predict = []\r\n",
        "model.eval() # set the model to evaluation mode\r\n",
        "with torch.no_grad():\r\n",
        "    for i, data in enumerate(test_loader):\r\n",
        "        inputs = data\r\n",
        "        inputs = inputs.to(device)\r\n",
        "        outputs = model(inputs)\r\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\r\n",
        "\r\n",
        "        for y in test_pred.cpu().numpy():\r\n",
        "            predict.append(y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIjAsNTnhgs"
      },
      "source": [
        "# **Write prediction to a CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "athJ5R7BnlN5",
        "outputId": "14ac4d82-2eed-4335-8027-11461f9432e3"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\r\n",
        "    f.write('Id,Class\\n')\r\n",
        "    for i, y in enumerate(predict):\r\n",
        "        f.write('{},{}\\n'.format(i, y))\r\n",
        "\r\n",
        "print('Saving results to prediction.csv')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to prediction.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV5cHjHLos62"
      },
      "source": [
        "# **Reference**\r\n",
        "\r\n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb)"
      ]
    }
  ]
}
