{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Offliners/ML/blob/main/HW2/homework2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghYxMSxsmAYm"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**\n",
        "\n",
        "The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "## **Download Data**\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3\n",
        "\n",
        "timit_11/\n",
        "\n",
        "* train_11.npy: training data\n",
        "* train_label_11.npy: training label\n",
        "* test_11.npy: testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N98vZGvSl-Fq",
        "outputId": "69035c69-beac-444f-c925-9a4d484b0d67"
      },
      "source": [
        "!gdown --id '1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG\n",
            "To: /content/data.zip\n",
            "376MB [00:04, 82.2MB/s]\n",
            "Archive:  data.zip\n",
            "  inflating: sampleSubmission.csv    \n",
            "  inflating: timit_11/timit_11/test_11.npy  \n",
            "  inflating: timit_11/timit_11/train_11.npy  \n",
            "  inflating: timit_11/timit_11/train_label_11.npy  \n",
            "data.zip  sample_data  sampleSubmission.csv  timit_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffxBMT3mU5g"
      },
      "source": [
        "# **Preparing Data**\n",
        "\n",
        "Load the training and testing data from the .npy file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRTYp6bemeRk",
        "outputId": "be8d22fc-1eae-47e5-f4b6-779a8ce00240"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpQ84yh8me6A"
      },
      "source": [
        "# **Create Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPvM4GoFmhzo"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmKia2rfmmk9",
        "outputId": "a59e6cb2-e231-4b3f-c1ea-c6362f9a2891"
      },
      "source": [
        "VAL_RATIO = 0.1\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: (1106938, 429)\n",
            "Size of validation set: (122994, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1fNcNamqKa"
      },
      "source": [
        "BATCH_SIZE = 4096\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kqycVinmj11"
      },
      "source": [
        "#### **notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7dGNUuGmtDD",
        "outputId": "b57d12fe-c9b9-43f1-808f-9a6a7396d002"
      },
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "170"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFtiY94m6X-"
      },
      "source": [
        "# **Create Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfFT6XJ8nBCS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(429, 2048)\n",
        "        self.layer2 = nn.Linear(2048, 2048)\n",
        "        self.layer3 = nn.Linear(2048, 1024)\n",
        "        self.layer4 = nn.Linear(1024, 512)\n",
        "        self.layer5 = nn.Linear(512, 128)\n",
        "        self.out = nn.Linear(128, 39) \n",
        "        self.dp = nn.Dropout(0.5)\n",
        "        self.bn1 = nn.BatchNorm1d(2048)\n",
        "        self.bn2 = nn.BatchNorm1d(2048)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.act_fn = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer5(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLq4t_knLUL"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBsChEconOOV",
        "outputId": "58a72a1e-e40f-447c-ad62-80d505003835"
      },
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 300               # number of training epoch\n",
        "learning_rate = 1e-4         # learning rate\n",
        "l2 = 1e-3                    # L2 regularization\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyjl3itJnWwL",
        "outputId": "e5858c8e-a17f-4a12-b25d-8522b7af1299"
      },
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/300] Train Acc: 0.372582 Loss: 2.385101 | Val Acc: 0.536701 loss: 1.583505\n",
            "saving model with acc 0.537\n",
            "[002/300] Train Acc: 0.510854 Loss: 1.732827 | Val Acc: 0.605875 loss: 1.322038\n",
            "saving model with acc 0.606\n",
            "[003/300] Train Acc: 0.558786 Loss: 1.533894 | Val Acc: 0.638698 loss: 1.187137\n",
            "saving model with acc 0.639\n",
            "[004/300] Train Acc: 0.586965 Loss: 1.420843 | Val Acc: 0.661260 loss: 1.109396\n",
            "saving model with acc 0.661\n",
            "[005/300] Train Acc: 0.606104 Loss: 1.346157 | Val Acc: 0.675391 loss: 1.054047\n",
            "saving model with acc 0.675\n",
            "[006/300] Train Acc: 0.620620 Loss: 1.289154 | Val Acc: 0.685619 loss: 1.018935\n",
            "saving model with acc 0.686\n",
            "[007/300] Train Acc: 0.633024 Loss: 1.245124 | Val Acc: 0.694798 loss: 0.982933\n",
            "saving model with acc 0.695\n",
            "[008/300] Train Acc: 0.642219 Loss: 1.207614 | Val Acc: 0.700693 loss: 0.960519\n",
            "saving model with acc 0.701\n",
            "[009/300] Train Acc: 0.650681 Loss: 1.176277 | Val Acc: 0.707270 loss: 0.938309\n",
            "saving model with acc 0.707\n",
            "[010/300] Train Acc: 0.657469 Loss: 1.150381 | Val Acc: 0.710222 loss: 0.924670\n",
            "saving model with acc 0.710\n",
            "[011/300] Train Acc: 0.664051 Loss: 1.126769 | Val Acc: 0.715474 loss: 0.906654\n",
            "saving model with acc 0.715\n",
            "[012/300] Train Acc: 0.669621 Loss: 1.104829 | Val Acc: 0.718287 loss: 0.892751\n",
            "saving model with acc 0.718\n",
            "[013/300] Train Acc: 0.674551 Loss: 1.087173 | Val Acc: 0.721930 loss: 0.880133\n",
            "saving model with acc 0.722\n",
            "[014/300] Train Acc: 0.679570 Loss: 1.069240 | Val Acc: 0.724686 loss: 0.870352\n",
            "saving model with acc 0.725\n",
            "[015/300] Train Acc: 0.683558 Loss: 1.053302 | Val Acc: 0.727182 loss: 0.862704\n",
            "saving model with acc 0.727\n",
            "[016/300] Train Acc: 0.686935 Loss: 1.038919 | Val Acc: 0.729637 loss: 0.851888\n",
            "saving model with acc 0.730\n",
            "[017/300] Train Acc: 0.691714 Loss: 1.024593 | Val Acc: 0.732629 loss: 0.843634\n",
            "saving model with acc 0.733\n",
            "[018/300] Train Acc: 0.695473 Loss: 1.009367 | Val Acc: 0.735719 loss: 0.833999\n",
            "saving model with acc 0.736\n",
            "[019/300] Train Acc: 0.699569 Loss: 0.996965 | Val Acc: 0.737792 loss: 0.826610\n",
            "saving model with acc 0.738\n",
            "[020/300] Train Acc: 0.702809 Loss: 0.985007 | Val Acc: 0.739434 loss: 0.823767\n",
            "saving model with acc 0.739\n",
            "[021/300] Train Acc: 0.705667 Loss: 0.972767 | Val Acc: 0.740182 loss: 0.815864\n",
            "saving model with acc 0.740\n",
            "[022/300] Train Acc: 0.708434 Loss: 0.963363 | Val Acc: 0.742963 loss: 0.809982\n",
            "saving model with acc 0.743\n",
            "[023/300] Train Acc: 0.711688 Loss: 0.951858 | Val Acc: 0.743874 loss: 0.807609\n",
            "saving model with acc 0.744\n",
            "[024/300] Train Acc: 0.714061 Loss: 0.943166 | Val Acc: 0.745679 loss: 0.798856\n",
            "saving model with acc 0.746\n",
            "[025/300] Train Acc: 0.716223 Loss: 0.933546 | Val Acc: 0.746077 loss: 0.796142\n",
            "saving model with acc 0.746\n",
            "[026/300] Train Acc: 0.719338 Loss: 0.925532 | Val Acc: 0.747069 loss: 0.795700\n",
            "saving model with acc 0.747\n",
            "[027/300] Train Acc: 0.721238 Loss: 0.916203 | Val Acc: 0.749411 loss: 0.788330\n",
            "saving model with acc 0.749\n",
            "[028/300] Train Acc: 0.722849 Loss: 0.910751 | Val Acc: 0.749093 loss: 0.784762\n",
            "[029/300] Train Acc: 0.724718 Loss: 0.902355 | Val Acc: 0.748646 loss: 0.785250\n",
            "[030/300] Train Acc: 0.726886 Loss: 0.895820 | Val Acc: 0.751809 loss: 0.779186\n",
            "saving model with acc 0.752\n",
            "[031/300] Train Acc: 0.728847 Loss: 0.889192 | Val Acc: 0.750589 loss: 0.778676\n",
            "[032/300] Train Acc: 0.730501 Loss: 0.881584 | Val Acc: 0.752004 loss: 0.774736\n",
            "saving model with acc 0.752\n",
            "[033/300] Train Acc: 0.731864 Loss: 0.876457 | Val Acc: 0.753167 loss: 0.772937\n",
            "saving model with acc 0.753\n",
            "[034/300] Train Acc: 0.733588 Loss: 0.870505 | Val Acc: 0.753541 loss: 0.770520\n",
            "saving model with acc 0.754\n",
            "[035/300] Train Acc: 0.734629 Loss: 0.865579 | Val Acc: 0.753508 loss: 0.771065\n",
            "[036/300] Train Acc: 0.736376 Loss: 0.860446 | Val Acc: 0.754516 loss: 0.768603\n",
            "saving model with acc 0.755\n",
            "[037/300] Train Acc: 0.737749 Loss: 0.855866 | Val Acc: 0.756078 loss: 0.766265\n",
            "saving model with acc 0.756\n",
            "[038/300] Train Acc: 0.738671 Loss: 0.851543 | Val Acc: 0.755590 loss: 0.763123\n",
            "[039/300] Train Acc: 0.739610 Loss: 0.847650 | Val Acc: 0.756769 loss: 0.761708\n",
            "saving model with acc 0.757\n",
            "[040/300] Train Acc: 0.741107 Loss: 0.842849 | Val Acc: 0.756069 loss: 0.766247\n",
            "[041/300] Train Acc: 0.742240 Loss: 0.837700 | Val Acc: 0.756492 loss: 0.756811\n",
            "[042/300] Train Acc: 0.742832 Loss: 0.835684 | Val Acc: 0.756443 loss: 0.755065\n",
            "[043/300] Train Acc: 0.744127 Loss: 0.831160 | Val Acc: 0.758395 loss: 0.754863\n",
            "saving model with acc 0.758\n",
            "[044/300] Train Acc: 0.744629 Loss: 0.827777 | Val Acc: 0.757882 loss: 0.754346\n",
            "[045/300] Train Acc: 0.746436 Loss: 0.823942 | Val Acc: 0.758370 loss: 0.756299\n",
            "[046/300] Train Acc: 0.746990 Loss: 0.820268 | Val Acc: 0.758476 loss: 0.752261\n",
            "saving model with acc 0.758\n",
            "[047/300] Train Acc: 0.748051 Loss: 0.816214 | Val Acc: 0.759549 loss: 0.748878\n",
            "saving model with acc 0.760\n",
            "[048/300] Train Acc: 0.748566 Loss: 0.815258 | Val Acc: 0.759444 loss: 0.750668\n",
            "[049/300] Train Acc: 0.748906 Loss: 0.811389 | Val Acc: 0.760053 loss: 0.750205\n",
            "saving model with acc 0.760\n",
            "[050/300] Train Acc: 0.750075 Loss: 0.809114 | Val Acc: 0.758858 loss: 0.750827\n",
            "[051/300] Train Acc: 0.750734 Loss: 0.807193 | Val Acc: 0.759435 loss: 0.749009\n",
            "[052/300] Train Acc: 0.751316 Loss: 0.804018 | Val Acc: 0.759728 loss: 0.747885\n",
            "[053/300] Train Acc: 0.752492 Loss: 0.800225 | Val Acc: 0.759980 loss: 0.746026\n",
            "[054/300] Train Acc: 0.753036 Loss: 0.798006 | Val Acc: 0.759053 loss: 0.746960\n",
            "[055/300] Train Acc: 0.753419 Loss: 0.796407 | Val Acc: 0.761468 loss: 0.744494\n",
            "saving model with acc 0.761\n",
            "[056/300] Train Acc: 0.754408 Loss: 0.794010 | Val Acc: 0.761793 loss: 0.743247\n",
            "saving model with acc 0.762\n",
            "[057/300] Train Acc: 0.754513 Loss: 0.792881 | Val Acc: 0.761037 loss: 0.740978\n",
            "[058/300] Train Acc: 0.755021 Loss: 0.789981 | Val Acc: 0.762566 loss: 0.736608\n",
            "saving model with acc 0.763\n",
            "[059/300] Train Acc: 0.755399 Loss: 0.787429 | Val Acc: 0.762070 loss: 0.743282\n",
            "[060/300] Train Acc: 0.756034 Loss: 0.785367 | Val Acc: 0.761679 loss: 0.743274\n",
            "[061/300] Train Acc: 0.757204 Loss: 0.783439 | Val Acc: 0.763533 loss: 0.736703\n",
            "saving model with acc 0.764\n",
            "[062/300] Train Acc: 0.757359 Loss: 0.782156 | Val Acc: 0.762070 loss: 0.739163\n",
            "[063/300] Train Acc: 0.757291 Loss: 0.780704 | Val Acc: 0.763086 loss: 0.741358\n",
            "[064/300] Train Acc: 0.758059 Loss: 0.777698 | Val Acc: 0.761980 loss: 0.740714\n",
            "[065/300] Train Acc: 0.758680 Loss: 0.777350 | Val Acc: 0.762509 loss: 0.739060\n",
            "[066/300] Train Acc: 0.759445 Loss: 0.774522 | Val Acc: 0.763541 loss: 0.734446\n",
            "saving model with acc 0.764\n",
            "[067/300] Train Acc: 0.759552 Loss: 0.772935 | Val Acc: 0.763704 loss: 0.733761\n",
            "saving model with acc 0.764\n",
            "[068/300] Train Acc: 0.760782 Loss: 0.771259 | Val Acc: 0.761257 loss: 0.740064\n",
            "[069/300] Train Acc: 0.760792 Loss: 0.770386 | Val Acc: 0.762468 loss: 0.739821\n",
            "[070/300] Train Acc: 0.760906 Loss: 0.769098 | Val Acc: 0.763240 loss: 0.734660\n",
            "[071/300] Train Acc: 0.760905 Loss: 0.767029 | Val Acc: 0.763574 loss: 0.735963\n",
            "[072/300] Train Acc: 0.762210 Loss: 0.765087 | Val Acc: 0.763127 loss: 0.737212\n",
            "[073/300] Train Acc: 0.762736 Loss: 0.762099 | Val Acc: 0.764777 loss: 0.737067\n",
            "saving model with acc 0.765\n",
            "[074/300] Train Acc: 0.762302 Loss: 0.762809 | Val Acc: 0.764045 loss: 0.735998\n",
            "[075/300] Train Acc: 0.762585 Loss: 0.761498 | Val Acc: 0.764972 loss: 0.732622\n",
            "saving model with acc 0.765\n",
            "[076/300] Train Acc: 0.763340 Loss: 0.759690 | Val Acc: 0.765265 loss: 0.731792\n",
            "saving model with acc 0.765\n",
            "[077/300] Train Acc: 0.764131 Loss: 0.757548 | Val Acc: 0.763834 loss: 0.735103\n",
            "[078/300] Train Acc: 0.764285 Loss: 0.756149 | Val Acc: 0.763582 loss: 0.738821\n",
            "[079/300] Train Acc: 0.764167 Loss: 0.756623 | Val Acc: 0.763598 loss: 0.738463\n",
            "[080/300] Train Acc: 0.765338 Loss: 0.753856 | Val Acc: 0.765265 loss: 0.729632\n",
            "[081/300] Train Acc: 0.764826 Loss: 0.754243 | Val Acc: 0.764858 loss: 0.733800\n",
            "[082/300] Train Acc: 0.765297 Loss: 0.752289 | Val Acc: 0.764110 loss: 0.733579\n",
            "[083/300] Train Acc: 0.765403 Loss: 0.752238 | Val Acc: 0.765029 loss: 0.730048\n",
            "[084/300] Train Acc: 0.766327 Loss: 0.749434 | Val Acc: 0.764867 loss: 0.733383\n",
            "[085/300] Train Acc: 0.766900 Loss: 0.747572 | Val Acc: 0.764850 loss: 0.735491\n",
            "[086/300] Train Acc: 0.766684 Loss: 0.747379 | Val Acc: 0.765574 loss: 0.733261\n",
            "saving model with acc 0.766\n",
            "[087/300] Train Acc: 0.766683 Loss: 0.747126 | Val Acc: 0.764615 loss: 0.733617\n",
            "[088/300] Train Acc: 0.766938 Loss: 0.746995 | Val Acc: 0.764232 loss: 0.735949\n",
            "[089/300] Train Acc: 0.767113 Loss: 0.744394 | Val Acc: 0.764346 loss: 0.734278\n",
            "[090/300] Train Acc: 0.767716 Loss: 0.743462 | Val Acc: 0.765200 loss: 0.730412\n",
            "[091/300] Train Acc: 0.768332 Loss: 0.742493 | Val Acc: 0.765021 loss: 0.733186\n",
            "[092/300] Train Acc: 0.768490 Loss: 0.741259 | Val Acc: 0.765395 loss: 0.728703\n",
            "[093/300] Train Acc: 0.769022 Loss: 0.740598 | Val Acc: 0.764249 loss: 0.734393\n",
            "[094/300] Train Acc: 0.769406 Loss: 0.738680 | Val Acc: 0.764680 loss: 0.736357\n",
            "[095/300] Train Acc: 0.769030 Loss: 0.738622 | Val Acc: 0.764289 loss: 0.733432\n",
            "[096/300] Train Acc: 0.770431 Loss: 0.737034 | Val Acc: 0.765216 loss: 0.731891\n",
            "[097/300] Train Acc: 0.769778 Loss: 0.737709 | Val Acc: 0.765915 loss: 0.730557\n",
            "saving model with acc 0.766\n",
            "[098/300] Train Acc: 0.769650 Loss: 0.737189 | Val Acc: 0.765346 loss: 0.730823\n",
            "[099/300] Train Acc: 0.770385 Loss: 0.735273 | Val Acc: 0.765428 loss: 0.731007\n",
            "[100/300] Train Acc: 0.769967 Loss: 0.734689 | Val Acc: 0.765574 loss: 0.731299\n",
            "[101/300] Train Acc: 0.770918 Loss: 0.732979 | Val Acc: 0.764110 loss: 0.735546\n",
            "[102/300] Train Acc: 0.771322 Loss: 0.732573 | Val Acc: 0.765745 loss: 0.730993\n",
            "[103/300] Train Acc: 0.771197 Loss: 0.731913 | Val Acc: 0.765753 loss: 0.730816\n",
            "[104/300] Train Acc: 0.771652 Loss: 0.730156 | Val Acc: 0.765761 loss: 0.735596\n",
            "[105/300] Train Acc: 0.771901 Loss: 0.729526 | Val Acc: 0.765232 loss: 0.733535\n",
            "[106/300] Train Acc: 0.771386 Loss: 0.731005 | Val Acc: 0.764387 loss: 0.733428\n",
            "[107/300] Train Acc: 0.772266 Loss: 0.728492 | Val Acc: 0.766883 loss: 0.729873\n",
            "saving model with acc 0.767\n",
            "[108/300] Train Acc: 0.772826 Loss: 0.727277 | Val Acc: 0.765972 loss: 0.731417\n",
            "[109/300] Train Acc: 0.772802 Loss: 0.727772 | Val Acc: 0.766257 loss: 0.730962\n",
            "[110/300] Train Acc: 0.773413 Loss: 0.726628 | Val Acc: 0.766078 loss: 0.733872\n",
            "[111/300] Train Acc: 0.772967 Loss: 0.725175 | Val Acc: 0.766777 loss: 0.727945\n",
            "[112/300] Train Acc: 0.773005 Loss: 0.725516 | Val Acc: 0.765704 loss: 0.730513\n",
            "[113/300] Train Acc: 0.773201 Loss: 0.724843 | Val Acc: 0.765241 loss: 0.734265\n",
            "[114/300] Train Acc: 0.773674 Loss: 0.723004 | Val Acc: 0.765590 loss: 0.733717\n",
            "[115/300] Train Acc: 0.774041 Loss: 0.722908 | Val Acc: 0.766777 loss: 0.728473\n",
            "[116/300] Train Acc: 0.774050 Loss: 0.722272 | Val Acc: 0.766232 loss: 0.733285\n",
            "[117/300] Train Acc: 0.773923 Loss: 0.722051 | Val Acc: 0.766550 loss: 0.730006\n",
            "[118/300] Train Acc: 0.773741 Loss: 0.722037 | Val Acc: 0.766395 loss: 0.731316\n",
            "[119/300] Train Acc: 0.774309 Loss: 0.720549 | Val Acc: 0.766810 loss: 0.729475\n",
            "[120/300] Train Acc: 0.774718 Loss: 0.719612 | Val Acc: 0.765980 loss: 0.731322\n",
            "[121/300] Train Acc: 0.774340 Loss: 0.719778 | Val Acc: 0.766704 loss: 0.728506\n",
            "[122/300] Train Acc: 0.775419 Loss: 0.717792 | Val Acc: 0.764379 loss: 0.734882\n",
            "[123/300] Train Acc: 0.775048 Loss: 0.717966 | Val Acc: 0.766476 loss: 0.732181\n",
            "[124/300] Train Acc: 0.775124 Loss: 0.717650 | Val Acc: 0.767094 loss: 0.731695\n",
            "saving model with acc 0.767\n",
            "[125/300] Train Acc: 0.775961 Loss: 0.715945 | Val Acc: 0.767729 loss: 0.732256\n",
            "saving model with acc 0.768\n",
            "[126/300] Train Acc: 0.775683 Loss: 0.716923 | Val Acc: 0.766476 loss: 0.730459\n",
            "[127/300] Train Acc: 0.776373 Loss: 0.715031 | Val Acc: 0.767054 loss: 0.730820\n",
            "[128/300] Train Acc: 0.775888 Loss: 0.714856 | Val Acc: 0.767346 loss: 0.730651\n",
            "[129/300] Train Acc: 0.776145 Loss: 0.714187 | Val Acc: 0.766826 loss: 0.731291\n",
            "[130/300] Train Acc: 0.776100 Loss: 0.714482 | Val Acc: 0.765054 loss: 0.734700\n",
            "[131/300] Train Acc: 0.776252 Loss: 0.714070 | Val Acc: 0.765769 loss: 0.733216\n",
            "[132/300] Train Acc: 0.777079 Loss: 0.712475 | Val Acc: 0.764777 loss: 0.738608\n",
            "[133/300] Train Acc: 0.776816 Loss: 0.712206 | Val Acc: 0.766574 loss: 0.732598\n",
            "[134/300] Train Acc: 0.776853 Loss: 0.713058 | Val Acc: 0.765924 loss: 0.734172\n",
            "[135/300] Train Acc: 0.776475 Loss: 0.712116 | Val Acc: 0.766216 loss: 0.732192\n",
            "[136/300] Train Acc: 0.776982 Loss: 0.711611 | Val Acc: 0.766346 loss: 0.732073\n",
            "[137/300] Train Acc: 0.777486 Loss: 0.711140 | Val Acc: 0.767143 loss: 0.733398\n",
            "[138/300] Train Acc: 0.776664 Loss: 0.712374 | Val Acc: 0.764533 loss: 0.731610\n",
            "[139/300] Train Acc: 0.776999 Loss: 0.710213 | Val Acc: 0.767265 loss: 0.730020\n",
            "[140/300] Train Acc: 0.777603 Loss: 0.708683 | Val Acc: 0.766078 loss: 0.733129\n",
            "[141/300] Train Acc: 0.777569 Loss: 0.708658 | Val Acc: 0.765737 loss: 0.732772\n",
            "[142/300] Train Acc: 0.777895 Loss: 0.708781 | Val Acc: 0.766533 loss: 0.732954\n",
            "[143/300] Train Acc: 0.778200 Loss: 0.707773 | Val Acc: 0.767306 loss: 0.728091\n",
            "[144/300] Train Acc: 0.778160 Loss: 0.707098 | Val Acc: 0.764704 loss: 0.735388\n",
            "[145/300] Train Acc: 0.778257 Loss: 0.707776 | Val Acc: 0.767582 loss: 0.732986\n",
            "[146/300] Train Acc: 0.778484 Loss: 0.706993 | Val Acc: 0.766753 loss: 0.730738\n",
            "[147/300] Train Acc: 0.778762 Loss: 0.705929 | Val Acc: 0.767094 loss: 0.733420\n",
            "[148/300] Train Acc: 0.778106 Loss: 0.706885 | Val Acc: 0.766541 loss: 0.731956\n",
            "[149/300] Train Acc: 0.779121 Loss: 0.705326 | Val Acc: 0.766826 loss: 0.729332\n",
            "[150/300] Train Acc: 0.778979 Loss: 0.704758 | Val Acc: 0.765745 loss: 0.732339\n",
            "[151/300] Train Acc: 0.778639 Loss: 0.705661 | Val Acc: 0.767753 loss: 0.732793\n",
            "saving model with acc 0.768\n",
            "[152/300] Train Acc: 0.778892 Loss: 0.703964 | Val Acc: 0.768785 loss: 0.731453\n",
            "saving model with acc 0.769\n",
            "[153/300] Train Acc: 0.778740 Loss: 0.703968 | Val Acc: 0.766509 loss: 0.733791\n",
            "[154/300] Train Acc: 0.779752 Loss: 0.702519 | Val Acc: 0.766810 loss: 0.733968\n",
            "[155/300] Train Acc: 0.779531 Loss: 0.703521 | Val Acc: 0.767054 loss: 0.731987\n",
            "[156/300] Train Acc: 0.779276 Loss: 0.703306 | Val Acc: 0.767322 loss: 0.728142\n",
            "[157/300] Train Acc: 0.779199 Loss: 0.703268 | Val Acc: 0.768298 loss: 0.730959\n",
            "[158/300] Train Acc: 0.779566 Loss: 0.702496 | Val Acc: 0.766737 loss: 0.733139\n",
            "[159/300] Train Acc: 0.779466 Loss: 0.702796 | Val Acc: 0.767403 loss: 0.733334\n",
            "[160/300] Train Acc: 0.779494 Loss: 0.701431 | Val Acc: 0.767363 loss: 0.732917\n",
            "[161/300] Train Acc: 0.780089 Loss: 0.701535 | Val Acc: 0.766834 loss: 0.728658\n",
            "[162/300] Train Acc: 0.779929 Loss: 0.701217 | Val Acc: 0.767428 loss: 0.730116\n",
            "[163/300] Train Acc: 0.779966 Loss: 0.700988 | Val Acc: 0.768623 loss: 0.728158\n",
            "[164/300] Train Acc: 0.779754 Loss: 0.702285 | Val Acc: 0.766761 loss: 0.731298\n",
            "[165/300] Train Acc: 0.780128 Loss: 0.699985 | Val Acc: 0.766712 loss: 0.732387\n",
            "[166/300] Train Acc: 0.780665 Loss: 0.699791 | Val Acc: 0.766314 loss: 0.731127\n",
            "[167/300] Train Acc: 0.780784 Loss: 0.697665 | Val Acc: 0.767005 loss: 0.732508\n",
            "[168/300] Train Acc: 0.779868 Loss: 0.699969 | Val Acc: 0.766639 loss: 0.727974\n",
            "[169/300] Train Acc: 0.780261 Loss: 0.698519 | Val Acc: 0.766013 loss: 0.731247\n",
            "[170/300] Train Acc: 0.780610 Loss: 0.699551 | Val Acc: 0.768200 loss: 0.728799\n",
            "[171/300] Train Acc: 0.780761 Loss: 0.698628 | Val Acc: 0.767460 loss: 0.730164\n",
            "[172/300] Train Acc: 0.781597 Loss: 0.696681 | Val Acc: 0.767168 loss: 0.733214\n",
            "[173/300] Train Acc: 0.780494 Loss: 0.697975 | Val Acc: 0.767810 loss: 0.731482\n",
            "[174/300] Train Acc: 0.781110 Loss: 0.696622 | Val Acc: 0.767688 loss: 0.731895\n",
            "[175/300] Train Acc: 0.781075 Loss: 0.696871 | Val Acc: 0.767233 loss: 0.729733\n",
            "[176/300] Train Acc: 0.781030 Loss: 0.697342 | Val Acc: 0.767054 loss: 0.734184\n",
            "[177/300] Train Acc: 0.781682 Loss: 0.695941 | Val Acc: 0.765997 loss: 0.731049\n",
            "[178/300] Train Acc: 0.781037 Loss: 0.696372 | Val Acc: 0.768029 loss: 0.730607\n",
            "[179/300] Train Acc: 0.780977 Loss: 0.695967 | Val Acc: 0.767883 loss: 0.732165\n",
            "[180/300] Train Acc: 0.781362 Loss: 0.695944 | Val Acc: 0.766875 loss: 0.733506\n",
            "[181/300] Train Acc: 0.781897 Loss: 0.695004 | Val Acc: 0.767916 loss: 0.732219\n",
            "[182/300] Train Acc: 0.781663 Loss: 0.695105 | Val Acc: 0.767680 loss: 0.731365\n",
            "[183/300] Train Acc: 0.781891 Loss: 0.695048 | Val Acc: 0.768224 loss: 0.732154\n",
            "[184/300] Train Acc: 0.782268 Loss: 0.694497 | Val Acc: 0.768078 loss: 0.730216\n",
            "[185/300] Train Acc: 0.781623 Loss: 0.694224 | Val Acc: 0.767663 loss: 0.730429\n",
            "[186/300] Train Acc: 0.781882 Loss: 0.693651 | Val Acc: 0.767777 loss: 0.730596\n",
            "[187/300] Train Acc: 0.781526 Loss: 0.695411 | Val Acc: 0.766777 loss: 0.732028\n",
            "[188/300] Train Acc: 0.781752 Loss: 0.693499 | Val Acc: 0.768460 loss: 0.729491\n",
            "[189/300] Train Acc: 0.782580 Loss: 0.692628 | Val Acc: 0.767590 loss: 0.734372\n",
            "[190/300] Train Acc: 0.782327 Loss: 0.693105 | Val Acc: 0.766940 loss: 0.734253\n",
            "[191/300] Train Acc: 0.782322 Loss: 0.693867 | Val Acc: 0.767729 loss: 0.730570\n",
            "[192/300] Train Acc: 0.783198 Loss: 0.691454 | Val Acc: 0.768151 loss: 0.730797\n",
            "[193/300] Train Acc: 0.782290 Loss: 0.692936 | Val Acc: 0.768135 loss: 0.729898\n",
            "[194/300] Train Acc: 0.782175 Loss: 0.692611 | Val Acc: 0.767720 loss: 0.728492\n",
            "[195/300] Train Acc: 0.782833 Loss: 0.692008 | Val Acc: 0.768883 loss: 0.728658\n",
            "saving model with acc 0.769\n",
            "[196/300] Train Acc: 0.782972 Loss: 0.691102 | Val Acc: 0.769151 loss: 0.725342\n",
            "saving model with acc 0.769\n",
            "[197/300] Train Acc: 0.782888 Loss: 0.692548 | Val Acc: 0.768493 loss: 0.728909\n",
            "[198/300] Train Acc: 0.782605 Loss: 0.691902 | Val Acc: 0.767834 loss: 0.730162\n",
            "[199/300] Train Acc: 0.782719 Loss: 0.692002 | Val Acc: 0.768330 loss: 0.730090\n",
            "[200/300] Train Acc: 0.782593 Loss: 0.690971 | Val Acc: 0.768615 loss: 0.731112\n",
            "[201/300] Train Acc: 0.783122 Loss: 0.689737 | Val Acc: 0.769395 loss: 0.734250\n",
            "saving model with acc 0.769\n",
            "[202/300] Train Acc: 0.783032 Loss: 0.690767 | Val Acc: 0.768664 loss: 0.732181\n",
            "[203/300] Train Acc: 0.782839 Loss: 0.690571 | Val Acc: 0.769582 loss: 0.728079\n",
            "saving model with acc 0.770\n",
            "[204/300] Train Acc: 0.783179 Loss: 0.690807 | Val Acc: 0.768176 loss: 0.730814\n",
            "[205/300] Train Acc: 0.783045 Loss: 0.689563 | Val Acc: 0.767859 loss: 0.730311\n",
            "[206/300] Train Acc: 0.782727 Loss: 0.689950 | Val Acc: 0.768802 loss: 0.731021\n",
            "[207/300] Train Acc: 0.783281 Loss: 0.688847 | Val Acc: 0.768826 loss: 0.729313\n",
            "[208/300] Train Acc: 0.783230 Loss: 0.689899 | Val Acc: 0.768785 loss: 0.731790\n",
            "[209/300] Train Acc: 0.783759 Loss: 0.688378 | Val Acc: 0.766802 loss: 0.732781\n",
            "[210/300] Train Acc: 0.783742 Loss: 0.688461 | Val Acc: 0.767761 loss: 0.727807\n",
            "[211/300] Train Acc: 0.783680 Loss: 0.687999 | Val Acc: 0.768420 loss: 0.725430\n",
            "[212/300] Train Acc: 0.783386 Loss: 0.688775 | Val Acc: 0.768729 loss: 0.724269\n",
            "[213/300] Train Acc: 0.783847 Loss: 0.687318 | Val Acc: 0.767924 loss: 0.731036\n",
            "[214/300] Train Acc: 0.783648 Loss: 0.688239 | Val Acc: 0.767655 loss: 0.735127\n",
            "[215/300] Train Acc: 0.783768 Loss: 0.688097 | Val Acc: 0.768533 loss: 0.736284\n",
            "[216/300] Train Acc: 0.783669 Loss: 0.688241 | Val Acc: 0.767680 loss: 0.730834\n",
            "[217/300] Train Acc: 0.783716 Loss: 0.687705 | Val Acc: 0.768224 loss: 0.729877\n",
            "[218/300] Train Acc: 0.783884 Loss: 0.686966 | Val Acc: 0.768103 loss: 0.732342\n",
            "[219/300] Train Acc: 0.784034 Loss: 0.686799 | Val Acc: 0.767712 loss: 0.732007\n",
            "[220/300] Train Acc: 0.784235 Loss: 0.685963 | Val Acc: 0.766867 loss: 0.733482\n",
            "[221/300] Train Acc: 0.783964 Loss: 0.686452 | Val Acc: 0.769054 loss: 0.731234\n",
            "[222/300] Train Acc: 0.784029 Loss: 0.687268 | Val Acc: 0.768257 loss: 0.728592\n",
            "[223/300] Train Acc: 0.783828 Loss: 0.687680 | Val Acc: 0.768306 loss: 0.730712\n",
            "[224/300] Train Acc: 0.784167 Loss: 0.687353 | Val Acc: 0.768493 loss: 0.726483\n",
            "[225/300] Train Acc: 0.784120 Loss: 0.686331 | Val Acc: 0.769054 loss: 0.731125\n",
            "[226/300] Train Acc: 0.784125 Loss: 0.686474 | Val Acc: 0.768314 loss: 0.731905\n",
            "[227/300] Train Acc: 0.784257 Loss: 0.685889 | Val Acc: 0.768021 loss: 0.730904\n",
            "[228/300] Train Acc: 0.784635 Loss: 0.685337 | Val Acc: 0.767916 loss: 0.729711\n",
            "[229/300] Train Acc: 0.784847 Loss: 0.685808 | Val Acc: 0.770306 loss: 0.728610\n",
            "saving model with acc 0.770\n",
            "[230/300] Train Acc: 0.784496 Loss: 0.685138 | Val Acc: 0.768517 loss: 0.727461\n",
            "[231/300] Train Acc: 0.784576 Loss: 0.685492 | Val Acc: 0.768143 loss: 0.732262\n",
            "[232/300] Train Acc: 0.784596 Loss: 0.685119 | Val Acc: 0.768517 loss: 0.734591\n",
            "[233/300] Train Acc: 0.784487 Loss: 0.684673 | Val Acc: 0.768216 loss: 0.731436\n",
            "[234/300] Train Acc: 0.784124 Loss: 0.685456 | Val Acc: 0.766956 loss: 0.731495\n",
            "[235/300] Train Acc: 0.784374 Loss: 0.685360 | Val Acc: 0.766964 loss: 0.734087\n",
            "[236/300] Train Acc: 0.785127 Loss: 0.684185 | Val Acc: 0.768428 loss: 0.730074\n",
            "[237/300] Train Acc: 0.785253 Loss: 0.682548 | Val Acc: 0.769672 loss: 0.727946\n",
            "[238/300] Train Acc: 0.784769 Loss: 0.683438 | Val Acc: 0.769436 loss: 0.728845\n",
            "[239/300] Train Acc: 0.785036 Loss: 0.684098 | Val Acc: 0.769265 loss: 0.730309\n",
            "[240/300] Train Acc: 0.784784 Loss: 0.684248 | Val Acc: 0.769029 loss: 0.730481\n",
            "[241/300] Train Acc: 0.785337 Loss: 0.682150 | Val Acc: 0.767729 loss: 0.735991\n",
            "[242/300] Train Acc: 0.785145 Loss: 0.684459 | Val Acc: 0.766281 loss: 0.736013\n",
            "[243/300] Train Acc: 0.785348 Loss: 0.683738 | Val Acc: 0.767907 loss: 0.730626\n",
            "[244/300] Train Acc: 0.785661 Loss: 0.683162 | Val Acc: 0.768916 loss: 0.729737\n",
            "[245/300] Train Acc: 0.785242 Loss: 0.683144 | Val Acc: 0.768956 loss: 0.729796\n",
            "[246/300] Train Acc: 0.785002 Loss: 0.683114 | Val Acc: 0.768802 loss: 0.728901\n",
            "[247/300] Train Acc: 0.785212 Loss: 0.682540 | Val Acc: 0.768119 loss: 0.733480\n",
            "[248/300] Train Acc: 0.785516 Loss: 0.682112 | Val Acc: 0.769533 loss: 0.729371\n",
            "[249/300] Train Acc: 0.784857 Loss: 0.681860 | Val Acc: 0.768834 loss: 0.731666\n",
            "[250/300] Train Acc: 0.785468 Loss: 0.681956 | Val Acc: 0.766777 loss: 0.734414\n",
            "[251/300] Train Acc: 0.785318 Loss: 0.682974 | Val Acc: 0.768241 loss: 0.729303\n",
            "[252/300] Train Acc: 0.785736 Loss: 0.680568 | Val Acc: 0.768411 loss: 0.728120\n",
            "[253/300] Train Acc: 0.785353 Loss: 0.681934 | Val Acc: 0.766493 loss: 0.734709\n",
            "[254/300] Train Acc: 0.785406 Loss: 0.681015 | Val Acc: 0.767785 loss: 0.731443\n",
            "[255/300] Train Acc: 0.785699 Loss: 0.680962 | Val Acc: 0.770420 loss: 0.726764\n",
            "saving model with acc 0.770\n",
            "[256/300] Train Acc: 0.785947 Loss: 0.681158 | Val Acc: 0.767794 loss: 0.730264\n",
            "[257/300] Train Acc: 0.786095 Loss: 0.680135 | Val Acc: 0.768533 loss: 0.733528\n",
            "[258/300] Train Acc: 0.786007 Loss: 0.679468 | Val Acc: 0.768338 loss: 0.732103\n",
            "[259/300] Train Acc: 0.785767 Loss: 0.680814 | Val Acc: 0.769680 loss: 0.731712\n",
            "[260/300] Train Acc: 0.786086 Loss: 0.680184 | Val Acc: 0.768306 loss: 0.726106\n",
            "[261/300] Train Acc: 0.785300 Loss: 0.680724 | Val Acc: 0.769493 loss: 0.729097\n",
            "[262/300] Train Acc: 0.785968 Loss: 0.680012 | Val Acc: 0.769948 loss: 0.729374\n",
            "[263/300] Train Acc: 0.786070 Loss: 0.680569 | Val Acc: 0.769013 loss: 0.734632\n",
            "[264/300] Train Acc: 0.785786 Loss: 0.680771 | Val Acc: 0.768655 loss: 0.732102\n",
            "[265/300] Train Acc: 0.785464 Loss: 0.680435 | Val Acc: 0.770477 loss: 0.728694\n",
            "saving model with acc 0.770\n",
            "[266/300] Train Acc: 0.785843 Loss: 0.679370 | Val Acc: 0.768720 loss: 0.730008\n",
            "[267/300] Train Acc: 0.786078 Loss: 0.680094 | Val Acc: 0.768875 loss: 0.735286\n",
            "[268/300] Train Acc: 0.786099 Loss: 0.678783 | Val Acc: 0.768997 loss: 0.733376\n",
            "[269/300] Train Acc: 0.785949 Loss: 0.680328 | Val Acc: 0.768550 loss: 0.733543\n",
            "[270/300] Train Acc: 0.786217 Loss: 0.680121 | Val Acc: 0.769143 loss: 0.732713\n",
            "[271/300] Train Acc: 0.786391 Loss: 0.679508 | Val Acc: 0.767997 loss: 0.729220\n",
            "[272/300] Train Acc: 0.785871 Loss: 0.679006 | Val Acc: 0.768338 loss: 0.733657\n",
            "[273/300] Train Acc: 0.786594 Loss: 0.678400 | Val Acc: 0.769647 loss: 0.730793\n",
            "[274/300] Train Acc: 0.786678 Loss: 0.678237 | Val Acc: 0.769184 loss: 0.727873\n",
            "[275/300] Train Acc: 0.786834 Loss: 0.678397 | Val Acc: 0.769143 loss: 0.730388\n",
            "[276/300] Train Acc: 0.786769 Loss: 0.678438 | Val Acc: 0.768907 loss: 0.727400\n",
            "[277/300] Train Acc: 0.786912 Loss: 0.678099 | Val Acc: 0.769558 loss: 0.729060\n",
            "[278/300] Train Acc: 0.786380 Loss: 0.678855 | Val Acc: 0.769038 loss: 0.734064\n",
            "[279/300] Train Acc: 0.786142 Loss: 0.679151 | Val Acc: 0.769875 loss: 0.727064\n",
            "[280/300] Train Acc: 0.787094 Loss: 0.677613 | Val Acc: 0.769696 loss: 0.728564\n",
            "[281/300] Train Acc: 0.786678 Loss: 0.677489 | Val Acc: 0.766981 loss: 0.731272\n",
            "[282/300] Train Acc: 0.786592 Loss: 0.678782 | Val Acc: 0.768298 loss: 0.729857\n",
            "[283/300] Train Acc: 0.786828 Loss: 0.677751 | Val Acc: 0.768346 loss: 0.736195\n",
            "[284/300] Train Acc: 0.786711 Loss: 0.678298 | Val Acc: 0.768485 loss: 0.731051\n",
            "[285/300] Train Acc: 0.786406 Loss: 0.678638 | Val Acc: 0.768574 loss: 0.734804\n",
            "[286/300] Train Acc: 0.787135 Loss: 0.676755 | Val Acc: 0.768802 loss: 0.731325\n",
            "[287/300] Train Acc: 0.787061 Loss: 0.676321 | Val Acc: 0.768306 loss: 0.734731\n",
            "[288/300] Train Acc: 0.786760 Loss: 0.676406 | Val Acc: 0.769810 loss: 0.736237\n",
            "[289/300] Train Acc: 0.787026 Loss: 0.676776 | Val Acc: 0.768802 loss: 0.738103\n",
            "[290/300] Train Acc: 0.786631 Loss: 0.677427 | Val Acc: 0.768558 loss: 0.727268\n",
            "[291/300] Train Acc: 0.786956 Loss: 0.677215 | Val Acc: 0.768753 loss: 0.730186\n",
            "[292/300] Train Acc: 0.786895 Loss: 0.676665 | Val Acc: 0.767468 loss: 0.733227\n",
            "[293/300] Train Acc: 0.787490 Loss: 0.677371 | Val Acc: 0.769338 loss: 0.734104\n",
            "[294/300] Train Acc: 0.787280 Loss: 0.676098 | Val Acc: 0.769290 loss: 0.731013\n",
            "[295/300] Train Acc: 0.786481 Loss: 0.676211 | Val Acc: 0.767924 loss: 0.729414\n",
            "[296/300] Train Acc: 0.786981 Loss: 0.676443 | Val Acc: 0.768273 loss: 0.732126\n",
            "[297/300] Train Acc: 0.787039 Loss: 0.676394 | Val Acc: 0.770192 loss: 0.730082\n",
            "[298/300] Train Acc: 0.787657 Loss: 0.674712 | Val Acc: 0.769078 loss: 0.731297\n",
            "[299/300] Train Acc: 0.787438 Loss: 0.675195 | Val Acc: 0.767785 loss: 0.732489\n",
            "[300/300] Train Acc: 0.787402 Loss: 0.676300 | Val Acc: 0.768379 loss: 0.728472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTr80Panaq8"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiBILuUEncLl",
        "outputId": "e2f2cf50-8258-40e6-f039-411807af8c38"
      },
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP_tUJeLnd94"
      },
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIjAsNTnhgs"
      },
      "source": [
        "# **Write prediction to a CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "athJ5R7BnlN5",
        "outputId": "ed831b06-09b8-4fa7-fbf4-c6fd863aa597"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))\n",
        "\n",
        "print('Saving results to prediction.csv')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to prediction.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV5cHjHLos62"
      },
      "source": [
        "# **Reference**\n",
        "\n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb)"
      ]
    }
  ]
}