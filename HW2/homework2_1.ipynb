{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOiuyA4+t7daFGvU4ChmdWt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Offliners/ML/blob/main/HW2/homework2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghYxMSxsmAYm"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**\n",
        "\n",
        "The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "## **Download Data**\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3\n",
        "\n",
        "timit_11/\n",
        "\n",
        "* train_11.npy: training data\n",
        "* train_label_11.npy: training label\n",
        "* test_11.npy: testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N98vZGvSl-Fq",
        "outputId": "2f195309-c6fc-405b-bd4e-7e06591d337c"
      },
      "source": [
        "!gdown --id '1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG\n",
            "To: /content/data.zip\n",
            "376MB [00:03, 111MB/s]\n",
            "Archive:  data.zip\n",
            "  inflating: sampleSubmission.csv    \n",
            "  inflating: timit_11/timit_11/test_11.npy  \n",
            "  inflating: timit_11/timit_11/train_11.npy  \n",
            "  inflating: timit_11/timit_11/train_label_11.npy  \n",
            "data.zip  sample_data  sampleSubmission.csv  timit_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffxBMT3mU5g"
      },
      "source": [
        "# **Preparing Data**\n",
        "\n",
        "Load the training and testing data from the .npy file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRTYp6bemeRk",
        "outputId": "48b0db2e-c250-49e3-bcee-fd2eab614292"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpQ84yh8me6A"
      },
      "source": [
        "# **Create Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPvM4GoFmhzo"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmKia2rfmmk9",
        "outputId": "4662665b-26de-4b21-9166-4670855c7f99"
      },
      "source": [
        "VAL_RATIO = 0.2\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: (983945, 429)\n",
            "Size of validation set: (245987, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1fNcNamqKa"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kqycVinmj11"
      },
      "source": [
        "#### **notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7dGNUuGmtDD",
        "outputId": "638d0a5c-4be0-47f5-cf6a-f2436b8b8627"
      },
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFtiY94m6X-"
      },
      "source": [
        "# **Create Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfFT6XJ8nBCS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(429, 2048)\n",
        "        self.layer2 = nn.Linear(2048, 2048)\n",
        "        self.layer3 = nn.Linear(2048, 1024)\n",
        "        self.layer4 = nn.Linear(1024, 512)\n",
        "        self.layer5 = nn.Linear(512, 128)\n",
        "        self.out = nn.Linear(128, 39) \n",
        "        self.dp = nn.Dropout(0.5)\n",
        "        self.bn1 = nn.BatchNorm1d(2048)\n",
        "        self.bn2 = nn.BatchNorm1d(2048)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.act_fn = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer5(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLq4t_knLUL"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBsChEconOOV",
        "outputId": "fcee591d-09b8-48d4-cde3-a63d8b39e9fb"
      },
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 200               # number of training epoch\n",
        "learning_rate = 1e-4         # learning rate\n",
        "l2 = 1e-3                    # L2 regularization\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyjl3itJnWwL",
        "outputId": "5ac707b5-e438-4409-fb21-d1ba8897d480"
      },
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/200] Train Acc: 0.435674 Loss: 2.060799 | Val Acc: 0.596479 loss: 1.357481\n",
            "saving model with acc 0.596\n",
            "[002/200] Train Acc: 0.559822 Loss: 1.513578 | Val Acc: 0.649274 loss: 1.156929\n",
            "saving model with acc 0.649\n",
            "[003/200] Train Acc: 0.598352 Loss: 1.364837 | Val Acc: 0.675137 loss: 1.063397\n",
            "saving model with acc 0.675\n",
            "[004/200] Train Acc: 0.620910 Loss: 1.280365 | Val Acc: 0.687000 loss: 1.015486\n",
            "saving model with acc 0.687\n",
            "[005/200] Train Acc: 0.636760 Loss: 1.222815 | Val Acc: 0.697204 loss: 0.977405\n",
            "saving model with acc 0.697\n",
            "[006/200] Train Acc: 0.647945 Loss: 1.180822 | Val Acc: 0.704594 loss: 0.948432\n",
            "saving model with acc 0.705\n",
            "[007/200] Train Acc: 0.657180 Loss: 1.145151 | Val Acc: 0.711842 loss: 0.923098\n",
            "saving model with acc 0.712\n",
            "[008/200] Train Acc: 0.665293 Loss: 1.116472 | Val Acc: 0.715501 loss: 0.908702\n",
            "saving model with acc 0.716\n",
            "[009/200] Train Acc: 0.672025 Loss: 1.088958 | Val Acc: 0.721758 loss: 0.889207\n",
            "saving model with acc 0.722\n",
            "[010/200] Train Acc: 0.678226 Loss: 1.067332 | Val Acc: 0.724262 loss: 0.877150\n",
            "saving model with acc 0.724\n",
            "[011/200] Train Acc: 0.684541 Loss: 1.045451 | Val Acc: 0.729648 loss: 0.861154\n",
            "saving model with acc 0.730\n",
            "[012/200] Train Acc: 0.689364 Loss: 1.027679 | Val Acc: 0.732055 loss: 0.851960\n",
            "saving model with acc 0.732\n",
            "[013/200] Train Acc: 0.693193 Loss: 1.013333 | Val Acc: 0.734336 loss: 0.842793\n",
            "saving model with acc 0.734\n",
            "[014/200] Train Acc: 0.697061 Loss: 0.999205 | Val Acc: 0.736206 loss: 0.835526\n",
            "saving model with acc 0.736\n",
            "[015/200] Train Acc: 0.700082 Loss: 0.986773 | Val Acc: 0.738242 loss: 0.828293\n",
            "saving model with acc 0.738\n",
            "[016/200] Train Acc: 0.703196 Loss: 0.976894 | Val Acc: 0.739019 loss: 0.824248\n",
            "saving model with acc 0.739\n",
            "[017/200] Train Acc: 0.705328 Loss: 0.967596 | Val Acc: 0.739921 loss: 0.821494\n",
            "saving model with acc 0.740\n",
            "[018/200] Train Acc: 0.708431 Loss: 0.958046 | Val Acc: 0.742966 loss: 0.811322\n",
            "saving model with acc 0.743\n",
            "[019/200] Train Acc: 0.710211 Loss: 0.950369 | Val Acc: 0.742515 loss: 0.809250\n",
            "[020/200] Train Acc: 0.712323 Loss: 0.942628 | Val Acc: 0.744348 loss: 0.805343\n",
            "saving model with acc 0.744\n",
            "[021/200] Train Acc: 0.714018 Loss: 0.936401 | Val Acc: 0.746560 loss: 0.797207\n",
            "saving model with acc 0.747\n",
            "[022/200] Train Acc: 0.714937 Loss: 0.931868 | Val Acc: 0.745279 loss: 0.799817\n",
            "[023/200] Train Acc: 0.716641 Loss: 0.925531 | Val Acc: 0.746271 loss: 0.795632\n",
            "[024/200] Train Acc: 0.718542 Loss: 0.918880 | Val Acc: 0.747446 loss: 0.791981\n",
            "saving model with acc 0.747\n",
            "[025/200] Train Acc: 0.718741 Loss: 0.916440 | Val Acc: 0.748670 loss: 0.789764\n",
            "saving model with acc 0.749\n",
            "[026/200] Train Acc: 0.720303 Loss: 0.912129 | Val Acc: 0.748296 loss: 0.788847\n",
            "[027/200] Train Acc: 0.721482 Loss: 0.906871 | Val Acc: 0.749133 loss: 0.782907\n",
            "saving model with acc 0.749\n",
            "[028/200] Train Acc: 0.722207 Loss: 0.903464 | Val Acc: 0.750162 loss: 0.781251\n",
            "saving model with acc 0.750\n",
            "[029/200] Train Acc: 0.723808 Loss: 0.898606 | Val Acc: 0.750003 loss: 0.783248\n",
            "[030/200] Train Acc: 0.724222 Loss: 0.895773 | Val Acc: 0.749385 loss: 0.784533\n",
            "[031/200] Train Acc: 0.725083 Loss: 0.893043 | Val Acc: 0.749405 loss: 0.780777\n",
            "[032/200] Train Acc: 0.725840 Loss: 0.888688 | Val Acc: 0.751076 loss: 0.777105\n",
            "saving model with acc 0.751\n",
            "[033/200] Train Acc: 0.726728 Loss: 0.886677 | Val Acc: 0.751487 loss: 0.775923\n",
            "saving model with acc 0.751\n",
            "[034/200] Train Acc: 0.727508 Loss: 0.883860 | Val Acc: 0.751048 loss: 0.778218\n",
            "[035/200] Train Acc: 0.728361 Loss: 0.881697 | Val Acc: 0.752784 loss: 0.772955\n",
            "saving model with acc 0.753\n",
            "[036/200] Train Acc: 0.729076 Loss: 0.878212 | Val Acc: 0.753174 loss: 0.770735\n",
            "saving model with acc 0.753\n",
            "[037/200] Train Acc: 0.729424 Loss: 0.876075 | Val Acc: 0.752414 loss: 0.773062\n",
            "[038/200] Train Acc: 0.730123 Loss: 0.874296 | Val Acc: 0.753036 loss: 0.769472\n",
            "[039/200] Train Acc: 0.730466 Loss: 0.872013 | Val Acc: 0.754389 loss: 0.763858\n",
            "saving model with acc 0.754\n",
            "[040/200] Train Acc: 0.731296 Loss: 0.869886 | Val Acc: 0.752995 loss: 0.770888\n",
            "[041/200] Train Acc: 0.731810 Loss: 0.867784 | Val Acc: 0.754162 loss: 0.767208\n",
            "[042/200] Train Acc: 0.732393 Loss: 0.865900 | Val Acc: 0.753585 loss: 0.765742\n",
            "[043/200] Train Acc: 0.732696 Loss: 0.863463 | Val Acc: 0.753666 loss: 0.766912\n",
            "[044/200] Train Acc: 0.733434 Loss: 0.862199 | Val Acc: 0.754853 loss: 0.764955\n",
            "saving model with acc 0.755\n",
            "[045/200] Train Acc: 0.733625 Loss: 0.861660 | Val Acc: 0.755068 loss: 0.763196\n",
            "saving model with acc 0.755\n",
            "[046/200] Train Acc: 0.734196 Loss: 0.858639 | Val Acc: 0.754475 loss: 0.765085\n",
            "[047/200] Train Acc: 0.734876 Loss: 0.857501 | Val Acc: 0.755524 loss: 0.761971\n",
            "saving model with acc 0.756\n",
            "[048/200] Train Acc: 0.734926 Loss: 0.854999 | Val Acc: 0.757178 loss: 0.759837\n",
            "saving model with acc 0.757\n",
            "[049/200] Train Acc: 0.735555 Loss: 0.854773 | Val Acc: 0.755581 loss: 0.764065\n",
            "[050/200] Train Acc: 0.735940 Loss: 0.852617 | Val Acc: 0.755987 loss: 0.762622\n",
            "[051/200] Train Acc: 0.736712 Loss: 0.850737 | Val Acc: 0.756650 loss: 0.759089\n",
            "[052/200] Train Acc: 0.736488 Loss: 0.850876 | Val Acc: 0.756259 loss: 0.762667\n",
            "[053/200] Train Acc: 0.736878 Loss: 0.848547 | Val Acc: 0.756398 loss: 0.760146\n",
            "[054/200] Train Acc: 0.737603 Loss: 0.846901 | Val Acc: 0.755650 loss: 0.763093\n",
            "[055/200] Train Acc: 0.738324 Loss: 0.845371 | Val Acc: 0.756564 loss: 0.758822\n",
            "[056/200] Train Acc: 0.738218 Loss: 0.844781 | Val Acc: 0.756219 loss: 0.760416\n",
            "[057/200] Train Acc: 0.738590 Loss: 0.842958 | Val Acc: 0.755967 loss: 0.763142\n",
            "[058/200] Train Acc: 0.738861 Loss: 0.842298 | Val Acc: 0.756386 loss: 0.760391\n",
            "[059/200] Train Acc: 0.739241 Loss: 0.840973 | Val Acc: 0.757007 loss: 0.756819\n",
            "[060/200] Train Acc: 0.738816 Loss: 0.840060 | Val Acc: 0.756991 loss: 0.759371\n",
            "[061/200] Train Acc: 0.739898 Loss: 0.838554 | Val Acc: 0.757255 loss: 0.757172\n",
            "saving model with acc 0.757\n",
            "[062/200] Train Acc: 0.740240 Loss: 0.837240 | Val Acc: 0.756760 loss: 0.758825\n",
            "[063/200] Train Acc: 0.740110 Loss: 0.837244 | Val Acc: 0.755963 loss: 0.760599\n",
            "[064/200] Train Acc: 0.740303 Loss: 0.837187 | Val Acc: 0.757638 loss: 0.756379\n",
            "saving model with acc 0.758\n",
            "[065/200] Train Acc: 0.740929 Loss: 0.835486 | Val Acc: 0.757280 loss: 0.757725\n",
            "[066/200] Train Acc: 0.741132 Loss: 0.833378 | Val Acc: 0.756507 loss: 0.758928\n",
            "[067/200] Train Acc: 0.741582 Loss: 0.832967 | Val Acc: 0.756573 loss: 0.758510\n",
            "[068/200] Train Acc: 0.741865 Loss: 0.831730 | Val Acc: 0.757081 loss: 0.757858\n",
            "[069/200] Train Acc: 0.741517 Loss: 0.831495 | Val Acc: 0.756544 loss: 0.756328\n",
            "[070/200] Train Acc: 0.741816 Loss: 0.831089 | Val Acc: 0.756808 loss: 0.757991\n",
            "[071/200] Train Acc: 0.742797 Loss: 0.828969 | Val Acc: 0.757243 loss: 0.757070\n",
            "[072/200] Train Acc: 0.742542 Loss: 0.829184 | Val Acc: 0.757333 loss: 0.754485\n",
            "[073/200] Train Acc: 0.743338 Loss: 0.826424 | Val Acc: 0.756560 loss: 0.756101\n",
            "[074/200] Train Acc: 0.742747 Loss: 0.829145 | Val Acc: 0.756638 loss: 0.756600\n",
            "[075/200] Train Acc: 0.743052 Loss: 0.826518 | Val Acc: 0.757186 loss: 0.755669\n",
            "[076/200] Train Acc: 0.743857 Loss: 0.825283 | Val Acc: 0.758443 loss: 0.753339\n",
            "saving model with acc 0.758\n",
            "[077/200] Train Acc: 0.743575 Loss: 0.824745 | Val Acc: 0.757987 loss: 0.759428\n",
            "[078/200] Train Acc: 0.743664 Loss: 0.823634 | Val Acc: 0.758398 loss: 0.754162\n",
            "[079/200] Train Acc: 0.743887 Loss: 0.823970 | Val Acc: 0.759178 loss: 0.752267\n",
            "saving model with acc 0.759\n",
            "[080/200] Train Acc: 0.743815 Loss: 0.824607 | Val Acc: 0.758475 loss: 0.753062\n",
            "[081/200] Train Acc: 0.744118 Loss: 0.823280 | Val Acc: 0.757666 loss: 0.754207\n",
            "[082/200] Train Acc: 0.744442 Loss: 0.820930 | Val Acc: 0.757406 loss: 0.758186\n",
            "[083/200] Train Acc: 0.745064 Loss: 0.821576 | Val Acc: 0.758686 loss: 0.754034\n",
            "[084/200] Train Acc: 0.744557 Loss: 0.821355 | Val Acc: 0.758552 loss: 0.751501\n",
            "[085/200] Train Acc: 0.745178 Loss: 0.820405 | Val Acc: 0.759040 loss: 0.753016\n",
            "[086/200] Train Acc: 0.744774 Loss: 0.820266 | Val Acc: 0.757113 loss: 0.756141\n",
            "[087/200] Train Acc: 0.744998 Loss: 0.819592 | Val Acc: 0.758085 loss: 0.756723\n",
            "[088/200] Train Acc: 0.744819 Loss: 0.820370 | Val Acc: 0.759048 loss: 0.754636\n",
            "[089/200] Train Acc: 0.745508 Loss: 0.819142 | Val Acc: 0.758869 loss: 0.752484\n",
            "[090/200] Train Acc: 0.745742 Loss: 0.817450 | Val Acc: 0.758593 loss: 0.751387\n",
            "[091/200] Train Acc: 0.745715 Loss: 0.817893 | Val Acc: 0.758963 loss: 0.753335\n",
            "[092/200] Train Acc: 0.745769 Loss: 0.817763 | Val Acc: 0.757475 loss: 0.753970\n",
            "[093/200] Train Acc: 0.745977 Loss: 0.816926 | Val Acc: 0.758109 loss: 0.751822\n",
            "[094/200] Train Acc: 0.745488 Loss: 0.816688 | Val Acc: 0.759292 loss: 0.751999\n",
            "saving model with acc 0.759\n",
            "[095/200] Train Acc: 0.746751 Loss: 0.814637 | Val Acc: 0.758808 loss: 0.751779\n",
            "[096/200] Train Acc: 0.746044 Loss: 0.814919 | Val Acc: 0.758878 loss: 0.754059\n",
            "[097/200] Train Acc: 0.747056 Loss: 0.814181 | Val Acc: 0.759386 loss: 0.751080\n",
            "saving model with acc 0.759\n",
            "[098/200] Train Acc: 0.746564 Loss: 0.814710 | Val Acc: 0.757934 loss: 0.753438\n",
            "[099/200] Train Acc: 0.746208 Loss: 0.814545 | Val Acc: 0.759788 loss: 0.749452\n",
            "saving model with acc 0.760\n",
            "[100/200] Train Acc: 0.746631 Loss: 0.813961 | Val Acc: 0.757520 loss: 0.755472\n",
            "[101/200] Train Acc: 0.746970 Loss: 0.813096 | Val Acc: 0.759524 loss: 0.751565\n",
            "[102/200] Train Acc: 0.746949 Loss: 0.812629 | Val Acc: 0.759235 loss: 0.751593\n",
            "[103/200] Train Acc: 0.747157 Loss: 0.812881 | Val Acc: 0.759231 loss: 0.749938\n",
            "[104/200] Train Acc: 0.746932 Loss: 0.812359 | Val Acc: 0.758264 loss: 0.755668\n",
            "[105/200] Train Acc: 0.747103 Loss: 0.812230 | Val Acc: 0.758934 loss: 0.750599\n",
            "[106/200] Train Acc: 0.747030 Loss: 0.812074 | Val Acc: 0.758817 loss: 0.753249\n",
            "[107/200] Train Acc: 0.747323 Loss: 0.811013 | Val Acc: 0.758918 loss: 0.751702\n",
            "[108/200] Train Acc: 0.747589 Loss: 0.811809 | Val Acc: 0.759642 loss: 0.753505\n",
            "[109/200] Train Acc: 0.746963 Loss: 0.811901 | Val Acc: 0.759434 loss: 0.750066\n",
            "[110/200] Train Acc: 0.747614 Loss: 0.810461 | Val Acc: 0.758878 loss: 0.752051\n",
            "[111/200] Train Acc: 0.747716 Loss: 0.810492 | Val Acc: 0.758251 loss: 0.754669\n",
            "[112/200] Train Acc: 0.747808 Loss: 0.810078 | Val Acc: 0.758938 loss: 0.753795\n",
            "[113/200] Train Acc: 0.748271 Loss: 0.809022 | Val Acc: 0.759317 loss: 0.751513\n",
            "[114/200] Train Acc: 0.748075 Loss: 0.809157 | Val Acc: 0.759211 loss: 0.752799\n",
            "[115/200] Train Acc: 0.747899 Loss: 0.809505 | Val Acc: 0.759390 loss: 0.750668\n",
            "[116/200] Train Acc: 0.748140 Loss: 0.809079 | Val Acc: 0.759995 loss: 0.750304\n",
            "saving model with acc 0.760\n",
            "[117/200] Train Acc: 0.747958 Loss: 0.808887 | Val Acc: 0.760707 loss: 0.748564\n",
            "saving model with acc 0.761\n",
            "[118/200] Train Acc: 0.748128 Loss: 0.808233 | Val Acc: 0.759934 loss: 0.748630\n",
            "[119/200] Train Acc: 0.748516 Loss: 0.807156 | Val Acc: 0.759284 loss: 0.751345\n",
            "[120/200] Train Acc: 0.748159 Loss: 0.808138 | Val Acc: 0.758085 loss: 0.755147\n",
            "[121/200] Train Acc: 0.748551 Loss: 0.807148 | Val Acc: 0.760780 loss: 0.745418\n",
            "saving model with acc 0.761\n",
            "[122/200] Train Acc: 0.748225 Loss: 0.807898 | Val Acc: 0.758910 loss: 0.754118\n",
            "[123/200] Train Acc: 0.747910 Loss: 0.808696 | Val Acc: 0.759914 loss: 0.752158\n",
            "[124/200] Train Acc: 0.748871 Loss: 0.807195 | Val Acc: 0.758878 loss: 0.752834\n",
            "[125/200] Train Acc: 0.748540 Loss: 0.807633 | Val Acc: 0.760020 loss: 0.751669\n",
            "[126/200] Train Acc: 0.748760 Loss: 0.806757 | Val Acc: 0.760333 loss: 0.749311\n",
            "[127/200] Train Acc: 0.748841 Loss: 0.806077 | Val Acc: 0.759813 loss: 0.750985\n",
            "[128/200] Train Acc: 0.748888 Loss: 0.806442 | Val Acc: 0.759926 loss: 0.752351\n",
            "[129/200] Train Acc: 0.749294 Loss: 0.805669 | Val Acc: 0.759853 loss: 0.752332\n",
            "[130/200] Train Acc: 0.748689 Loss: 0.805997 | Val Acc: 0.759674 loss: 0.751859\n",
            "[131/200] Train Acc: 0.748810 Loss: 0.804862 | Val Acc: 0.758190 loss: 0.753948\n",
            "[132/200] Train Acc: 0.749801 Loss: 0.804241 | Val Acc: 0.759369 loss: 0.752720\n",
            "[133/200] Train Acc: 0.749087 Loss: 0.805696 | Val Acc: 0.760975 loss: 0.749270\n",
            "saving model with acc 0.761\n",
            "[134/200] Train Acc: 0.749004 Loss: 0.804593 | Val Acc: 0.759882 loss: 0.751963\n",
            "[135/200] Train Acc: 0.749785 Loss: 0.803692 | Val Acc: 0.760329 loss: 0.748408\n",
            "[136/200] Train Acc: 0.748769 Loss: 0.805215 | Val Acc: 0.759853 loss: 0.748200\n",
            "[137/200] Train Acc: 0.749523 Loss: 0.803775 | Val Acc: 0.758861 loss: 0.753107\n",
            "[138/200] Train Acc: 0.749076 Loss: 0.804096 | Val Acc: 0.759479 loss: 0.747643\n",
            "[139/200] Train Acc: 0.749763 Loss: 0.802979 | Val Acc: 0.759024 loss: 0.751212\n",
            "[140/200] Train Acc: 0.749552 Loss: 0.803113 | Val Acc: 0.759280 loss: 0.749109\n",
            "[141/200] Train Acc: 0.749549 Loss: 0.802652 | Val Acc: 0.759593 loss: 0.747024\n",
            "[142/200] Train Acc: 0.750172 Loss: 0.801963 | Val Acc: 0.759487 loss: 0.751083\n",
            "[143/200] Train Acc: 0.750448 Loss: 0.801694 | Val Acc: 0.759735 loss: 0.750241\n",
            "[144/200] Train Acc: 0.749639 Loss: 0.802298 | Val Acc: 0.760410 loss: 0.748875\n",
            "[145/200] Train Acc: 0.749936 Loss: 0.801207 | Val Acc: 0.759117 loss: 0.751765\n",
            "[146/200] Train Acc: 0.749477 Loss: 0.801181 | Val Acc: 0.758577 loss: 0.750790\n",
            "[147/200] Train Acc: 0.749722 Loss: 0.800711 | Val Acc: 0.760857 loss: 0.748379\n",
            "[148/200] Train Acc: 0.750394 Loss: 0.801127 | Val Acc: 0.759166 loss: 0.752426\n",
            "[149/200] Train Acc: 0.750449 Loss: 0.800191 | Val Acc: 0.759910 loss: 0.749796\n",
            "[150/200] Train Acc: 0.750624 Loss: 0.800179 | Val Acc: 0.759337 loss: 0.752231\n",
            "[151/200] Train Acc: 0.750044 Loss: 0.800811 | Val Acc: 0.759170 loss: 0.752344\n",
            "[152/200] Train Acc: 0.749711 Loss: 0.800912 | Val Acc: 0.758780 loss: 0.751936\n",
            "[153/200] Train Acc: 0.750387 Loss: 0.800650 | Val Acc: 0.759873 loss: 0.750598\n",
            "[154/200] Train Acc: 0.749657 Loss: 0.801911 | Val Acc: 0.760231 loss: 0.750680\n",
            "[155/200] Train Acc: 0.750146 Loss: 0.801856 | Val Acc: 0.759821 loss: 0.752144\n",
            "[156/200] Train Acc: 0.750217 Loss: 0.799375 | Val Acc: 0.760361 loss: 0.749603\n",
            "[157/200] Train Acc: 0.750344 Loss: 0.800019 | Val Acc: 0.759085 loss: 0.750659\n",
            "[158/200] Train Acc: 0.750670 Loss: 0.799564 | Val Acc: 0.759654 loss: 0.749880\n",
            "[159/200] Train Acc: 0.750117 Loss: 0.799894 | Val Acc: 0.760097 loss: 0.749390\n",
            "[160/200] Train Acc: 0.750268 Loss: 0.798099 | Val Acc: 0.760528 loss: 0.748225\n",
            "[161/200] Train Acc: 0.751069 Loss: 0.798537 | Val Acc: 0.759207 loss: 0.751486\n",
            "[162/200] Train Acc: 0.750590 Loss: 0.800051 | Val Acc: 0.760853 loss: 0.745959\n",
            "[163/200] Train Acc: 0.750735 Loss: 0.798753 | Val Acc: 0.758430 loss: 0.752813\n",
            "[164/200] Train Acc: 0.750959 Loss: 0.798761 | Val Acc: 0.760308 loss: 0.748542\n",
            "[165/200] Train Acc: 0.750862 Loss: 0.799266 | Val Acc: 0.760662 loss: 0.746455\n",
            "[166/200] Train Acc: 0.750806 Loss: 0.798079 | Val Acc: 0.760975 loss: 0.749788\n",
            "[167/200] Train Acc: 0.751269 Loss: 0.798270 | Val Acc: 0.760447 loss: 0.747107\n",
            "[168/200] Train Acc: 0.750351 Loss: 0.799238 | Val Acc: 0.760463 loss: 0.745899\n",
            "[169/200] Train Acc: 0.751232 Loss: 0.797833 | Val Acc: 0.759837 loss: 0.749446\n",
            "[170/200] Train Acc: 0.751222 Loss: 0.798939 | Val Acc: 0.760857 loss: 0.746745\n",
            "[171/200] Train Acc: 0.751422 Loss: 0.797271 | Val Acc: 0.759211 loss: 0.751123\n",
            "[172/200] Train Acc: 0.751010 Loss: 0.797927 | Val Acc: 0.760894 loss: 0.748900\n",
            "[173/200] Train Acc: 0.751156 Loss: 0.797507 | Val Acc: 0.759227 loss: 0.751025\n",
            "[174/200] Train Acc: 0.751131 Loss: 0.797255 | Val Acc: 0.759186 loss: 0.750162\n",
            "[175/200] Train Acc: 0.751456 Loss: 0.797176 | Val Acc: 0.760150 loss: 0.749203\n",
            "[176/200] Train Acc: 0.751511 Loss: 0.796953 | Val Acc: 0.759621 loss: 0.749865\n",
            "[177/200] Train Acc: 0.751397 Loss: 0.796832 | Val Acc: 0.760052 loss: 0.749650\n",
            "[178/200] Train Acc: 0.751309 Loss: 0.797425 | Val Acc: 0.760540 loss: 0.747573\n",
            "[179/200] Train Acc: 0.751556 Loss: 0.796626 | Val Acc: 0.760142 loss: 0.750459\n",
            "[180/200] Train Acc: 0.751682 Loss: 0.796856 | Val Acc: 0.759898 loss: 0.750048\n",
            "[181/200] Train Acc: 0.750986 Loss: 0.798308 | Val Acc: 0.760772 loss: 0.750266\n",
            "[182/200] Train Acc: 0.751589 Loss: 0.796249 | Val Acc: 0.760365 loss: 0.748641\n",
            "[183/200] Train Acc: 0.751453 Loss: 0.796550 | Val Acc: 0.760463 loss: 0.749364\n",
            "[184/200] Train Acc: 0.751260 Loss: 0.798346 | Val Acc: 0.759577 loss: 0.749411\n",
            "[185/200] Train Acc: 0.751629 Loss: 0.795016 | Val Acc: 0.759861 loss: 0.749906\n",
            "[186/200] Train Acc: 0.751383 Loss: 0.795772 | Val Acc: 0.760426 loss: 0.748953\n",
            "[187/200] Train Acc: 0.751851 Loss: 0.795612 | Val Acc: 0.760963 loss: 0.750014\n",
            "[188/200] Train Acc: 0.751727 Loss: 0.795571 | Val Acc: 0.760418 loss: 0.746851\n",
            "[189/200] Train Acc: 0.751613 Loss: 0.795839 | Val Acc: 0.758918 loss: 0.752179\n",
            "[190/200] Train Acc: 0.751574 Loss: 0.795641 | Val Acc: 0.759861 loss: 0.752957\n",
            "[191/200] Train Acc: 0.752174 Loss: 0.794894 | Val Acc: 0.762069 loss: 0.744132\n",
            "saving model with acc 0.762\n",
            "[192/200] Train Acc: 0.751751 Loss: 0.796567 | Val Acc: 0.760020 loss: 0.749431\n",
            "[193/200] Train Acc: 0.751924 Loss: 0.794768 | Val Acc: 0.760239 loss: 0.748308\n",
            "[194/200] Train Acc: 0.751913 Loss: 0.795190 | Val Acc: 0.758995 loss: 0.749367\n",
            "[195/200] Train Acc: 0.752024 Loss: 0.794218 | Val Acc: 0.759247 loss: 0.752733\n",
            "[196/200] Train Acc: 0.751719 Loss: 0.794766 | Val Acc: 0.759674 loss: 0.750726\n",
            "[197/200] Train Acc: 0.752368 Loss: 0.794465 | Val Acc: 0.758987 loss: 0.751500\n",
            "[198/200] Train Acc: 0.752182 Loss: 0.794182 | Val Acc: 0.761390 loss: 0.748712\n",
            "[199/200] Train Acc: 0.751941 Loss: 0.793855 | Val Acc: 0.759548 loss: 0.750824\n",
            "[200/200] Train Acc: 0.752002 Loss: 0.794251 | Val Acc: 0.760138 loss: 0.751134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTr80Panaq8"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiBILuUEncLl",
        "outputId": "63a389fa-b4ff-4535-e8fb-5a727e360ac5"
      },
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP_tUJeLnd94"
      },
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIjAsNTnhgs"
      },
      "source": [
        "# **Write prediction to a CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "athJ5R7BnlN5",
        "outputId": "7e6c932a-2b9a-48c1-a8b0-50859be81746"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))\n",
        "\n",
        "print('Saving results to prediction.csv')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to prediction.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV5cHjHLos62"
      },
      "source": [
        "# **Reference**\n",
        "\n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb)"
      ]
    }
  ]
}