{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOeI5NcHokwI7YSyctbTSux",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Offliners/ML/blob/main/HW2/homework2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghYxMSxsmAYm"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**\n",
        "\n",
        "The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "## **Download Data**\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3\n",
        "\n",
        "timit_11/\n",
        "\n",
        "* train_11.npy: training data\n",
        "* train_label_11.npy: training label\n",
        "* test_11.npy: testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N98vZGvSl-Fq",
        "outputId": "a409798a-a79e-49fd-8326-cea8396346e6"
      },
      "source": [
        "!gdown --id '1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG\n",
            "To: /content/data.zip\n",
            "376MB [00:02, 143MB/s]\n",
            "Archive:  data.zip\n",
            "  inflating: sampleSubmission.csv    \n",
            "  inflating: timit_11/timit_11/test_11.npy  \n",
            "  inflating: timit_11/timit_11/train_11.npy  \n",
            "  inflating: timit_11/timit_11/train_label_11.npy  \n",
            "data.zip  sample_data  sampleSubmission.csv  timit_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffxBMT3mU5g"
      },
      "source": [
        "# **Preparing Data**\n",
        "\n",
        "Load the training and testing data from the .npy file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRTYp6bemeRk",
        "outputId": "f13dc4d3-10cc-436f-b3f8-ad5e87ade1f1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpQ84yh8me6A"
      },
      "source": [
        "# **Create Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPvM4GoFmhzo"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmKia2rfmmk9",
        "outputId": "5a6d6a78-2154-49c4-db30-4569192bfd7b"
      },
      "source": [
        "VAL_RATIO = 0.2\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: (983945, 429)\n",
            "Size of validation set: (245987, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1fNcNamqKa"
      },
      "source": [
        "BATCH_SIZE = 2048\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kqycVinmj11"
      },
      "source": [
        "#### **notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7dGNUuGmtDD",
        "outputId": "3569e854-6c7e-4aa6-8921-a86e907ec50e"
      },
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFtiY94m6X-"
      },
      "source": [
        "# **Create Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfFT6XJ8nBCS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(429, 2048)\n",
        "        self.layer2 = nn.Linear(2048, 2048)\n",
        "        self.layer3 = nn.Linear(2048, 1024)\n",
        "        self.layer4 = nn.Linear(1024, 512)\n",
        "        self.layer5 = nn.Linear(512, 128)\n",
        "        self.out = nn.Linear(128, 39) \n",
        "        self.dp = nn.Dropout(0.5)\n",
        "        self.bn1 = nn.BatchNorm1d(2048)\n",
        "        self.bn2 = nn.BatchNorm1d(2048)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.act_fn = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer5(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLq4t_knLUL"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBsChEconOOV",
        "outputId": "e5f2cbdd-1ad0-4b89-f3ae-ee6260bc277e"
      },
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 300               # number of training epoch\n",
        "learning_rate = 1e-4         # learning rate\n",
        "l2 = 1e-3                    # L2 regularization\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyjl3itJnWwL",
        "outputId": "5cf3d831-e6c5-4717-8222-f4103bc9b39e"
      },
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/300] Train Acc: 0.400558 Loss: 2.236807 | Val Acc: 0.563709 loss: 1.487389\n",
            "saving model with acc 0.564\n",
            "[002/300] Train Acc: 0.531868 Loss: 1.630679 | Val Acc: 0.625570 loss: 1.246427\n",
            "saving model with acc 0.626\n",
            "[003/300] Train Acc: 0.576699 Loss: 1.453847 | Val Acc: 0.658807 loss: 1.126477\n",
            "saving model with acc 0.659\n",
            "[004/300] Train Acc: 0.602060 Loss: 1.355697 | Val Acc: 0.675040 loss: 1.064869\n",
            "saving model with acc 0.675\n",
            "[005/300] Train Acc: 0.619835 Loss: 1.289585 | Val Acc: 0.686532 loss: 1.021345\n",
            "saving model with acc 0.687\n",
            "[006/300] Train Acc: 0.633055 Loss: 1.239790 | Val Acc: 0.695927 loss: 0.984057\n",
            "saving model with acc 0.696\n",
            "[007/300] Train Acc: 0.643415 Loss: 1.200590 | Val Acc: 0.702936 loss: 0.959987\n",
            "saving model with acc 0.703\n",
            "[008/300] Train Acc: 0.652175 Loss: 1.165948 | Val Acc: 0.708651 loss: 0.938085\n",
            "saving model with acc 0.709\n",
            "[009/300] Train Acc: 0.659965 Loss: 1.137012 | Val Acc: 0.713680 loss: 0.919240\n",
            "saving model with acc 0.714\n",
            "[010/300] Train Acc: 0.666955 Loss: 1.113301 | Val Acc: 0.718127 loss: 0.906790\n",
            "saving model with acc 0.718\n",
            "[011/300] Train Acc: 0.673239 Loss: 1.088939 | Val Acc: 0.721957 loss: 0.889121\n",
            "saving model with acc 0.722\n",
            "[012/300] Train Acc: 0.677871 Loss: 1.072235 | Val Acc: 0.725945 loss: 0.877318\n",
            "saving model with acc 0.726\n",
            "[013/300] Train Acc: 0.682812 Loss: 1.052260 | Val Acc: 0.727099 loss: 0.872288\n",
            "saving model with acc 0.727\n",
            "[014/300] Train Acc: 0.688325 Loss: 1.033754 | Val Acc: 0.730457 loss: 0.859118\n",
            "saving model with acc 0.730\n",
            "[015/300] Train Acc: 0.691997 Loss: 1.019043 | Val Acc: 0.733006 loss: 0.851144\n",
            "saving model with acc 0.733\n",
            "[016/300] Train Acc: 0.696672 Loss: 1.003933 | Val Acc: 0.735071 loss: 0.843595\n",
            "saving model with acc 0.735\n",
            "[017/300] Train Acc: 0.700180 Loss: 0.990599 | Val Acc: 0.737193 loss: 0.837982\n",
            "saving model with acc 0.737\n",
            "[018/300] Train Acc: 0.703937 Loss: 0.977241 | Val Acc: 0.739376 loss: 0.827500\n",
            "saving model with acc 0.739\n",
            "[019/300] Train Acc: 0.706754 Loss: 0.967490 | Val Acc: 0.740864 loss: 0.822567\n",
            "saving model with acc 0.741\n",
            "[020/300] Train Acc: 0.710178 Loss: 0.955839 | Val Acc: 0.741856 loss: 0.818099\n",
            "saving model with acc 0.742\n",
            "[021/300] Train Acc: 0.712435 Loss: 0.946529 | Val Acc: 0.744678 loss: 0.812371\n",
            "saving model with acc 0.745\n",
            "[022/300] Train Acc: 0.714759 Loss: 0.937280 | Val Acc: 0.744243 loss: 0.807588\n",
            "[023/300] Train Acc: 0.717319 Loss: 0.928974 | Val Acc: 0.745336 loss: 0.807836\n",
            "saving model with acc 0.745\n",
            "[024/300] Train Acc: 0.718634 Loss: 0.921480 | Val Acc: 0.746487 loss: 0.803269\n",
            "saving model with acc 0.746\n",
            "[025/300] Train Acc: 0.720449 Loss: 0.914819 | Val Acc: 0.747145 loss: 0.798281\n",
            "saving model with acc 0.747\n",
            "[026/300] Train Acc: 0.722461 Loss: 0.907435 | Val Acc: 0.748218 loss: 0.795626\n",
            "saving model with acc 0.748\n",
            "[027/300] Train Acc: 0.724267 Loss: 0.901071 | Val Acc: 0.749536 loss: 0.790118\n",
            "saving model with acc 0.750\n",
            "[028/300] Train Acc: 0.726012 Loss: 0.893959 | Val Acc: 0.750747 loss: 0.786505\n",
            "saving model with acc 0.751\n",
            "[029/300] Train Acc: 0.727338 Loss: 0.889595 | Val Acc: 0.749897 loss: 0.788871\n",
            "[030/300] Train Acc: 0.729398 Loss: 0.883169 | Val Acc: 0.750983 loss: 0.785427\n",
            "saving model with acc 0.751\n",
            "[031/300] Train Acc: 0.730674 Loss: 0.877432 | Val Acc: 0.751296 loss: 0.782431\n",
            "saving model with acc 0.751\n",
            "[032/300] Train Acc: 0.732010 Loss: 0.873200 | Val Acc: 0.752223 loss: 0.780687\n",
            "saving model with acc 0.752\n",
            "[033/300] Train Acc: 0.732788 Loss: 0.868994 | Val Acc: 0.751954 loss: 0.777826\n",
            "[034/300] Train Acc: 0.734067 Loss: 0.865468 | Val Acc: 0.753292 loss: 0.777175\n",
            "saving model with acc 0.753\n",
            "[035/300] Train Acc: 0.735226 Loss: 0.860765 | Val Acc: 0.754788 loss: 0.773032\n",
            "saving model with acc 0.755\n",
            "[036/300] Train Acc: 0.736018 Loss: 0.857709 | Val Acc: 0.754446 loss: 0.771351\n",
            "[037/300] Train Acc: 0.737379 Loss: 0.852775 | Val Acc: 0.753206 loss: 0.775910\n",
            "[038/300] Train Acc: 0.737455 Loss: 0.851192 | Val Acc: 0.754605 loss: 0.773201\n",
            "[039/300] Train Acc: 0.738442 Loss: 0.847227 | Val Acc: 0.755255 loss: 0.769297\n",
            "saving model with acc 0.755\n",
            "[040/300] Train Acc: 0.739336 Loss: 0.843777 | Val Acc: 0.755162 loss: 0.769500\n",
            "[041/300] Train Acc: 0.740250 Loss: 0.842129 | Val Acc: 0.755154 loss: 0.768588\n",
            "[042/300] Train Acc: 0.741238 Loss: 0.836980 | Val Acc: 0.754963 loss: 0.766811\n",
            "[043/300] Train Acc: 0.741855 Loss: 0.834652 | Val Acc: 0.755430 loss: 0.769092\n",
            "saving model with acc 0.755\n",
            "[044/300] Train Acc: 0.742744 Loss: 0.832709 | Val Acc: 0.755796 loss: 0.768126\n",
            "saving model with acc 0.756\n",
            "[045/300] Train Acc: 0.742996 Loss: 0.831141 | Val Acc: 0.756349 loss: 0.765773\n",
            "saving model with acc 0.756\n",
            "[046/300] Train Acc: 0.744050 Loss: 0.827032 | Val Acc: 0.756194 loss: 0.763874\n",
            "[047/300] Train Acc: 0.744771 Loss: 0.826206 | Val Acc: 0.756869 loss: 0.761233\n",
            "saving model with acc 0.757\n",
            "[048/300] Train Acc: 0.745599 Loss: 0.822598 | Val Acc: 0.756227 loss: 0.764145\n",
            "[049/300] Train Acc: 0.746065 Loss: 0.821517 | Val Acc: 0.757142 loss: 0.762067\n",
            "saving model with acc 0.757\n",
            "[050/300] Train Acc: 0.746143 Loss: 0.818279 | Val Acc: 0.757841 loss: 0.761971\n",
            "saving model with acc 0.758\n",
            "[051/300] Train Acc: 0.747531 Loss: 0.815659 | Val Acc: 0.757373 loss: 0.760459\n",
            "[052/300] Train Acc: 0.746868 Loss: 0.814178 | Val Acc: 0.757418 loss: 0.761983\n",
            "[053/300] Train Acc: 0.748215 Loss: 0.811437 | Val Acc: 0.756849 loss: 0.764223\n",
            "[054/300] Train Acc: 0.748016 Loss: 0.811685 | Val Acc: 0.756544 loss: 0.764038\n",
            "[055/300] Train Acc: 0.748681 Loss: 0.809985 | Val Acc: 0.757942 loss: 0.762267\n",
            "saving model with acc 0.758\n",
            "[056/300] Train Acc: 0.749777 Loss: 0.807536 | Val Acc: 0.758369 loss: 0.760070\n",
            "saving model with acc 0.758\n",
            "[057/300] Train Acc: 0.749862 Loss: 0.805544 | Val Acc: 0.758056 loss: 0.759634\n",
            "[058/300] Train Acc: 0.751053 Loss: 0.803023 | Val Acc: 0.758776 loss: 0.759178\n",
            "saving model with acc 0.759\n",
            "[059/300] Train Acc: 0.751020 Loss: 0.801579 | Val Acc: 0.759105 loss: 0.756764\n",
            "saving model with acc 0.759\n",
            "[060/300] Train Acc: 0.751624 Loss: 0.800712 | Val Acc: 0.759199 loss: 0.757118\n",
            "saving model with acc 0.759\n",
            "[061/300] Train Acc: 0.751776 Loss: 0.798957 | Val Acc: 0.758512 loss: 0.758177\n",
            "[062/300] Train Acc: 0.752734 Loss: 0.797866 | Val Acc: 0.759284 loss: 0.754562\n",
            "saving model with acc 0.759\n",
            "[063/300] Train Acc: 0.752151 Loss: 0.797176 | Val Acc: 0.757682 loss: 0.758944\n",
            "[064/300] Train Acc: 0.753247 Loss: 0.794090 | Val Acc: 0.759463 loss: 0.756580\n",
            "saving model with acc 0.759\n",
            "[065/300] Train Acc: 0.753528 Loss: 0.794515 | Val Acc: 0.758861 loss: 0.757608\n",
            "[066/300] Train Acc: 0.753563 Loss: 0.791929 | Val Acc: 0.759768 loss: 0.755639\n",
            "saving model with acc 0.760\n",
            "[067/300] Train Acc: 0.754004 Loss: 0.790466 | Val Acc: 0.759491 loss: 0.755401\n",
            "[068/300] Train Acc: 0.754700 Loss: 0.789096 | Val Acc: 0.760207 loss: 0.754021\n",
            "saving model with acc 0.760\n",
            "[069/300] Train Acc: 0.754945 Loss: 0.787763 | Val Acc: 0.759743 loss: 0.753504\n",
            "[070/300] Train Acc: 0.754849 Loss: 0.786842 | Val Acc: 0.758943 loss: 0.757909\n",
            "[071/300] Train Acc: 0.755016 Loss: 0.786349 | Val Acc: 0.759817 loss: 0.756167\n",
            "[072/300] Train Acc: 0.756167 Loss: 0.783407 | Val Acc: 0.759845 loss: 0.755224\n",
            "[073/300] Train Acc: 0.756012 Loss: 0.782843 | Val Acc: 0.759097 loss: 0.755051\n",
            "[074/300] Train Acc: 0.756216 Loss: 0.782500 | Val Acc: 0.760215 loss: 0.753472\n",
            "saving model with acc 0.760\n",
            "[075/300] Train Acc: 0.757153 Loss: 0.780379 | Val Acc: 0.760451 loss: 0.754165\n",
            "saving model with acc 0.760\n",
            "[076/300] Train Acc: 0.757113 Loss: 0.779142 | Val Acc: 0.760910 loss: 0.752679\n",
            "saving model with acc 0.761\n",
            "[077/300] Train Acc: 0.757679 Loss: 0.778731 | Val Acc: 0.760459 loss: 0.755985\n",
            "[078/300] Train Acc: 0.758417 Loss: 0.776938 | Val Acc: 0.759906 loss: 0.753615\n",
            "[079/300] Train Acc: 0.757887 Loss: 0.777469 | Val Acc: 0.760044 loss: 0.755492\n",
            "[080/300] Train Acc: 0.758421 Loss: 0.776313 | Val Acc: 0.760544 loss: 0.752212\n",
            "[081/300] Train Acc: 0.758534 Loss: 0.774798 | Val Acc: 0.760337 loss: 0.754106\n",
            "[082/300] Train Acc: 0.758992 Loss: 0.773499 | Val Acc: 0.760869 loss: 0.753857\n",
            "[083/300] Train Acc: 0.759726 Loss: 0.771450 | Val Acc: 0.761353 loss: 0.752092\n",
            "saving model with acc 0.761\n",
            "[084/300] Train Acc: 0.759605 Loss: 0.771662 | Val Acc: 0.761666 loss: 0.752821\n",
            "saving model with acc 0.762\n",
            "[085/300] Train Acc: 0.760024 Loss: 0.770808 | Val Acc: 0.761052 loss: 0.752368\n",
            "[086/300] Train Acc: 0.759445 Loss: 0.770630 | Val Acc: 0.759605 loss: 0.755381\n",
            "[087/300] Train Acc: 0.760158 Loss: 0.768698 | Val Acc: 0.760792 loss: 0.755211\n",
            "[088/300] Train Acc: 0.760339 Loss: 0.767881 | Val Acc: 0.760382 loss: 0.754736\n",
            "[089/300] Train Acc: 0.760437 Loss: 0.768695 | Val Acc: 0.760662 loss: 0.754558\n",
            "[090/300] Train Acc: 0.760966 Loss: 0.766048 | Val Acc: 0.760386 loss: 0.753855\n",
            "[091/300] Train Acc: 0.761058 Loss: 0.765942 | Val Acc: 0.762227 loss: 0.751019\n",
            "saving model with acc 0.762\n",
            "[092/300] Train Acc: 0.761620 Loss: 0.764478 | Val Acc: 0.760792 loss: 0.752011\n",
            "[093/300] Train Acc: 0.762342 Loss: 0.762504 | Val Acc: 0.761760 loss: 0.750631\n",
            "[094/300] Train Acc: 0.761817 Loss: 0.764118 | Val Acc: 0.761922 loss: 0.751354\n",
            "[095/300] Train Acc: 0.761995 Loss: 0.763713 | Val Acc: 0.761910 loss: 0.753232\n",
            "[096/300] Train Acc: 0.762906 Loss: 0.760798 | Val Acc: 0.762048 loss: 0.752879\n",
            "[097/300] Train Acc: 0.762181 Loss: 0.761461 | Val Acc: 0.761052 loss: 0.751052\n",
            "[098/300] Train Acc: 0.763296 Loss: 0.758579 | Val Acc: 0.760642 loss: 0.752789\n",
            "[099/300] Train Acc: 0.763133 Loss: 0.759831 | Val Acc: 0.761520 loss: 0.752168\n",
            "[100/300] Train Acc: 0.762904 Loss: 0.759007 | Val Acc: 0.760552 loss: 0.753117\n",
            "[101/300] Train Acc: 0.762862 Loss: 0.758906 | Val Acc: 0.762016 loss: 0.748772\n",
            "[102/300] Train Acc: 0.763199 Loss: 0.758260 | Val Acc: 0.761951 loss: 0.751921\n",
            "[103/300] Train Acc: 0.763327 Loss: 0.757599 | Val Acc: 0.761345 loss: 0.749162\n",
            "[104/300] Train Acc: 0.763798 Loss: 0.755387 | Val Acc: 0.761040 loss: 0.755195\n",
            "[105/300] Train Acc: 0.763999 Loss: 0.755869 | Val Acc: 0.761309 loss: 0.751685\n",
            "[106/300] Train Acc: 0.763927 Loss: 0.755855 | Val Acc: 0.761243 loss: 0.754591\n",
            "[107/300] Train Acc: 0.764101 Loss: 0.755875 | Val Acc: 0.761987 loss: 0.749618\n",
            "[108/300] Train Acc: 0.764645 Loss: 0.754019 | Val Acc: 0.761959 loss: 0.752138\n",
            "[109/300] Train Acc: 0.764475 Loss: 0.753065 | Val Acc: 0.760422 loss: 0.753632\n",
            "[110/300] Train Acc: 0.764919 Loss: 0.753224 | Val Acc: 0.762097 loss: 0.749474\n",
            "[111/300] Train Acc: 0.764648 Loss: 0.752230 | Val Acc: 0.761418 loss: 0.755601\n",
            "[112/300] Train Acc: 0.765577 Loss: 0.751816 | Val Acc: 0.760874 loss: 0.753540\n",
            "[113/300] Train Acc: 0.764967 Loss: 0.752307 | Val Acc: 0.761841 loss: 0.751405\n",
            "[114/300] Train Acc: 0.765787 Loss: 0.751171 | Val Acc: 0.761528 loss: 0.751596\n",
            "[115/300] Train Acc: 0.766020 Loss: 0.749371 | Val Acc: 0.762630 loss: 0.751359\n",
            "saving model with acc 0.763\n",
            "[116/300] Train Acc: 0.765658 Loss: 0.751076 | Val Acc: 0.761085 loss: 0.752739\n",
            "[117/300] Train Acc: 0.765543 Loss: 0.750090 | Val Acc: 0.760902 loss: 0.753435\n",
            "[118/300] Train Acc: 0.766258 Loss: 0.748538 | Val Acc: 0.761910 loss: 0.751279\n",
            "[119/300] Train Acc: 0.765891 Loss: 0.747913 | Val Acc: 0.762309 loss: 0.753604\n",
            "[120/300] Train Acc: 0.766411 Loss: 0.747654 | Val Acc: 0.761569 loss: 0.755524\n",
            "[121/300] Train Acc: 0.766404 Loss: 0.747680 | Val Acc: 0.762268 loss: 0.750416\n",
            "[122/300] Train Acc: 0.766750 Loss: 0.746970 | Val Acc: 0.762244 loss: 0.754139\n",
            "[123/300] Train Acc: 0.766339 Loss: 0.747789 | Val Acc: 0.762048 loss: 0.753829\n",
            "[124/300] Train Acc: 0.766106 Loss: 0.746707 | Val Acc: 0.761097 loss: 0.755257\n",
            "[125/300] Train Acc: 0.766549 Loss: 0.746358 | Val Acc: 0.761581 loss: 0.753204\n",
            "[126/300] Train Acc: 0.766540 Loss: 0.745945 | Val Acc: 0.761959 loss: 0.752772\n",
            "[127/300] Train Acc: 0.767200 Loss: 0.744459 | Val Acc: 0.762459 loss: 0.751542\n",
            "[128/300] Train Acc: 0.767352 Loss: 0.744028 | Val Acc: 0.762036 loss: 0.752512\n",
            "[129/300] Train Acc: 0.767371 Loss: 0.743844 | Val Acc: 0.761735 loss: 0.752025\n",
            "[130/300] Train Acc: 0.767077 Loss: 0.744273 | Val Acc: 0.761849 loss: 0.755797\n",
            "[131/300] Train Acc: 0.766857 Loss: 0.744818 | Val Acc: 0.762113 loss: 0.751223\n",
            "[132/300] Train Acc: 0.767886 Loss: 0.742752 | Val Acc: 0.763398 loss: 0.752808\n",
            "saving model with acc 0.763\n",
            "[133/300] Train Acc: 0.767676 Loss: 0.743374 | Val Acc: 0.763150 loss: 0.753816\n",
            "[134/300] Train Acc: 0.767436 Loss: 0.742538 | Val Acc: 0.762012 loss: 0.753015\n",
            "[135/300] Train Acc: 0.767820 Loss: 0.742425 | Val Acc: 0.762573 loss: 0.751777\n",
            "[136/300] Train Acc: 0.768056 Loss: 0.742282 | Val Acc: 0.762435 loss: 0.750906\n",
            "[137/300] Train Acc: 0.768180 Loss: 0.741297 | Val Acc: 0.761788 loss: 0.752701\n",
            "[138/300] Train Acc: 0.768559 Loss: 0.740914 | Val Acc: 0.763179 loss: 0.747554\n",
            "[139/300] Train Acc: 0.768093 Loss: 0.741372 | Val Acc: 0.763231 loss: 0.749081\n",
            "[140/300] Train Acc: 0.768188 Loss: 0.741747 | Val Acc: 0.763179 loss: 0.749518\n",
            "[141/300] Train Acc: 0.768453 Loss: 0.740475 | Val Acc: 0.762565 loss: 0.753630\n",
            "[142/300] Train Acc: 0.769263 Loss: 0.738079 | Val Acc: 0.762174 loss: 0.755402\n",
            "[143/300] Train Acc: 0.768906 Loss: 0.739131 | Val Acc: 0.762971 loss: 0.750598\n",
            "[144/300] Train Acc: 0.768529 Loss: 0.739253 | Val Acc: 0.762422 loss: 0.751265\n",
            "[145/300] Train Acc: 0.768375 Loss: 0.739438 | Val Acc: 0.763150 loss: 0.752219\n",
            "[146/300] Train Acc: 0.768611 Loss: 0.738667 | Val Acc: 0.762626 loss: 0.752146\n",
            "[147/300] Train Acc: 0.769465 Loss: 0.738765 | Val Acc: 0.763760 loss: 0.746614\n",
            "saving model with acc 0.764\n",
            "[148/300] Train Acc: 0.769027 Loss: 0.738557 | Val Acc: 0.762300 loss: 0.749396\n",
            "[149/300] Train Acc: 0.768671 Loss: 0.738473 | Val Acc: 0.762435 loss: 0.751481\n",
            "[150/300] Train Acc: 0.769476 Loss: 0.737151 | Val Acc: 0.762000 loss: 0.752242\n",
            "[151/300] Train Acc: 0.769349 Loss: 0.736474 | Val Acc: 0.762723 loss: 0.747065\n",
            "[152/300] Train Acc: 0.769366 Loss: 0.737565 | Val Acc: 0.762378 loss: 0.750812\n",
            "[153/300] Train Acc: 0.769590 Loss: 0.735654 | Val Acc: 0.761947 loss: 0.753726\n",
            "[154/300] Train Acc: 0.769635 Loss: 0.736511 | Val Acc: 0.762183 loss: 0.752789\n",
            "[155/300] Train Acc: 0.769886 Loss: 0.735412 | Val Acc: 0.762353 loss: 0.754172\n",
            "[156/300] Train Acc: 0.769807 Loss: 0.735766 | Val Acc: 0.762723 loss: 0.751987\n",
            "[157/300] Train Acc: 0.770153 Loss: 0.735430 | Val Acc: 0.762683 loss: 0.750136\n",
            "[158/300] Train Acc: 0.769484 Loss: 0.734876 | Val Acc: 0.762784 loss: 0.753801\n",
            "[159/300] Train Acc: 0.769853 Loss: 0.735432 | Val Acc: 0.763162 loss: 0.748706\n",
            "[160/300] Train Acc: 0.769865 Loss: 0.734596 | Val Acc: 0.763650 loss: 0.750054\n",
            "[161/300] Train Acc: 0.770132 Loss: 0.733813 | Val Acc: 0.762552 loss: 0.750989\n",
            "[162/300] Train Acc: 0.770270 Loss: 0.732878 | Val Acc: 0.763069 loss: 0.750441\n",
            "[163/300] Train Acc: 0.770416 Loss: 0.734236 | Val Acc: 0.762386 loss: 0.754158\n",
            "[164/300] Train Acc: 0.770465 Loss: 0.733875 | Val Acc: 0.763585 loss: 0.749858\n",
            "[165/300] Train Acc: 0.770520 Loss: 0.733602 | Val Acc: 0.762735 loss: 0.751170\n",
            "[166/300] Train Acc: 0.770601 Loss: 0.732709 | Val Acc: 0.762866 loss: 0.750536\n",
            "[167/300] Train Acc: 0.770486 Loss: 0.732852 | Val Acc: 0.762300 loss: 0.752064\n",
            "[168/300] Train Acc: 0.770889 Loss: 0.732215 | Val Acc: 0.764175 loss: 0.749075\n",
            "saving model with acc 0.764\n",
            "[169/300] Train Acc: 0.770780 Loss: 0.732238 | Val Acc: 0.762264 loss: 0.752288\n",
            "[170/300] Train Acc: 0.770706 Loss: 0.732172 | Val Acc: 0.763134 loss: 0.750373\n",
            "[171/300] Train Acc: 0.770930 Loss: 0.731708 | Val Acc: 0.763081 loss: 0.748979\n",
            "[172/300] Train Acc: 0.771182 Loss: 0.731266 | Val Acc: 0.763170 loss: 0.751623\n",
            "[173/300] Train Acc: 0.771148 Loss: 0.731359 | Val Acc: 0.761817 loss: 0.752396\n",
            "[174/300] Train Acc: 0.770717 Loss: 0.731794 | Val Acc: 0.762914 loss: 0.750743\n",
            "[175/300] Train Acc: 0.771661 Loss: 0.730246 | Val Acc: 0.763467 loss: 0.750295\n",
            "[176/300] Train Acc: 0.771914 Loss: 0.729657 | Val Acc: 0.762870 loss: 0.752393\n",
            "[177/300] Train Acc: 0.771228 Loss: 0.730358 | Val Acc: 0.763203 loss: 0.747741\n",
            "[178/300] Train Acc: 0.771392 Loss: 0.730903 | Val Acc: 0.763467 loss: 0.748070\n",
            "[179/300] Train Acc: 0.772027 Loss: 0.729858 | Val Acc: 0.763118 loss: 0.752274\n",
            "[180/300] Train Acc: 0.770927 Loss: 0.731103 | Val Acc: 0.763300 loss: 0.750285\n",
            "[181/300] Train Acc: 0.770679 Loss: 0.730134 | Val Acc: 0.763557 loss: 0.751631\n",
            "[182/300] Train Acc: 0.771881 Loss: 0.728690 | Val Acc: 0.763252 loss: 0.752280\n",
            "[183/300] Train Acc: 0.772041 Loss: 0.728240 | Val Acc: 0.763162 loss: 0.754371\n",
            "[184/300] Train Acc: 0.771091 Loss: 0.729554 | Val Acc: 0.762195 loss: 0.753003\n",
            "[185/300] Train Acc: 0.771549 Loss: 0.728964 | Val Acc: 0.762569 loss: 0.754000\n",
            "[186/300] Train Acc: 0.771789 Loss: 0.729062 | Val Acc: 0.763406 loss: 0.752516\n",
            "[187/300] Train Acc: 0.771826 Loss: 0.728863 | Val Acc: 0.763223 loss: 0.754141\n",
            "[188/300] Train Acc: 0.771990 Loss: 0.728050 | Val Acc: 0.763069 loss: 0.749828\n",
            "[189/300] Train Acc: 0.771960 Loss: 0.727238 | Val Acc: 0.762805 loss: 0.753219\n",
            "[190/300] Train Acc: 0.771969 Loss: 0.727443 | Val Acc: 0.764199 loss: 0.750625\n",
            "saving model with acc 0.764\n",
            "[191/300] Train Acc: 0.772432 Loss: 0.726446 | Val Acc: 0.763475 loss: 0.753696\n",
            "[192/300] Train Acc: 0.772241 Loss: 0.728233 | Val Acc: 0.762971 loss: 0.751382\n",
            "[193/300] Train Acc: 0.772142 Loss: 0.727389 | Val Acc: 0.763565 loss: 0.751354\n",
            "[194/300] Train Acc: 0.772159 Loss: 0.728141 | Val Acc: 0.763321 loss: 0.751158\n",
            "[195/300] Train Acc: 0.772309 Loss: 0.726787 | Val Acc: 0.763349 loss: 0.752035\n",
            "[196/300] Train Acc: 0.772717 Loss: 0.725268 | Val Acc: 0.763459 loss: 0.751548\n",
            "[197/300] Train Acc: 0.772601 Loss: 0.726076 | Val Acc: 0.762947 loss: 0.752662\n",
            "[198/300] Train Acc: 0.771810 Loss: 0.727657 | Val Acc: 0.763927 loss: 0.749802\n",
            "[199/300] Train Acc: 0.772715 Loss: 0.726012 | Val Acc: 0.763707 loss: 0.750515\n",
            "[200/300] Train Acc: 0.772215 Loss: 0.726128 | Val Acc: 0.763105 loss: 0.752616\n",
            "[201/300] Train Acc: 0.772870 Loss: 0.726318 | Val Acc: 0.763699 loss: 0.752425\n",
            "[202/300] Train Acc: 0.772621 Loss: 0.725050 | Val Acc: 0.763821 loss: 0.750954\n",
            "[203/300] Train Acc: 0.772242 Loss: 0.725942 | Val Acc: 0.763687 loss: 0.752301\n",
            "[204/300] Train Acc: 0.772430 Loss: 0.726209 | Val Acc: 0.762727 loss: 0.754378\n",
            "[205/300] Train Acc: 0.772886 Loss: 0.725328 | Val Acc: 0.762471 loss: 0.755902\n",
            "[206/300] Train Acc: 0.772830 Loss: 0.725452 | Val Acc: 0.763073 loss: 0.753292\n",
            "[207/300] Train Acc: 0.772721 Loss: 0.724915 | Val Acc: 0.764500 loss: 0.753205\n",
            "saving model with acc 0.764\n",
            "[208/300] Train Acc: 0.773062 Loss: 0.725066 | Val Acc: 0.762544 loss: 0.753783\n",
            "[209/300] Train Acc: 0.773226 Loss: 0.724042 | Val Acc: 0.763914 loss: 0.752229\n",
            "[210/300] Train Acc: 0.773074 Loss: 0.724572 | Val Acc: 0.763837 loss: 0.749107\n",
            "[211/300] Train Acc: 0.773259 Loss: 0.723477 | Val Acc: 0.764248 loss: 0.750983\n",
            "[212/300] Train Acc: 0.773784 Loss: 0.723911 | Val Acc: 0.763675 loss: 0.750787\n",
            "[213/300] Train Acc: 0.773244 Loss: 0.722657 | Val Acc: 0.763321 loss: 0.749900\n",
            "[214/300] Train Acc: 0.772953 Loss: 0.723738 | Val Acc: 0.763394 loss: 0.751165\n",
            "[215/300] Train Acc: 0.773441 Loss: 0.722605 | Val Acc: 0.764427 loss: 0.750253\n",
            "[216/300] Train Acc: 0.773765 Loss: 0.723058 | Val Acc: 0.763024 loss: 0.754200\n",
            "[217/300] Train Acc: 0.773600 Loss: 0.723032 | Val Acc: 0.763414 loss: 0.754215\n",
            "[218/300] Train Acc: 0.773542 Loss: 0.722264 | Val Acc: 0.763053 loss: 0.752866\n",
            "[219/300] Train Acc: 0.773034 Loss: 0.722345 | Val Acc: 0.763183 loss: 0.751267\n",
            "[220/300] Train Acc: 0.773475 Loss: 0.721934 | Val Acc: 0.763085 loss: 0.750834\n",
            "[221/300] Train Acc: 0.773336 Loss: 0.722624 | Val Acc: 0.762857 loss: 0.751084\n",
            "[222/300] Train Acc: 0.773843 Loss: 0.720626 | Val Acc: 0.762780 loss: 0.750331\n",
            "[223/300] Train Acc: 0.773628 Loss: 0.721789 | Val Acc: 0.763422 loss: 0.754629\n",
            "[224/300] Train Acc: 0.773953 Loss: 0.721211 | Val Acc: 0.763174 loss: 0.752170\n",
            "[225/300] Train Acc: 0.774140 Loss: 0.721062 | Val Acc: 0.763219 loss: 0.752320\n",
            "[226/300] Train Acc: 0.773371 Loss: 0.722505 | Val Acc: 0.762992 loss: 0.751596\n",
            "[227/300] Train Acc: 0.773955 Loss: 0.721531 | Val Acc: 0.763044 loss: 0.752190\n",
            "[228/300] Train Acc: 0.773700 Loss: 0.722586 | Val Acc: 0.763626 loss: 0.748613\n",
            "[229/300] Train Acc: 0.773355 Loss: 0.723148 | Val Acc: 0.763719 loss: 0.747942\n",
            "[230/300] Train Acc: 0.774003 Loss: 0.720719 | Val Acc: 0.763910 loss: 0.750750\n",
            "[231/300] Train Acc: 0.774072 Loss: 0.720323 | Val Acc: 0.763614 loss: 0.750781\n",
            "[232/300] Train Acc: 0.773567 Loss: 0.722667 | Val Acc: 0.764219 loss: 0.748261\n",
            "[233/300] Train Acc: 0.774415 Loss: 0.720716 | Val Acc: 0.763443 loss: 0.750645\n",
            "[234/300] Train Acc: 0.774337 Loss: 0.719103 | Val Acc: 0.763183 loss: 0.753214\n",
            "[235/300] Train Acc: 0.773844 Loss: 0.720494 | Val Acc: 0.764451 loss: 0.748225\n",
            "[236/300] Train Acc: 0.773723 Loss: 0.721107 | Val Acc: 0.764150 loss: 0.748385\n",
            "[237/300] Train Acc: 0.774482 Loss: 0.720076 | Val Acc: 0.763882 loss: 0.748167\n",
            "[238/300] Train Acc: 0.774299 Loss: 0.720318 | Val Acc: 0.763109 loss: 0.753102\n",
            "[239/300] Train Acc: 0.773784 Loss: 0.721258 | Val Acc: 0.763979 loss: 0.748208\n",
            "[240/300] Train Acc: 0.774641 Loss: 0.719139 | Val Acc: 0.762146 loss: 0.753176\n",
            "[241/300] Train Acc: 0.774263 Loss: 0.719276 | Val Acc: 0.762670 loss: 0.752977\n",
            "[242/300] Train Acc: 0.774024 Loss: 0.719592 | Val Acc: 0.765097 loss: 0.746378\n",
            "saving model with acc 0.765\n",
            "[243/300] Train Acc: 0.773892 Loss: 0.719714 | Val Acc: 0.764024 loss: 0.747158\n",
            "[244/300] Train Acc: 0.774420 Loss: 0.719085 | Val Acc: 0.763341 loss: 0.753679\n",
            "[245/300] Train Acc: 0.774569 Loss: 0.719583 | Val Acc: 0.763996 loss: 0.750637\n",
            "[246/300] Train Acc: 0.774444 Loss: 0.719625 | Val Acc: 0.762772 loss: 0.754105\n",
            "[247/300] Train Acc: 0.774572 Loss: 0.718334 | Val Acc: 0.762508 loss: 0.755082\n",
            "[248/300] Train Acc: 0.775061 Loss: 0.718826 | Val Acc: 0.764313 loss: 0.750953\n",
            "[249/300] Train Acc: 0.774561 Loss: 0.719010 | Val Acc: 0.763386 loss: 0.755926\n",
            "[250/300] Train Acc: 0.775035 Loss: 0.717301 | Val Acc: 0.764516 loss: 0.749608\n",
            "[251/300] Train Acc: 0.774722 Loss: 0.718555 | Val Acc: 0.764199 loss: 0.750992\n",
            "[252/300] Train Acc: 0.774642 Loss: 0.718850 | Val Acc: 0.764378 loss: 0.750455\n",
            "[253/300] Train Acc: 0.773967 Loss: 0.718907 | Val Acc: 0.762829 loss: 0.754233\n",
            "[254/300] Train Acc: 0.774670 Loss: 0.717707 | Val Acc: 0.763874 loss: 0.752784\n",
            "[255/300] Train Acc: 0.774566 Loss: 0.717532 | Val Acc: 0.763406 loss: 0.753075\n",
            "[256/300] Train Acc: 0.774794 Loss: 0.717012 | Val Acc: 0.762622 loss: 0.756606\n",
            "[257/300] Train Acc: 0.774522 Loss: 0.717736 | Val Acc: 0.763605 loss: 0.750750\n",
            "[258/300] Train Acc: 0.774577 Loss: 0.718347 | Val Acc: 0.762894 loss: 0.753331\n",
            "[259/300] Train Acc: 0.775550 Loss: 0.717329 | Val Acc: 0.763000 loss: 0.749982\n",
            "[260/300] Train Acc: 0.775407 Loss: 0.717132 | Val Acc: 0.763280 loss: 0.753427\n",
            "[261/300] Train Acc: 0.774414 Loss: 0.718190 | Val Acc: 0.763158 loss: 0.752242\n",
            "[262/300] Train Acc: 0.775257 Loss: 0.716915 | Val Acc: 0.763142 loss: 0.752952\n",
            "[263/300] Train Acc: 0.775297 Loss: 0.717295 | Val Acc: 0.763727 loss: 0.750130\n",
            "[264/300] Train Acc: 0.775067 Loss: 0.717014 | Val Acc: 0.762426 loss: 0.757875\n",
            "[265/300] Train Acc: 0.775088 Loss: 0.716340 | Val Acc: 0.763284 loss: 0.755321\n",
            "[266/300] Train Acc: 0.775269 Loss: 0.717177 | Val Acc: 0.763735 loss: 0.750582\n",
            "[267/300] Train Acc: 0.775025 Loss: 0.717824 | Val Acc: 0.764012 loss: 0.749872\n",
            "[268/300] Train Acc: 0.775563 Loss: 0.714689 | Val Acc: 0.763296 loss: 0.753853\n",
            "[269/300] Train Acc: 0.775142 Loss: 0.716750 | Val Acc: 0.765699 loss: 0.745892\n",
            "saving model with acc 0.766\n",
            "[270/300] Train Acc: 0.775365 Loss: 0.716783 | Val Acc: 0.764329 loss: 0.751096\n",
            "[271/300] Train Acc: 0.775010 Loss: 0.717204 | Val Acc: 0.763524 loss: 0.751220\n",
            "[272/300] Train Acc: 0.775071 Loss: 0.716641 | Val Acc: 0.762996 loss: 0.755723\n",
            "[273/300] Train Acc: 0.775545 Loss: 0.715596 | Val Acc: 0.762699 loss: 0.753328\n",
            "[274/300] Train Acc: 0.775084 Loss: 0.716090 | Val Acc: 0.763516 loss: 0.751475\n",
            "[275/300] Train Acc: 0.775192 Loss: 0.715283 | Val Acc: 0.763650 loss: 0.753168\n",
            "[276/300] Train Acc: 0.775420 Loss: 0.717103 | Val Acc: 0.764366 loss: 0.747901\n",
            "[277/300] Train Acc: 0.776295 Loss: 0.714575 | Val Acc: 0.764394 loss: 0.749485\n",
            "[278/300] Train Acc: 0.775830 Loss: 0.715181 | Val Acc: 0.763715 loss: 0.752258\n",
            "[279/300] Train Acc: 0.774757 Loss: 0.716923 | Val Acc: 0.763097 loss: 0.752736\n",
            "[280/300] Train Acc: 0.775325 Loss: 0.715512 | Val Acc: 0.763784 loss: 0.750181\n",
            "[281/300] Train Acc: 0.775568 Loss: 0.714172 | Val Acc: 0.762756 loss: 0.752733\n",
            "[282/300] Train Acc: 0.775752 Loss: 0.715080 | Val Acc: 0.763260 loss: 0.755080\n",
            "[283/300] Train Acc: 0.775424 Loss: 0.716120 | Val Acc: 0.763020 loss: 0.752678\n",
            "[284/300] Train Acc: 0.775429 Loss: 0.714072 | Val Acc: 0.763679 loss: 0.751089\n",
            "[285/300] Train Acc: 0.775415 Loss: 0.714748 | Val Acc: 0.763898 loss: 0.750872\n",
            "[286/300] Train Acc: 0.775839 Loss: 0.714328 | Val Acc: 0.763565 loss: 0.754178\n",
            "[287/300] Train Acc: 0.775665 Loss: 0.715506 | Val Acc: 0.763004 loss: 0.752688\n",
            "[288/300] Train Acc: 0.775325 Loss: 0.715407 | Val Acc: 0.763983 loss: 0.748665\n",
            "[289/300] Train Acc: 0.775344 Loss: 0.715157 | Val Acc: 0.762695 loss: 0.755758\n",
            "[290/300] Train Acc: 0.775723 Loss: 0.714812 | Val Acc: 0.763459 loss: 0.750325\n",
            "[291/300] Train Acc: 0.776340 Loss: 0.713399 | Val Acc: 0.764569 loss: 0.750562\n",
            "[292/300] Train Acc: 0.775703 Loss: 0.714368 | Val Acc: 0.763305 loss: 0.750840\n",
            "[293/300] Train Acc: 0.776180 Loss: 0.714380 | Val Acc: 0.764927 loss: 0.747736\n",
            "[294/300] Train Acc: 0.775777 Loss: 0.714868 | Val Acc: 0.763939 loss: 0.751700\n",
            "[295/300] Train Acc: 0.775991 Loss: 0.714025 | Val Acc: 0.763744 loss: 0.749877\n",
            "[296/300] Train Acc: 0.775847 Loss: 0.714653 | Val Acc: 0.763601 loss: 0.750449\n",
            "[297/300] Train Acc: 0.776610 Loss: 0.713685 | Val Acc: 0.763634 loss: 0.753063\n",
            "[298/300] Train Acc: 0.775651 Loss: 0.714500 | Val Acc: 0.763435 loss: 0.753597\n",
            "[299/300] Train Acc: 0.776101 Loss: 0.713669 | Val Acc: 0.763040 loss: 0.751508\n",
            "[300/300] Train Acc: 0.776448 Loss: 0.712966 | Val Acc: 0.764195 loss: 0.747653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTr80Panaq8"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiBILuUEncLl",
        "outputId": "8772990d-07a8-48eb-df38-d173a1ad0ea8"
      },
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP_tUJeLnd94"
      },
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIjAsNTnhgs"
      },
      "source": [
        "# **Write prediction to a CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "athJ5R7BnlN5",
        "outputId": "9cb9232a-e2f4-4cf3-8bfd-193175db5773"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))\n",
        "\n",
        "print('Saving results to prediction.csv')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to prediction.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV5cHjHLos62"
      },
      "source": [
        "# **Reference**\n",
        "\n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb)"
      ]
    }
  ]
}