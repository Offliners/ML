{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOzn1bFCbr5k81iuSYT7Qew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Offliners/ML/blob/main/HW2/homework2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghYxMSxsmAYm"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**\r\n",
        "\r\n",
        "The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\r\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\r\n",
        "\r\n",
        "This homework is a multiclass classification task, we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\r\n",
        "\r\n",
        "## **Download Data**\r\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3\r\n",
        "\r\n",
        "timit_11/\r\n",
        "\r\n",
        "* train_11.npy: training data\r\n",
        "* train_label_11.npy: training label\r\n",
        "* test_11.npy: testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N98vZGvSl-Fq",
        "outputId": "2e1d5a14-e446-43f6-e3c8-f14a369e54a7"
      },
      "source": [
        "!gdown --id '1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG' --output data.zip\r\n",
        "!unzip data.zip\r\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG\n",
            "To: /content/data.zip\n",
            "376MB [00:03, 124MB/s]\n",
            "Archive:  data.zip\n",
            "  inflating: sampleSubmission.csv    \n",
            "  inflating: timit_11/timit_11/test_11.npy  \n",
            "  inflating: timit_11/timit_11/train_11.npy  \n",
            "  inflating: timit_11/timit_11/train_label_11.npy  \n",
            "data.zip  sample_data  sampleSubmission.csv  timit_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffxBMT3mU5g"
      },
      "source": [
        "# **Preparing Data**\r\n",
        "\r\n",
        "Load the training and testing data from the .npy file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRTYp6bemeRk",
        "outputId": "3305bce1-a9e6-43d5-855a-af728d4be5b6"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "print('Loading data ...')\r\n",
        "\r\n",
        "data_root='./timit_11/timit_11/'\r\n",
        "train = np.load(data_root + 'train_11.npy')\r\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\r\n",
        "test = np.load(data_root + 'test_11.npy')\r\n",
        "\r\n",
        "print('Size of training data: {}'.format(train.shape))\r\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpQ84yh8me6A"
      },
      "source": [
        "# **Create Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPvM4GoFmhzo"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import Dataset\r\n",
        "\r\n",
        "class TIMITDataset(Dataset):\r\n",
        "    def __init__(self, X, y=None):\r\n",
        "        self.data = torch.from_numpy(X).float()\r\n",
        "        if y is not None:\r\n",
        "            y = y.astype(np.int)\r\n",
        "            self.label = torch.LongTensor(y)\r\n",
        "        else:\r\n",
        "            self.label = None\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if self.label is not None:\r\n",
        "            return self.data[idx], self.label[idx]\r\n",
        "        else:\r\n",
        "            return self.data[idx]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmKia2rfmmk9",
        "outputId": "212c37a8-df32-47cc-c836-859623e7f6e9"
      },
      "source": [
        "VAL_RATIO = 0.2\r\n",
        "\r\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\r\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\r\n",
        "print('Size of training set: {}'.format(train_x.shape))\r\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: (983945, 429)\n",
            "Size of validation set: (245987, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1fNcNamqKa"
      },
      "source": [
        "BATCH_SIZE = 512\r\n",
        "\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "train_set = TIMITDataset(train_x, train_y)\r\n",
        "val_set = TIMITDataset(val_x, val_y)\r\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\r\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kqycVinmj11"
      },
      "source": [
        "#### **notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7dGNUuGmtDD",
        "outputId": "224c26f0-ac3b-45dc-8505-4c5ab1aa4b4d"
      },
      "source": [
        "import gc\r\n",
        "\r\n",
        "del train, train_label, train_x, train_y, val_x, val_y\r\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFtiY94m6X-"
      },
      "source": [
        "# **Create Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfFT6XJ8nBCS"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "class Classifier(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Classifier, self).__init__()\r\n",
        "        self.layer1 = nn.Linear(429, 2048)\r\n",
        "        self.layer2 = nn.Linear(2048, 2048)\r\n",
        "        self.layer3 = nn.Linear(2048, 1024)\r\n",
        "        self.layer4 = nn.Linear(1024, 512)\r\n",
        "        self.layer5 = nn.Linear(512, 128)\r\n",
        "        self.out = nn.Linear(128, 39) \r\n",
        "        self.dp = nn.Dropout(0.5)\r\n",
        "        self.bn1 = nn.BatchNorm1d(2048)\r\n",
        "        self.bn2 = nn.BatchNorm1d(2048)\r\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\r\n",
        "        self.bn4 = nn.BatchNorm1d(512)\r\n",
        "        self.bn5 = nn.BatchNorm1d(128)\r\n",
        "\r\n",
        "        self.act_fn = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.layer1(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.dp(x)\r\n",
        "\r\n",
        "        x = self.layer2(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "        x = self.bn2(x)\r\n",
        "        x = self.dp(x)\r\n",
        "\r\n",
        "        x = self.layer3(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "        x = self.bn3(x)\r\n",
        "        x = self.dp(x)\r\n",
        "\r\n",
        "        x = self.layer4(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "        x = self.bn4(x)\r\n",
        "        x = self.dp(x)\r\n",
        "\r\n",
        "        x = self.layer5(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "        x = self.bn5(x)\r\n",
        "        x = self.dp(x)\r\n",
        "\r\n",
        "        x = self.out(x)\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLq4t_knLUL"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBsChEconOOV",
        "outputId": "a62bd3ad-0448-4f1c-bfb0-4d8dd485f599"
      },
      "source": [
        "#check device\r\n",
        "def get_device():\r\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "\r\n",
        "# fix random seed\r\n",
        "def same_seeds(seed):\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        torch.cuda.manual_seed(seed)\r\n",
        "        torch.cuda.manual_seed_all(seed)  \r\n",
        "    np.random.seed(seed)  \r\n",
        "    torch.backends.cudnn.benchmark = False\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "# fix random seed for reproducibility\r\n",
        "same_seeds(0)\r\n",
        "\r\n",
        "# get device \r\n",
        "device = get_device()\r\n",
        "print(f'DEVICE: {device}')\r\n",
        "\r\n",
        "# training parameters\r\n",
        "num_epoch = 200               # number of training epoch\r\n",
        "learning_rate = 1e-4         # learning rate\r\n",
        "l2 = 1e-3                    # L2 regularization\r\n",
        "\r\n",
        "# the path where checkpoint saved\r\n",
        "model_path = './model.ckpt'\r\n",
        "\r\n",
        "# create model, define a loss function, and optimizer\r\n",
        "model = Classifier().to(device)\r\n",
        "criterion = nn.CrossEntropyLoss() \r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyjl3itJnWwL",
        "outputId": "d1055166-e977-4576-ac44-4de6d35d3472"
      },
      "source": [
        "# start training\r\n",
        "\r\n",
        "best_acc = 0.0\r\n",
        "for epoch in range(num_epoch):\r\n",
        "    train_acc = 0.0\r\n",
        "    train_loss = 0.0\r\n",
        "    val_acc = 0.0\r\n",
        "    val_loss = 0.0\r\n",
        "\r\n",
        "    # training\r\n",
        "    model.train() # set the model to training mode\r\n",
        "    for i, data in enumerate(train_loader):\r\n",
        "        inputs, labels = data\r\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "        optimizer.zero_grad() \r\n",
        "        outputs = model(inputs) \r\n",
        "        batch_loss = criterion(outputs, labels)\r\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\r\n",
        "        batch_loss.backward() \r\n",
        "        optimizer.step() \r\n",
        "\r\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\r\n",
        "        train_loss += batch_loss.item()\r\n",
        "\r\n",
        "    # validation\r\n",
        "    if len(val_set) > 0:\r\n",
        "        model.eval() # set the model to evaluation mode\r\n",
        "        with torch.no_grad():\r\n",
        "            for i, data in enumerate(val_loader):\r\n",
        "                inputs, labels = data\r\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "                outputs = model(inputs)\r\n",
        "                batch_loss = criterion(outputs, labels) \r\n",
        "                _, val_pred = torch.max(outputs, 1) \r\n",
        "            \r\n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\r\n",
        "                val_loss += batch_loss.item()\r\n",
        "\r\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\r\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\r\n",
        "            ))\r\n",
        "\r\n",
        "            # if the model improves, save a checkpoint at this epoch\r\n",
        "            if val_acc > best_acc:\r\n",
        "                best_acc = val_acc\r\n",
        "                torch.save(model.state_dict(), model_path)\r\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\r\n",
        "    else:\r\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\r\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\r\n",
        "        ))\r\n",
        "\r\n",
        "# if not validating, save the last epoch\r\n",
        "if len(val_set) == 0:\r\n",
        "    torch.save(model.state_dict(), model_path)\r\n",
        "    print('saving model at last epoch')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/200] Train Acc: 0.467192 Loss: 1.913715 | Val Acc: 0.622980 loss: 1.248807\n",
            "saving model with acc 0.623\n",
            "[002/200] Train Acc: 0.581271 Loss: 1.426617 | Val Acc: 0.666491 loss: 1.092193\n",
            "saving model with acc 0.666\n",
            "[003/200] Train Acc: 0.614193 Loss: 1.300954 | Val Acc: 0.685256 loss: 1.019454\n",
            "saving model with acc 0.685\n",
            "[004/200] Train Acc: 0.633182 Loss: 1.229937 | Val Acc: 0.696293 loss: 0.978966\n",
            "saving model with acc 0.696\n",
            "[005/200] Train Acc: 0.646716 Loss: 1.181010 | Val Acc: 0.706070 loss: 0.943704\n",
            "saving model with acc 0.706\n",
            "[006/200] Train Acc: 0.656996 Loss: 1.142254 | Val Acc: 0.712782 loss: 0.918096\n",
            "saving model with acc 0.713\n",
            "[007/200] Train Acc: 0.665316 Loss: 1.111972 | Val Acc: 0.718717 loss: 0.899170\n",
            "saving model with acc 0.719\n",
            "[008/200] Train Acc: 0.672791 Loss: 1.085998 | Val Acc: 0.721668 loss: 0.884975\n",
            "saving model with acc 0.722\n",
            "[009/200] Train Acc: 0.678668 Loss: 1.065886 | Val Acc: 0.724762 loss: 0.871427\n",
            "saving model with acc 0.725\n",
            "[010/200] Train Acc: 0.682172 Loss: 1.049484 | Val Acc: 0.728522 loss: 0.860561\n",
            "saving model with acc 0.729\n",
            "[011/200] Train Acc: 0.686333 Loss: 1.034388 | Val Acc: 0.730518 loss: 0.851650\n",
            "saving model with acc 0.731\n",
            "[012/200] Train Acc: 0.689189 Loss: 1.023128 | Val Acc: 0.732746 loss: 0.845247\n",
            "saving model with acc 0.733\n",
            "[013/200] Train Acc: 0.692267 Loss: 1.012077 | Val Acc: 0.734693 loss: 0.835425\n",
            "saving model with acc 0.735\n",
            "[014/200] Train Acc: 0.694210 Loss: 1.002433 | Val Acc: 0.736348 loss: 0.827954\n",
            "saving model with acc 0.736\n",
            "[015/200] Train Acc: 0.696903 Loss: 0.994517 | Val Acc: 0.736771 loss: 0.826180\n",
            "saving model with acc 0.737\n",
            "[016/200] Train Acc: 0.698082 Loss: 0.989208 | Val Acc: 0.738641 loss: 0.823423\n",
            "saving model with acc 0.739\n",
            "[017/200] Train Acc: 0.700199 Loss: 0.982249 | Val Acc: 0.740173 loss: 0.814753\n",
            "saving model with acc 0.740\n",
            "[018/200] Train Acc: 0.701671 Loss: 0.975737 | Val Acc: 0.740100 loss: 0.813832\n",
            "[019/200] Train Acc: 0.702475 Loss: 0.972677 | Val Acc: 0.742234 loss: 0.808938\n",
            "saving model with acc 0.742\n",
            "[020/200] Train Acc: 0.704077 Loss: 0.967114 | Val Acc: 0.741616 loss: 0.807792\n",
            "[021/200] Train Acc: 0.705301 Loss: 0.962743 | Val Acc: 0.742458 loss: 0.805989\n",
            "saving model with acc 0.742\n",
            "[022/200] Train Acc: 0.706177 Loss: 0.959356 | Val Acc: 0.743336 loss: 0.803168\n",
            "saving model with acc 0.743\n",
            "[023/200] Train Acc: 0.706214 Loss: 0.957020 | Val Acc: 0.743726 loss: 0.802789\n",
            "saving model with acc 0.744\n",
            "[024/200] Train Acc: 0.708404 Loss: 0.952415 | Val Acc: 0.743730 loss: 0.801001\n",
            "saving model with acc 0.744\n",
            "[025/200] Train Acc: 0.708362 Loss: 0.949801 | Val Acc: 0.743751 loss: 0.797517\n",
            "saving model with acc 0.744\n",
            "[026/200] Train Acc: 0.709055 Loss: 0.947013 | Val Acc: 0.744869 loss: 0.795324\n",
            "saving model with acc 0.745\n",
            "[027/200] Train Acc: 0.709910 Loss: 0.943757 | Val Acc: 0.747064 loss: 0.790887\n",
            "saving model with acc 0.747\n",
            "[028/200] Train Acc: 0.710400 Loss: 0.940950 | Val Acc: 0.747340 loss: 0.789910\n",
            "saving model with acc 0.747\n",
            "[029/200] Train Acc: 0.711730 Loss: 0.937900 | Val Acc: 0.746942 loss: 0.788524\n",
            "[030/200] Train Acc: 0.711762 Loss: 0.936317 | Val Acc: 0.746206 loss: 0.790496\n",
            "[031/200] Train Acc: 0.712677 Loss: 0.934021 | Val Acc: 0.747564 loss: 0.787418\n",
            "saving model with acc 0.748\n",
            "[032/200] Train Acc: 0.712919 Loss: 0.932327 | Val Acc: 0.746873 loss: 0.789162\n",
            "[033/200] Train Acc: 0.713527 Loss: 0.930257 | Val Acc: 0.747011 loss: 0.784207\n",
            "[034/200] Train Acc: 0.713993 Loss: 0.928790 | Val Acc: 0.747848 loss: 0.784915\n",
            "saving model with acc 0.748\n",
            "[035/200] Train Acc: 0.714630 Loss: 0.925427 | Val Acc: 0.747222 loss: 0.786365\n",
            "[036/200] Train Acc: 0.714795 Loss: 0.924037 | Val Acc: 0.748271 loss: 0.784309\n",
            "saving model with acc 0.748\n",
            "[037/200] Train Acc: 0.715408 Loss: 0.923191 | Val Acc: 0.747881 loss: 0.782609\n",
            "[038/200] Train Acc: 0.716083 Loss: 0.921647 | Val Acc: 0.748613 loss: 0.781735\n",
            "saving model with acc 0.749\n",
            "[039/200] Train Acc: 0.716254 Loss: 0.921059 | Val Acc: 0.749320 loss: 0.779745\n",
            "saving model with acc 0.749\n",
            "[040/200] Train Acc: 0.716005 Loss: 0.919642 | Val Acc: 0.749105 loss: 0.781424\n",
            "[041/200] Train Acc: 0.716948 Loss: 0.918246 | Val Acc: 0.748653 loss: 0.781886\n",
            "[042/200] Train Acc: 0.717286 Loss: 0.916329 | Val Acc: 0.749544 loss: 0.775989\n",
            "saving model with acc 0.750\n",
            "[043/200] Train Acc: 0.717503 Loss: 0.914462 | Val Acc: 0.749804 loss: 0.777354\n",
            "saving model with acc 0.750\n",
            "[044/200] Train Acc: 0.718219 Loss: 0.913989 | Val Acc: 0.749592 loss: 0.779017\n",
            "[045/200] Train Acc: 0.718647 Loss: 0.912761 | Val Acc: 0.750060 loss: 0.774604\n",
            "saving model with acc 0.750\n",
            "[046/200] Train Acc: 0.718440 Loss: 0.912137 | Val Acc: 0.749556 loss: 0.774699\n",
            "[047/200] Train Acc: 0.719281 Loss: 0.911029 | Val Acc: 0.751979 loss: 0.772079\n",
            "saving model with acc 0.752\n",
            "[048/200] Train Acc: 0.719885 Loss: 0.908858 | Val Acc: 0.749666 loss: 0.775771\n",
            "[049/200] Train Acc: 0.719271 Loss: 0.908808 | Val Acc: 0.748718 loss: 0.780526\n",
            "[050/200] Train Acc: 0.720059 Loss: 0.906624 | Val Acc: 0.750808 loss: 0.775866\n",
            "[051/200] Train Acc: 0.720070 Loss: 0.906916 | Val Acc: 0.750836 loss: 0.774488\n",
            "[052/200] Train Acc: 0.720153 Loss: 0.906334 | Val Acc: 0.751938 loss: 0.772744\n",
            "[053/200] Train Acc: 0.719995 Loss: 0.905717 | Val Acc: 0.750369 loss: 0.776226\n",
            "[054/200] Train Acc: 0.720725 Loss: 0.904923 | Val Acc: 0.752117 loss: 0.773101\n",
            "saving model with acc 0.752\n",
            "[055/200] Train Acc: 0.720550 Loss: 0.902717 | Val Acc: 0.752353 loss: 0.771172\n",
            "saving model with acc 0.752\n",
            "[056/200] Train Acc: 0.720575 Loss: 0.902891 | Val Acc: 0.749519 loss: 0.776528\n",
            "[057/200] Train Acc: 0.721096 Loss: 0.901929 | Val Acc: 0.749971 loss: 0.778409\n",
            "[058/200] Train Acc: 0.721363 Loss: 0.902241 | Val Acc: 0.752670 loss: 0.769519\n",
            "saving model with acc 0.753\n",
            "[059/200] Train Acc: 0.720704 Loss: 0.902098 | Val Acc: 0.751214 loss: 0.773269\n",
            "[060/200] Train Acc: 0.721779 Loss: 0.900740 | Val Acc: 0.751422 loss: 0.770216\n",
            "[061/200] Train Acc: 0.721593 Loss: 0.901138 | Val Acc: 0.752019 loss: 0.771857\n",
            "[062/200] Train Acc: 0.721615 Loss: 0.901292 | Val Acc: 0.752015 loss: 0.771254\n",
            "[063/200] Train Acc: 0.722060 Loss: 0.899118 | Val Acc: 0.752304 loss: 0.770159\n",
            "[064/200] Train Acc: 0.722000 Loss: 0.898878 | Val Acc: 0.752873 loss: 0.767398\n",
            "saving model with acc 0.753\n",
            "[065/200] Train Acc: 0.723090 Loss: 0.897839 | Val Acc: 0.752580 loss: 0.768842\n",
            "[066/200] Train Acc: 0.721949 Loss: 0.899018 | Val Acc: 0.750397 loss: 0.773158\n",
            "[067/200] Train Acc: 0.723217 Loss: 0.896111 | Val Acc: 0.752210 loss: 0.768871\n",
            "[068/200] Train Acc: 0.722673 Loss: 0.897580 | Val Acc: 0.751093 loss: 0.771340\n",
            "[069/200] Train Acc: 0.722976 Loss: 0.895466 | Val Acc: 0.752824 loss: 0.768482\n",
            "[070/200] Train Acc: 0.722881 Loss: 0.895507 | Val Acc: 0.751828 loss: 0.768571\n",
            "[071/200] Train Acc: 0.723178 Loss: 0.894718 | Val Acc: 0.752296 loss: 0.766985\n",
            "[072/200] Train Acc: 0.723588 Loss: 0.894159 | Val Acc: 0.752284 loss: 0.767765\n",
            "[073/200] Train Acc: 0.722630 Loss: 0.895968 | Val Acc: 0.752202 loss: 0.766873\n",
            "[074/200] Train Acc: 0.723451 Loss: 0.894589 | Val Acc: 0.752658 loss: 0.769431\n",
            "[075/200] Train Acc: 0.723577 Loss: 0.893295 | Val Acc: 0.752776 loss: 0.766694\n",
            "[076/200] Train Acc: 0.723605 Loss: 0.893474 | Val Acc: 0.751625 loss: 0.768861\n",
            "[077/200] Train Acc: 0.723736 Loss: 0.893828 | Val Acc: 0.752259 loss: 0.768610\n",
            "[078/200] Train Acc: 0.723777 Loss: 0.892206 | Val Acc: 0.753389 loss: 0.765651\n",
            "saving model with acc 0.753\n",
            "[079/200] Train Acc: 0.723923 Loss: 0.893534 | Val Acc: 0.752938 loss: 0.767752\n",
            "[080/200] Train Acc: 0.724024 Loss: 0.892298 | Val Acc: 0.754186 loss: 0.764111\n",
            "saving model with acc 0.754\n",
            "[081/200] Train Acc: 0.723969 Loss: 0.890565 | Val Acc: 0.752914 loss: 0.765713\n",
            "[082/200] Train Acc: 0.723990 Loss: 0.892002 | Val Acc: 0.753580 loss: 0.767176\n",
            "[083/200] Train Acc: 0.724227 Loss: 0.890722 | Val Acc: 0.753658 loss: 0.768662\n",
            "[084/200] Train Acc: 0.724862 Loss: 0.890566 | Val Acc: 0.754113 loss: 0.765696\n",
            "[085/200] Train Acc: 0.725163 Loss: 0.889838 | Val Acc: 0.753548 loss: 0.766510\n",
            "[086/200] Train Acc: 0.724945 Loss: 0.889467 | Val Acc: 0.752755 loss: 0.767479\n",
            "[087/200] Train Acc: 0.724410 Loss: 0.890067 | Val Acc: 0.752337 loss: 0.770242\n",
            "[088/200] Train Acc: 0.725021 Loss: 0.889610 | Val Acc: 0.753503 loss: 0.765507\n",
            "[089/200] Train Acc: 0.724799 Loss: 0.890187 | Val Acc: 0.753170 loss: 0.766126\n",
            "[090/200] Train Acc: 0.724667 Loss: 0.889171 | Val Acc: 0.753381 loss: 0.766266\n",
            "[091/200] Train Acc: 0.725361 Loss: 0.888300 | Val Acc: 0.753276 loss: 0.767014\n",
            "[092/200] Train Acc: 0.724385 Loss: 0.889673 | Val Acc: 0.752426 loss: 0.767068\n",
            "[093/200] Train Acc: 0.725749 Loss: 0.888423 | Val Acc: 0.753162 loss: 0.764950\n",
            "[094/200] Train Acc: 0.725393 Loss: 0.888728 | Val Acc: 0.754154 loss: 0.763177\n",
            "[095/200] Train Acc: 0.724869 Loss: 0.887538 | Val Acc: 0.753564 loss: 0.766403\n",
            "[096/200] Train Acc: 0.724992 Loss: 0.888319 | Val Acc: 0.754113 loss: 0.763164\n",
            "[097/200] Train Acc: 0.724870 Loss: 0.887882 | Val Acc: 0.754072 loss: 0.765360\n",
            "[098/200] Train Acc: 0.725249 Loss: 0.887043 | Val Acc: 0.752792 loss: 0.766626\n",
            "[099/200] Train Acc: 0.725190 Loss: 0.887570 | Val Acc: 0.752243 loss: 0.765357\n",
            "[100/200] Train Acc: 0.725651 Loss: 0.886781 | Val Acc: 0.752170 loss: 0.768011\n",
            "[101/200] Train Acc: 0.725593 Loss: 0.887318 | Val Acc: 0.753316 loss: 0.765892\n",
            "[102/200] Train Acc: 0.724969 Loss: 0.887626 | Val Acc: 0.753263 loss: 0.765802\n",
            "[103/200] Train Acc: 0.725470 Loss: 0.886552 | Val Acc: 0.753869 loss: 0.764588\n",
            "[104/200] Train Acc: 0.725494 Loss: 0.887812 | Val Acc: 0.754308 loss: 0.763964\n",
            "saving model with acc 0.754\n",
            "[105/200] Train Acc: 0.725569 Loss: 0.886556 | Val Acc: 0.754194 loss: 0.762461\n",
            "[106/200] Train Acc: 0.725425 Loss: 0.886030 | Val Acc: 0.752776 loss: 0.766229\n",
            "[107/200] Train Acc: 0.725625 Loss: 0.885179 | Val Acc: 0.753637 loss: 0.764264\n",
            "[108/200] Train Acc: 0.725260 Loss: 0.885709 | Val Acc: 0.754154 loss: 0.764156\n",
            "[109/200] Train Acc: 0.725388 Loss: 0.885532 | Val Acc: 0.754337 loss: 0.766314\n",
            "saving model with acc 0.754\n",
            "[110/200] Train Acc: 0.726352 Loss: 0.884215 | Val Acc: 0.753979 loss: 0.763746\n",
            "[111/200] Train Acc: 0.725776 Loss: 0.884619 | Val Acc: 0.753711 loss: 0.765300\n",
            "[112/200] Train Acc: 0.725895 Loss: 0.884412 | Val Acc: 0.753227 loss: 0.763091\n",
            "[113/200] Train Acc: 0.725539 Loss: 0.884240 | Val Acc: 0.754414 loss: 0.762285\n",
            "saving model with acc 0.754\n",
            "[114/200] Train Acc: 0.726228 Loss: 0.883658 | Val Acc: 0.753194 loss: 0.765313\n",
            "[115/200] Train Acc: 0.725679 Loss: 0.883536 | Val Acc: 0.753755 loss: 0.761956\n",
            "[116/200] Train Acc: 0.726076 Loss: 0.883763 | Val Acc: 0.754621 loss: 0.765628\n",
            "saving model with acc 0.755\n",
            "[117/200] Train Acc: 0.726122 Loss: 0.884049 | Val Acc: 0.754280 loss: 0.763462\n",
            "[118/200] Train Acc: 0.726346 Loss: 0.883278 | Val Acc: 0.752666 loss: 0.766658\n",
            "[119/200] Train Acc: 0.726401 Loss: 0.883051 | Val Acc: 0.754552 loss: 0.761458\n",
            "[120/200] Train Acc: 0.726335 Loss: 0.883361 | Val Acc: 0.752499 loss: 0.768652\n",
            "[121/200] Train Acc: 0.726043 Loss: 0.883595 | Val Acc: 0.753865 loss: 0.762092\n",
            "[122/200] Train Acc: 0.726096 Loss: 0.883633 | Val Acc: 0.753914 loss: 0.766938\n",
            "[123/200] Train Acc: 0.725722 Loss: 0.883523 | Val Acc: 0.754060 loss: 0.764015\n",
            "[124/200] Train Acc: 0.725721 Loss: 0.884315 | Val Acc: 0.753580 loss: 0.764223\n",
            "[125/200] Train Acc: 0.726052 Loss: 0.882296 | Val Acc: 0.754410 loss: 0.763565\n",
            "[126/200] Train Acc: 0.726525 Loss: 0.882857 | Val Acc: 0.752930 loss: 0.765449\n",
            "[127/200] Train Acc: 0.725900 Loss: 0.882878 | Val Acc: 0.752406 loss: 0.769023\n",
            "[128/200] Train Acc: 0.726683 Loss: 0.882719 | Val Acc: 0.753263 loss: 0.768061\n",
            "[129/200] Train Acc: 0.726925 Loss: 0.882032 | Val Acc: 0.753735 loss: 0.764992\n",
            "[130/200] Train Acc: 0.726656 Loss: 0.883021 | Val Acc: 0.753202 loss: 0.765121\n",
            "[131/200] Train Acc: 0.726135 Loss: 0.882217 | Val Acc: 0.753467 loss: 0.765993\n",
            "[132/200] Train Acc: 0.727069 Loss: 0.881734 | Val Acc: 0.754198 loss: 0.764487\n",
            "[133/200] Train Acc: 0.726419 Loss: 0.882571 | Val Acc: 0.753963 loss: 0.763494\n",
            "[134/200] Train Acc: 0.726398 Loss: 0.881401 | Val Acc: 0.753121 loss: 0.766158\n",
            "[135/200] Train Acc: 0.726840 Loss: 0.880794 | Val Acc: 0.752735 loss: 0.766457\n",
            "[136/200] Train Acc: 0.726686 Loss: 0.882699 | Val Acc: 0.753857 loss: 0.761433\n",
            "[137/200] Train Acc: 0.727022 Loss: 0.880901 | Val Acc: 0.752938 loss: 0.767638\n",
            "[138/200] Train Acc: 0.726588 Loss: 0.882319 | Val Acc: 0.753381 loss: 0.762048\n",
            "[139/200] Train Acc: 0.727074 Loss: 0.881612 | Val Acc: 0.754288 loss: 0.763720\n",
            "[140/200] Train Acc: 0.727197 Loss: 0.880969 | Val Acc: 0.755609 loss: 0.759766\n",
            "saving model with acc 0.756\n",
            "[141/200] Train Acc: 0.727440 Loss: 0.880353 | Val Acc: 0.754544 loss: 0.759086\n",
            "[142/200] Train Acc: 0.726766 Loss: 0.881016 | Val Acc: 0.753011 loss: 0.766594\n",
            "[143/200] Train Acc: 0.726984 Loss: 0.882277 | Val Acc: 0.754755 loss: 0.761275\n",
            "[144/200] Train Acc: 0.726805 Loss: 0.880036 | Val Acc: 0.754670 loss: 0.763120\n",
            "[145/200] Train Acc: 0.726747 Loss: 0.881029 | Val Acc: 0.754674 loss: 0.765684\n",
            "[146/200] Train Acc: 0.726691 Loss: 0.880535 | Val Acc: 0.754093 loss: 0.765212\n",
            "[147/200] Train Acc: 0.726917 Loss: 0.879999 | Val Acc: 0.754471 loss: 0.762593\n",
            "[148/200] Train Acc: 0.726808 Loss: 0.880788 | Val Acc: 0.754056 loss: 0.765939\n",
            "[149/200] Train Acc: 0.726831 Loss: 0.881779 | Val Acc: 0.753235 loss: 0.766742\n",
            "[150/200] Train Acc: 0.726879 Loss: 0.880027 | Val Acc: 0.755259 loss: 0.763107\n",
            "[151/200] Train Acc: 0.726935 Loss: 0.879870 | Val Acc: 0.753784 loss: 0.763364\n",
            "[152/200] Train Acc: 0.726967 Loss: 0.879442 | Val Acc: 0.753479 loss: 0.762598\n",
            "[153/200] Train Acc: 0.727596 Loss: 0.878802 | Val Acc: 0.753560 loss: 0.764123\n",
            "[154/200] Train Acc: 0.727807 Loss: 0.879890 | Val Acc: 0.754540 loss: 0.762338\n",
            "[155/200] Train Acc: 0.726803 Loss: 0.880671 | Val Acc: 0.753552 loss: 0.764774\n",
            "[156/200] Train Acc: 0.727442 Loss: 0.879032 | Val Acc: 0.754873 loss: 0.761900\n",
            "[157/200] Train Acc: 0.727402 Loss: 0.879009 | Val Acc: 0.752422 loss: 0.764498\n",
            "[158/200] Train Acc: 0.727389 Loss: 0.879149 | Val Acc: 0.753893 loss: 0.762996\n",
            "[159/200] Train Acc: 0.726815 Loss: 0.880127 | Val Acc: 0.754979 loss: 0.762826\n",
            "[160/200] Train Acc: 0.727258 Loss: 0.879071 | Val Acc: 0.755365 loss: 0.760893\n",
            "[161/200] Train Acc: 0.727346 Loss: 0.880155 | Val Acc: 0.754394 loss: 0.760631\n",
            "[162/200] Train Acc: 0.727353 Loss: 0.879292 | Val Acc: 0.755674 loss: 0.758947\n",
            "saving model with acc 0.756\n",
            "[163/200] Train Acc: 0.727551 Loss: 0.878999 | Val Acc: 0.752820 loss: 0.765991\n",
            "[164/200] Train Acc: 0.728020 Loss: 0.877831 | Val Acc: 0.754747 loss: 0.761845\n",
            "[165/200] Train Acc: 0.727250 Loss: 0.878883 | Val Acc: 0.754670 loss: 0.757740\n",
            "[166/200] Train Acc: 0.727829 Loss: 0.879396 | Val Acc: 0.754975 loss: 0.760198\n",
            "[167/200] Train Acc: 0.727307 Loss: 0.878273 | Val Acc: 0.754898 loss: 0.760961\n",
            "[168/200] Train Acc: 0.727658 Loss: 0.878845 | Val Acc: 0.755479 loss: 0.758432\n",
            "[169/200] Train Acc: 0.727732 Loss: 0.877850 | Val Acc: 0.753072 loss: 0.765742\n",
            "[170/200] Train Acc: 0.727662 Loss: 0.877370 | Val Acc: 0.754381 loss: 0.762229\n",
            "[171/200] Train Acc: 0.728344 Loss: 0.877522 | Val Acc: 0.754475 loss: 0.761366\n",
            "[172/200] Train Acc: 0.727672 Loss: 0.878313 | Val Acc: 0.754304 loss: 0.762349\n",
            "[173/200] Train Acc: 0.728165 Loss: 0.877240 | Val Acc: 0.754650 loss: 0.762254\n",
            "[174/200] Train Acc: 0.727886 Loss: 0.878206 | Val Acc: 0.754715 loss: 0.760463\n",
            "[175/200] Train Acc: 0.727211 Loss: 0.878146 | Val Acc: 0.754288 loss: 0.762415\n",
            "[176/200] Train Acc: 0.727378 Loss: 0.878299 | Val Acc: 0.754983 loss: 0.762472\n",
            "[177/200] Train Acc: 0.727690 Loss: 0.877530 | Val Acc: 0.755007 loss: 0.761815\n",
            "[178/200] Train Acc: 0.728314 Loss: 0.876410 | Val Acc: 0.755341 loss: 0.758790\n",
            "[179/200] Train Acc: 0.727505 Loss: 0.877418 | Val Acc: 0.755426 loss: 0.760001\n",
            "[180/200] Train Acc: 0.727657 Loss: 0.878148 | Val Acc: 0.754959 loss: 0.760836\n",
            "[181/200] Train Acc: 0.727851 Loss: 0.877329 | Val Acc: 0.754133 loss: 0.762978\n",
            "[182/200] Train Acc: 0.728105 Loss: 0.877525 | Val Acc: 0.754755 loss: 0.761767\n",
            "[183/200] Train Acc: 0.728275 Loss: 0.876505 | Val Acc: 0.754231 loss: 0.761212\n",
            "[184/200] Train Acc: 0.728215 Loss: 0.877885 | Val Acc: 0.753459 loss: 0.763025\n",
            "[185/200] Train Acc: 0.728363 Loss: 0.876591 | Val Acc: 0.753332 loss: 0.763564\n",
            "[186/200] Train Acc: 0.728005 Loss: 0.876437 | Val Acc: 0.755020 loss: 0.760145\n",
            "[187/200] Train Acc: 0.727585 Loss: 0.877442 | Val Acc: 0.754052 loss: 0.761092\n",
            "[188/200] Train Acc: 0.728152 Loss: 0.876670 | Val Acc: 0.754621 loss: 0.760001\n",
            "[189/200] Train Acc: 0.727929 Loss: 0.877436 | Val Acc: 0.754682 loss: 0.759960\n",
            "[190/200] Train Acc: 0.727834 Loss: 0.877082 | Val Acc: 0.754223 loss: 0.763547\n",
            "[191/200] Train Acc: 0.728146 Loss: 0.878372 | Val Acc: 0.755158 loss: 0.758498\n",
            "[192/200] Train Acc: 0.728755 Loss: 0.876820 | Val Acc: 0.754247 loss: 0.760239\n",
            "[193/200] Train Acc: 0.727648 Loss: 0.876694 | Val Acc: 0.756337 loss: 0.756354\n",
            "saving model with acc 0.756\n",
            "[194/200] Train Acc: 0.727912 Loss: 0.877250 | Val Acc: 0.754556 loss: 0.761612\n",
            "[195/200] Train Acc: 0.728202 Loss: 0.877322 | Val Acc: 0.753820 loss: 0.766376\n",
            "[196/200] Train Acc: 0.728406 Loss: 0.877295 | Val Acc: 0.754735 loss: 0.761276\n",
            "[197/200] Train Acc: 0.728273 Loss: 0.876946 | Val Acc: 0.754747 loss: 0.758434\n",
            "[198/200] Train Acc: 0.728160 Loss: 0.877274 | Val Acc: 0.753629 loss: 0.764909\n",
            "[199/200] Train Acc: 0.727851 Loss: 0.876821 | Val Acc: 0.755032 loss: 0.761814\n",
            "[200/200] Train Acc: 0.727173 Loss: 0.879088 | Val Acc: 0.755373 loss: 0.760540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTr80Panaq8"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiBILuUEncLl",
        "outputId": "b8dad0fa-3b4a-4569-d216-dc361446434e"
      },
      "source": [
        "# create testing dataset\r\n",
        "test_set = TIMITDataset(test, None)\r\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\r\n",
        "\r\n",
        "# create model and load weights from checkpoint\r\n",
        "model = Classifier().to(device)\r\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP_tUJeLnd94"
      },
      "source": [
        "predict = []\r\n",
        "model.eval() # set the model to evaluation mode\r\n",
        "with torch.no_grad():\r\n",
        "    for i, data in enumerate(test_loader):\r\n",
        "        inputs = data\r\n",
        "        inputs = inputs.to(device)\r\n",
        "        outputs = model(inputs)\r\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\r\n",
        "\r\n",
        "        for y in test_pred.cpu().numpy():\r\n",
        "            predict.append(y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIjAsNTnhgs"
      },
      "source": [
        "# **Write prediction to a CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "athJ5R7BnlN5",
        "outputId": "64aefa60-2fdd-4fd3-bc96-d82dd8b76f57"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\r\n",
        "    f.write('Id,Class\\n')\r\n",
        "    for i, y in enumerate(predict):\r\n",
        "        f.write('{},{}\\n'.format(i, y))\r\n",
        "\r\n",
        "print('Saving results to prediction.csv')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to prediction.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV5cHjHLos62"
      },
      "source": [
        "# **Reference**\r\n",
        "\r\n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb)"
      ]
    }
  ]
}