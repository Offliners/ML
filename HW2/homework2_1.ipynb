{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOEXIaEw/EBzMl5njSlSnHe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Offliners/ML/blob/main/HW2/homework2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghYxMSxsmAYm"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**\r\n",
        "\r\n",
        "The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\r\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\r\n",
        "\r\n",
        "This homework is a multiclass classification task, we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\r\n",
        "\r\n",
        "## **Download Data**\r\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3\r\n",
        "\r\n",
        "timit_11/\r\n",
        "\r\n",
        "* train_11.npy: training data\r\n",
        "* train_label_11.npy: training label\r\n",
        "* test_11.npy: testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N98vZGvSl-Fq",
        "outputId": "fcf31a7c-074a-4183-a45c-d10b50ac2c4b"
      },
      "source": [
        "!gdown --id '1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG' --output data.zip\r\n",
        "!unzip data.zip\r\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG\n",
            "To: /content/data.zip\n",
            "376MB [00:01, 246MB/s]\n",
            "Archive:  data.zip\n",
            "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "data.zip  sample_data  sampleSubmission.csv  timit_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffxBMT3mU5g"
      },
      "source": [
        "# **Preparing Data**\r\n",
        "\r\n",
        "Load the training and testing data from the .npy file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRTYp6bemeRk",
        "outputId": "151eba25-b3a5-4ef1-e6c0-9bc1b6e878a0"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "print('Loading data ...')\r\n",
        "\r\n",
        "data_root='./timit_11/timit_11/'\r\n",
        "train = np.load(data_root + 'train_11.npy')\r\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\r\n",
        "test = np.load(data_root + 'test_11.npy')\r\n",
        "\r\n",
        "print('Size of training data: {}'.format(train.shape))\r\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpQ84yh8me6A"
      },
      "source": [
        "# **Create Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPvM4GoFmhzo"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import Dataset\r\n",
        "\r\n",
        "class TIMITDataset(Dataset):\r\n",
        "    def __init__(self, X, y=None):\r\n",
        "        self.data = torch.from_numpy(X).float()\r\n",
        "        if y is not None:\r\n",
        "            y = y.astype(np.int)\r\n",
        "            self.label = torch.LongTensor(y)\r\n",
        "        else:\r\n",
        "            self.label = None\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if self.label is not None:\r\n",
        "            return self.data[idx], self.label[idx]\r\n",
        "        else:\r\n",
        "            return self.data[idx]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmKia2rfmmk9",
        "outputId": "fffcd0a6-23d0-422e-c145-631f37f93668"
      },
      "source": [
        "VAL_RATIO = 0.2\r\n",
        "\r\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\r\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\r\n",
        "print('Size of training set: {}'.format(train_x.shape))\r\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: (983945, 429)\n",
            "Size of validation set: (245987, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1fNcNamqKa"
      },
      "source": [
        "BATCH_SIZE = 512\r\n",
        "\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "train_set = TIMITDataset(train_x, train_y)\r\n",
        "val_set = TIMITDataset(val_x, val_y)\r\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\r\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kqycVinmj11"
      },
      "source": [
        "#### **notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7dGNUuGmtDD",
        "outputId": "6bdd7220-89a4-4f18-a13a-709d40b6d099"
      },
      "source": [
        "import gc\r\n",
        "\r\n",
        "del train, train_label, train_x, train_y, val_x, val_y\r\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "170"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFtiY94m6X-"
      },
      "source": [
        "# **Create Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfFT6XJ8nBCS"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "class Classifier(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Classifier, self).__init__()\r\n",
        "        self.layer1 = nn.Linear(429, 1024)\r\n",
        "        self.layer2 = nn.Linear(1024, 1024)\r\n",
        "        self.layer3 = nn.Linear(1024, 512)\r\n",
        "        self.layer4 = nn.Linear(512, 128)\r\n",
        "        self.out = nn.Linear(128, 39) \r\n",
        "        self.dp = nn.Dropout(0.5)\r\n",
        "        self.bn1 = nn.BatchNorm1d(1024)\r\n",
        "        self.bn2 = nn.BatchNorm1d(1024)\r\n",
        "        self.bn3 = nn.BatchNorm1d(512)\r\n",
        "        self.bn4 = nn.BatchNorm1d(128)\r\n",
        "\r\n",
        "        self.act_fn = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.layer1(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        x = self.dp(x)\r\n",
        "\r\n",
        "        x = self.layer2(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "        x = self.bn2(x)\r\n",
        "        x = self.dp(x)\r\n",
        "\r\n",
        "        x = self.layer3(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "        x = self.bn3(x)\r\n",
        "        x = self.dp(x)\r\n",
        "\r\n",
        "        x = self.layer4(x)\r\n",
        "        x = self.act_fn(x)\r\n",
        "        x = self.bn4(x)\r\n",
        "        x = self.dp(x)\r\n",
        "\r\n",
        "        x = self.out(x)\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLq4t_knLUL"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBsChEconOOV",
        "outputId": "9cd1a594-77a8-4b54-b9f2-83cb1148453d"
      },
      "source": [
        "#check device\r\n",
        "def get_device():\r\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "\r\n",
        "# fix random seed\r\n",
        "def same_seeds(seed):\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        torch.cuda.manual_seed(seed)\r\n",
        "        torch.cuda.manual_seed_all(seed)  \r\n",
        "    np.random.seed(seed)  \r\n",
        "    torch.backends.cudnn.benchmark = False\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "# fix random seed for reproducibility\r\n",
        "same_seeds(0)\r\n",
        "\r\n",
        "# get device \r\n",
        "device = get_device()\r\n",
        "print(f'DEVICE: {device}')\r\n",
        "\r\n",
        "# training parameters\r\n",
        "num_epoch = 100               # number of training epoch\r\n",
        "learning_rate = 1e-4         # learning rate\r\n",
        "l2 = 1e-3                    # L2 regularization\r\n",
        "\r\n",
        "# the path where checkpoint saved\r\n",
        "model_path = './model.ckpt'\r\n",
        "\r\n",
        "# create model, define a loss function, and optimizer\r\n",
        "model = Classifier().to(device)\r\n",
        "criterion = nn.CrossEntropyLoss() \r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyjl3itJnWwL",
        "outputId": "8b1a9951-72e2-4f8c-cf40-af05536b703f"
      },
      "source": [
        "# start training\r\n",
        "\r\n",
        "best_acc = 0.0\r\n",
        "for epoch in range(num_epoch):\r\n",
        "    train_acc = 0.0\r\n",
        "    train_loss = 0.0\r\n",
        "    val_acc = 0.0\r\n",
        "    val_loss = 0.0\r\n",
        "\r\n",
        "    # training\r\n",
        "    model.train() # set the model to training mode\r\n",
        "    for i, data in enumerate(train_loader):\r\n",
        "        inputs, labels = data\r\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "        optimizer.zero_grad() \r\n",
        "        outputs = model(inputs) \r\n",
        "        batch_loss = criterion(outputs, labels)\r\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\r\n",
        "        batch_loss.backward() \r\n",
        "        optimizer.step() \r\n",
        "\r\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\r\n",
        "        train_loss += batch_loss.item()\r\n",
        "\r\n",
        "    # validation\r\n",
        "    if len(val_set) > 0:\r\n",
        "        model.eval() # set the model to evaluation mode\r\n",
        "        with torch.no_grad():\r\n",
        "            for i, data in enumerate(val_loader):\r\n",
        "                inputs, labels = data\r\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "                outputs = model(inputs)\r\n",
        "                batch_loss = criterion(outputs, labels) \r\n",
        "                _, val_pred = torch.max(outputs, 1) \r\n",
        "            \r\n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\r\n",
        "                val_loss += batch_loss.item()\r\n",
        "\r\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\r\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\r\n",
        "            ))\r\n",
        "\r\n",
        "            # if the model improves, save a checkpoint at this epoch\r\n",
        "            if val_acc > best_acc:\r\n",
        "                best_acc = val_acc\r\n",
        "                torch.save(model.state_dict(), model_path)\r\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\r\n",
        "    else:\r\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\r\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\r\n",
        "        ))\r\n",
        "\r\n",
        "# if not validating, save the last epoch\r\n",
        "if len(val_set) == 0:\r\n",
        "    torch.save(model.state_dict(), model_path)\r\n",
        "    print('saving model at last epoch')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/100] Train Acc: 0.447226 Loss: 1.978016 | Val Acc: 0.598593 loss: 1.332401\n",
            "saving model with acc 0.599\n",
            "[002/100] Train Acc: 0.553974 Loss: 1.512310 | Val Acc: 0.643790 loss: 1.164261\n",
            "saving model with acc 0.644\n",
            "[003/100] Train Acc: 0.585244 Loss: 1.392696 | Val Acc: 0.664958 loss: 1.086877\n",
            "saving model with acc 0.665\n",
            "[004/100] Train Acc: 0.604130 Loss: 1.323179 | Val Acc: 0.679564 loss: 1.034711\n",
            "saving model with acc 0.680\n",
            "[005/100] Train Acc: 0.617043 Loss: 1.274601 | Val Acc: 0.688248 loss: 1.000711\n",
            "saving model with acc 0.688\n",
            "[006/100] Train Acc: 0.627599 Loss: 1.236815 | Val Acc: 0.696850 loss: 0.974408\n",
            "saving model with acc 0.697\n",
            "[007/100] Train Acc: 0.636474 Loss: 1.206841 | Val Acc: 0.702045 loss: 0.952885\n",
            "saving model with acc 0.702\n",
            "[008/100] Train Acc: 0.643605 Loss: 1.180479 | Val Acc: 0.708102 loss: 0.929834\n",
            "saving model with acc 0.708\n",
            "[009/100] Train Acc: 0.649022 Loss: 1.161681 | Val Acc: 0.711257 loss: 0.917410\n",
            "saving model with acc 0.711\n",
            "[010/100] Train Acc: 0.653568 Loss: 1.145086 | Val Acc: 0.715233 loss: 0.904172\n",
            "saving model with acc 0.715\n",
            "[011/100] Train Acc: 0.658192 Loss: 1.128778 | Val Acc: 0.717009 loss: 0.895175\n",
            "saving model with acc 0.717\n",
            "[012/100] Train Acc: 0.661174 Loss: 1.117311 | Val Acc: 0.719579 loss: 0.886991\n",
            "saving model with acc 0.720\n",
            "[013/100] Train Acc: 0.664010 Loss: 1.107523 | Val Acc: 0.722989 loss: 0.875534\n",
            "saving model with acc 0.723\n",
            "[014/100] Train Acc: 0.665952 Loss: 1.098683 | Val Acc: 0.724030 loss: 0.872029\n",
            "saving model with acc 0.724\n",
            "[015/100] Train Acc: 0.668224 Loss: 1.090275 | Val Acc: 0.725030 loss: 0.867695\n",
            "saving model with acc 0.725\n",
            "[016/100] Train Acc: 0.670502 Loss: 1.084560 | Val Acc: 0.727136 loss: 0.858478\n",
            "saving model with acc 0.727\n",
            "[017/100] Train Acc: 0.671623 Loss: 1.078307 | Val Acc: 0.727587 loss: 0.856515\n",
            "saving model with acc 0.728\n",
            "[018/100] Train Acc: 0.672826 Loss: 1.072842 | Val Acc: 0.729197 loss: 0.852613\n",
            "saving model with acc 0.729\n",
            "[019/100] Train Acc: 0.674822 Loss: 1.067545 | Val Acc: 0.729652 loss: 0.849569\n",
            "saving model with acc 0.730\n",
            "[020/100] Train Acc: 0.675690 Loss: 1.062357 | Val Acc: 0.729831 loss: 0.846419\n",
            "saving model with acc 0.730\n",
            "[021/100] Train Acc: 0.676714 Loss: 1.059624 | Val Acc: 0.731482 loss: 0.840396\n",
            "saving model with acc 0.731\n",
            "[022/100] Train Acc: 0.677629 Loss: 1.054593 | Val Acc: 0.731364 loss: 0.841150\n",
            "[023/100] Train Acc: 0.678502 Loss: 1.051965 | Val Acc: 0.733612 loss: 0.833699\n",
            "saving model with acc 0.734\n",
            "[024/100] Train Acc: 0.679472 Loss: 1.049439 | Val Acc: 0.733962 loss: 0.833777\n",
            "saving model with acc 0.734\n",
            "[025/100] Train Acc: 0.679778 Loss: 1.046239 | Val Acc: 0.733754 loss: 0.833125\n",
            "[026/100] Train Acc: 0.681400 Loss: 1.042609 | Val Acc: 0.734437 loss: 0.830740\n",
            "saving model with acc 0.734\n",
            "[027/100] Train Acc: 0.681332 Loss: 1.040228 | Val Acc: 0.734161 loss: 0.831182\n",
            "[028/100] Train Acc: 0.682349 Loss: 1.037972 | Val Acc: 0.735136 loss: 0.827972\n",
            "saving model with acc 0.735\n",
            "[029/100] Train Acc: 0.682888 Loss: 1.036478 | Val Acc: 0.736584 loss: 0.824179\n",
            "saving model with acc 0.737\n",
            "[030/100] Train Acc: 0.683388 Loss: 1.033771 | Val Acc: 0.737234 loss: 0.820302\n",
            "saving model with acc 0.737\n",
            "[031/100] Train Acc: 0.684198 Loss: 1.032339 | Val Acc: 0.736441 loss: 0.821568\n",
            "[032/100] Train Acc: 0.684592 Loss: 1.028687 | Val Acc: 0.736954 loss: 0.822499\n",
            "[033/100] Train Acc: 0.684332 Loss: 1.028289 | Val Acc: 0.737169 loss: 0.819140\n",
            "[034/100] Train Acc: 0.685521 Loss: 1.026294 | Val Acc: 0.737559 loss: 0.817367\n",
            "saving model with acc 0.738\n",
            "[035/100] Train Acc: 0.685263 Loss: 1.025384 | Val Acc: 0.739015 loss: 0.815007\n",
            "saving model with acc 0.739\n",
            "[036/100] Train Acc: 0.686268 Loss: 1.024454 | Val Acc: 0.739072 loss: 0.813296\n",
            "saving model with acc 0.739\n",
            "[037/100] Train Acc: 0.686788 Loss: 1.022313 | Val Acc: 0.739515 loss: 0.812903\n",
            "saving model with acc 0.740\n",
            "[038/100] Train Acc: 0.687132 Loss: 1.020309 | Val Acc: 0.738978 loss: 0.814647\n",
            "[039/100] Train Acc: 0.687098 Loss: 1.020243 | Val Acc: 0.740084 loss: 0.811926\n",
            "saving model with acc 0.740\n",
            "[040/100] Train Acc: 0.687436 Loss: 1.019588 | Val Acc: 0.739750 loss: 0.810763\n",
            "[041/100] Train Acc: 0.687608 Loss: 1.017789 | Val Acc: 0.739242 loss: 0.810595\n",
            "[042/100] Train Acc: 0.687392 Loss: 1.016895 | Val Acc: 0.739966 loss: 0.810881\n",
            "[043/100] Train Acc: 0.687431 Loss: 1.016469 | Val Acc: 0.739840 loss: 0.808420\n",
            "[044/100] Train Acc: 0.688011 Loss: 1.014416 | Val Acc: 0.741433 loss: 0.808921\n",
            "saving model with acc 0.741\n",
            "[045/100] Train Acc: 0.688508 Loss: 1.015018 | Val Acc: 0.740230 loss: 0.808839\n",
            "[046/100] Train Acc: 0.688901 Loss: 1.013722 | Val Acc: 0.740755 loss: 0.807063\n",
            "[047/100] Train Acc: 0.689110 Loss: 1.012142 | Val Acc: 0.742263 loss: 0.803676\n",
            "saving model with acc 0.742\n",
            "[048/100] Train Acc: 0.689373 Loss: 1.011101 | Val Acc: 0.741125 loss: 0.806443\n",
            "[049/100] Train Acc: 0.688993 Loss: 1.010077 | Val Acc: 0.741226 loss: 0.805263\n",
            "[050/100] Train Acc: 0.689702 Loss: 1.009671 | Val Acc: 0.741125 loss: 0.805653\n",
            "[051/100] Train Acc: 0.689767 Loss: 1.009715 | Val Acc: 0.741450 loss: 0.802651\n",
            "[052/100] Train Acc: 0.689639 Loss: 1.009464 | Val Acc: 0.742047 loss: 0.802676\n",
            "[053/100] Train Acc: 0.690131 Loss: 1.008009 | Val Acc: 0.742381 loss: 0.802235\n",
            "saving model with acc 0.742\n",
            "[054/100] Train Acc: 0.690695 Loss: 1.006849 | Val Acc: 0.742442 loss: 0.801405\n",
            "saving model with acc 0.742\n",
            "[055/100] Train Acc: 0.689924 Loss: 1.007379 | Val Acc: 0.742808 loss: 0.802360\n",
            "saving model with acc 0.743\n",
            "[056/100] Train Acc: 0.690133 Loss: 1.007761 | Val Acc: 0.742568 loss: 0.802151\n",
            "[057/100] Train Acc: 0.690811 Loss: 1.006567 | Val Acc: 0.742295 loss: 0.802337\n",
            "[058/100] Train Acc: 0.690831 Loss: 1.006096 | Val Acc: 0.743425 loss: 0.798639\n",
            "saving model with acc 0.743\n",
            "[059/100] Train Acc: 0.690827 Loss: 1.005555 | Val Acc: 0.743129 loss: 0.799666\n",
            "[060/100] Train Acc: 0.691170 Loss: 1.004758 | Val Acc: 0.743023 loss: 0.800078\n",
            "[061/100] Train Acc: 0.690975 Loss: 1.004400 | Val Acc: 0.741799 loss: 0.802187\n",
            "[062/100] Train Acc: 0.691656 Loss: 1.002806 | Val Acc: 0.743995 loss: 0.797721\n",
            "saving model with acc 0.744\n",
            "[063/100] Train Acc: 0.691143 Loss: 1.002754 | Val Acc: 0.743417 loss: 0.799187\n",
            "[064/100] Train Acc: 0.691121 Loss: 1.003164 | Val Acc: 0.743247 loss: 0.798768\n",
            "[065/100] Train Acc: 0.692089 Loss: 1.001044 | Val Acc: 0.743682 loss: 0.795204\n",
            "[066/100] Train Acc: 0.692013 Loss: 1.002404 | Val Acc: 0.743417 loss: 0.799101\n",
            "[067/100] Train Acc: 0.691996 Loss: 1.002333 | Val Acc: 0.743539 loss: 0.796124\n",
            "[068/100] Train Acc: 0.691908 Loss: 1.001743 | Val Acc: 0.743064 loss: 0.798450\n",
            "[069/100] Train Acc: 0.692662 Loss: 1.001208 | Val Acc: 0.743214 loss: 0.797966\n",
            "[070/100] Train Acc: 0.691928 Loss: 0.999998 | Val Acc: 0.744759 loss: 0.794149\n",
            "saving model with acc 0.745\n",
            "[071/100] Train Acc: 0.692111 Loss: 1.000421 | Val Acc: 0.743877 loss: 0.796928\n",
            "[072/100] Train Acc: 0.692692 Loss: 0.999245 | Val Acc: 0.744621 loss: 0.794998\n",
            "[073/100] Train Acc: 0.692409 Loss: 0.999863 | Val Acc: 0.743637 loss: 0.797521\n",
            "[074/100] Train Acc: 0.692197 Loss: 1.000173 | Val Acc: 0.744934 loss: 0.793649\n",
            "saving model with acc 0.745\n",
            "[075/100] Train Acc: 0.693344 Loss: 0.997742 | Val Acc: 0.744096 loss: 0.794352\n",
            "[076/100] Train Acc: 0.692687 Loss: 0.999193 | Val Acc: 0.744511 loss: 0.793879\n",
            "[077/100] Train Acc: 0.692970 Loss: 0.999087 | Val Acc: 0.744381 loss: 0.794467\n",
            "[078/100] Train Acc: 0.692956 Loss: 0.998197 | Val Acc: 0.744202 loss: 0.795117\n",
            "[079/100] Train Acc: 0.692947 Loss: 0.997076 | Val Acc: 0.744283 loss: 0.794061\n",
            "[080/100] Train Acc: 0.692914 Loss: 0.998095 | Val Acc: 0.745214 loss: 0.793865\n",
            "saving model with acc 0.745\n",
            "[081/100] Train Acc: 0.692947 Loss: 0.997448 | Val Acc: 0.745255 loss: 0.791042\n",
            "saving model with acc 0.745\n",
            "[082/100] Train Acc: 0.693100 Loss: 0.997288 | Val Acc: 0.745804 loss: 0.792418\n",
            "saving model with acc 0.746\n",
            "[083/100] Train Acc: 0.692994 Loss: 0.996858 | Val Acc: 0.745035 loss: 0.792436\n",
            "[084/100] Train Acc: 0.693181 Loss: 0.996943 | Val Acc: 0.745068 loss: 0.793145\n",
            "[085/100] Train Acc: 0.693346 Loss: 0.996496 | Val Acc: 0.744621 loss: 0.794212\n",
            "[086/100] Train Acc: 0.693446 Loss: 0.996473 | Val Acc: 0.744763 loss: 0.793019\n",
            "[087/100] Train Acc: 0.693404 Loss: 0.995963 | Val Acc: 0.745027 loss: 0.792891\n",
            "[088/100] Train Acc: 0.693903 Loss: 0.994747 | Val Acc: 0.743930 loss: 0.794129\n",
            "[089/100] Train Acc: 0.693030 Loss: 0.996358 | Val Acc: 0.745543 loss: 0.791942\n",
            "[090/100] Train Acc: 0.693559 Loss: 0.994327 | Val Acc: 0.745531 loss: 0.790070\n",
            "[091/100] Train Acc: 0.693825 Loss: 0.994703 | Val Acc: 0.746552 loss: 0.788985\n",
            "saving model with acc 0.747\n",
            "[092/100] Train Acc: 0.693388 Loss: 0.995847 | Val Acc: 0.743946 loss: 0.794652\n",
            "[093/100] Train Acc: 0.693357 Loss: 0.996298 | Val Acc: 0.745507 loss: 0.790873\n",
            "[094/100] Train Acc: 0.693716 Loss: 0.993974 | Val Acc: 0.745304 loss: 0.793970\n",
            "[095/100] Train Acc: 0.693710 Loss: 0.993948 | Val Acc: 0.745548 loss: 0.790369\n",
            "[096/100] Train Acc: 0.693482 Loss: 0.994724 | Val Acc: 0.745324 loss: 0.793152\n",
            "[097/100] Train Acc: 0.694165 Loss: 0.992866 | Val Acc: 0.745438 loss: 0.791732\n",
            "[098/100] Train Acc: 0.693672 Loss: 0.994034 | Val Acc: 0.745377 loss: 0.792348\n",
            "[099/100] Train Acc: 0.694309 Loss: 0.994316 | Val Acc: 0.745698 loss: 0.789078\n",
            "[100/100] Train Acc: 0.693263 Loss: 0.994709 | Val Acc: 0.745519 loss: 0.790291\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTr80Panaq8"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiBILuUEncLl",
        "outputId": "48a85b5f-a665-4763-97fa-94ead4b89741"
      },
      "source": [
        "# create testing dataset\r\n",
        "test_set = TIMITDataset(test, None)\r\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\r\n",
        "\r\n",
        "# create model and load weights from checkpoint\r\n",
        "model = Classifier().to(device)\r\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP_tUJeLnd94"
      },
      "source": [
        "predict = []\r\n",
        "model.eval() # set the model to evaluation mode\r\n",
        "with torch.no_grad():\r\n",
        "    for i, data in enumerate(test_loader):\r\n",
        "        inputs = data\r\n",
        "        inputs = inputs.to(device)\r\n",
        "        outputs = model(inputs)\r\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\r\n",
        "\r\n",
        "        for y in test_pred.cpu().numpy():\r\n",
        "            predict.append(y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIjAsNTnhgs"
      },
      "source": [
        "# **Write prediction to a CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "athJ5R7BnlN5",
        "outputId": "6fca5b3f-ceee-4a2e-f4e3-a4d70ce8493c"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\r\n",
        "    f.write('Id,Class\\n')\r\n",
        "    for i, y in enumerate(predict):\r\n",
        "        f.write('{},{}\\n'.format(i, y))\r\n",
        "\r\n",
        "print('Saving results to prediction.csv')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to prediction.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV5cHjHLos62"
      },
      "source": [
        "# **Reference**\r\n",
        "\r\n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb)"
      ]
    }
  ]
}