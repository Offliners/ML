{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Offliners/ML/blob/main/HW2/homework2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghYxMSxsmAYm"
      },
      "source": [
        "# **Homework 2-1 Phoneme Classification**\n",
        "\n",
        "The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
        "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
        "\n",
        "This homework is a multiclass classification task, we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
        "\n",
        "## **Download Data**\n",
        "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3\n",
        "\n",
        "timit_11/\n",
        "\n",
        "* train_11.npy: training data\n",
        "* train_label_11.npy: training label\n",
        "* test_11.npy: testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N98vZGvSl-Fq",
        "outputId": "545fcc79-f7a0-4e3b-c0ec-e961ed376eaf"
      },
      "source": [
        "!gdown --id '1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1duKUYSwilRG6BF8cLz8L_LRGDE7EFLHG\n",
            "To: /content/data.zip\n",
            "376MB [00:02, 128MB/s]\n",
            "Archive:  data.zip\n",
            "  inflating: sampleSubmission.csv    \n",
            "  inflating: timit_11/timit_11/test_11.npy  \n",
            "  inflating: timit_11/timit_11/train_11.npy  \n",
            "  inflating: timit_11/timit_11/train_label_11.npy  \n",
            "data.zip  sample_data  sampleSubmission.csv  timit_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ffxBMT3mU5g"
      },
      "source": [
        "# **Preparing Data**\n",
        "\n",
        "Load the training and testing data from the .npy file (NumPy array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRTYp6bemeRk",
        "outputId": "2e4f1e90-3ad1-4d2e-fdc9-f9f576d6a9ed"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./timit_11/timit_11/'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpQ84yh8me6A"
      },
      "source": [
        "# **Create Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPvM4GoFmhzo"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmKia2rfmmk9",
        "outputId": "18e9a88e-7637-4be4-ce01-89aded585111"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler().fit(train)\n",
        "VAL_RATIO = 0.1\n",
        "\n",
        "train = scaler.transform(train)\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: (1106938, 429)\n",
            "Size of validation set: (122994, 429)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA1fNcNamqKa"
      },
      "source": [
        "BATCH_SIZE = 4096\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kqycVinmj11"
      },
      "source": [
        "#### **notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later the data size is quite huge, so be aware of memory usage in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7dGNUuGmtDD",
        "outputId": "bace7e98-95d0-421f-93f1-65fa689cf499"
      },
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXFtiY94m6X-"
      },
      "source": [
        "# **Create Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfFT6XJ8nBCS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(429, 2048)\n",
        "        self.layer2 = nn.Linear(2048, 2048)\n",
        "        self.layer3 = nn.Linear(2048, 2048)\n",
        "        self.layer4 = nn.Linear(2048, 1024)\n",
        "        self.layer5 = nn.Linear(1024, 512)\n",
        "        self.layer6 = nn.Linear(512, 128)\n",
        "        self.out = nn.Linear(128, 39) \n",
        "        self.dp = nn.Dropout(0.5)\n",
        "        self.bn1 = nn.BatchNorm1d(2048)\n",
        "        self.bn2 = nn.BatchNorm1d(2048)\n",
        "        self.bn3 = nn.BatchNorm1d(2048)\n",
        "        self.bn4 = nn.BatchNorm1d(1024)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.bn6 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.act_fn = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer5(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.layer6(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.bn6(x)\n",
        "        x = self.dp(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLq4t_knLUL"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBsChEconOOV",
        "outputId": "15b38ffb-150b-4e8c-f8f4-9130fae0c3e0"
      },
      "source": [
        "#check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 400               # number of training epoch\n",
        "learning_rate = 1e-4         # learning rate\n",
        "l2 = 1e-3                    # L2 regularization\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyjl3itJnWwL",
        "outputId": "2037f3f7-373c-4462-b1c8-84c10c58f080"
      },
      "source": [
        "# start training\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[001/400] Train Acc: 0.348914 Loss: 2.475971 | Val Acc: 0.509887 loss: 1.670745\n",
            "saving model with acc 0.510\n",
            "[002/400] Train Acc: 0.493851 Loss: 1.796272 | Val Acc: 0.589118 loss: 1.391982\n",
            "saving model with acc 0.589\n",
            "[003/400] Train Acc: 0.546293 Loss: 1.587456 | Val Acc: 0.626624 loss: 1.243734\n",
            "saving model with acc 0.627\n",
            "[004/400] Train Acc: 0.579035 Loss: 1.456908 | Val Acc: 0.654129 loss: 1.140376\n",
            "saving model with acc 0.654\n",
            "[005/400] Train Acc: 0.600908 Loss: 1.370407 | Val Acc: 0.670756 loss: 1.078856\n",
            "saving model with acc 0.671\n",
            "[006/400] Train Acc: 0.618489 Loss: 1.304310 | Val Acc: 0.685196 loss: 1.030104\n",
            "saving model with acc 0.685\n",
            "[007/400] Train Acc: 0.631054 Loss: 1.255671 | Val Acc: 0.693473 loss: 0.995120\n",
            "saving model with acc 0.693\n",
            "[008/400] Train Acc: 0.642021 Loss: 1.214925 | Val Acc: 0.700790 loss: 0.967315\n",
            "saving model with acc 0.701\n",
            "[009/400] Train Acc: 0.650686 Loss: 1.181371 | Val Acc: 0.705360 loss: 0.946107\n",
            "saving model with acc 0.705\n",
            "[010/400] Train Acc: 0.658354 Loss: 1.152560 | Val Acc: 0.712010 loss: 0.924139\n",
            "saving model with acc 0.712\n",
            "[011/400] Train Acc: 0.665065 Loss: 1.128366 | Val Acc: 0.716425 loss: 0.907294\n",
            "saving model with acc 0.716\n",
            "[012/400] Train Acc: 0.671025 Loss: 1.104470 | Val Acc: 0.720580 loss: 0.892285\n",
            "saving model with acc 0.721\n",
            "[013/400] Train Acc: 0.675710 Loss: 1.086556 | Val Acc: 0.722442 loss: 0.886365\n",
            "saving model with acc 0.722\n",
            "[014/400] Train Acc: 0.680989 Loss: 1.067702 | Val Acc: 0.726247 loss: 0.871360\n",
            "saving model with acc 0.726\n",
            "[015/400] Train Acc: 0.684890 Loss: 1.052738 | Val Acc: 0.730060 loss: 0.856461\n",
            "saving model with acc 0.730\n",
            "[016/400] Train Acc: 0.688563 Loss: 1.036936 | Val Acc: 0.731377 loss: 0.851463\n",
            "saving model with acc 0.731\n",
            "[017/400] Train Acc: 0.691965 Loss: 1.024727 | Val Acc: 0.733483 loss: 0.843912\n",
            "saving model with acc 0.733\n",
            "[018/400] Train Acc: 0.695861 Loss: 1.011147 | Val Acc: 0.734142 loss: 0.837769\n",
            "saving model with acc 0.734\n",
            "[019/400] Train Acc: 0.699374 Loss: 0.997020 | Val Acc: 0.736036 loss: 0.829294\n",
            "saving model with acc 0.736\n",
            "[020/400] Train Acc: 0.702988 Loss: 0.984421 | Val Acc: 0.739792 loss: 0.820387\n",
            "saving model with acc 0.740\n",
            "[021/400] Train Acc: 0.706369 Loss: 0.972187 | Val Acc: 0.741540 loss: 0.814335\n",
            "saving model with acc 0.742\n",
            "[022/400] Train Acc: 0.708137 Loss: 0.963867 | Val Acc: 0.742866 loss: 0.809419\n",
            "saving model with acc 0.743\n",
            "[023/400] Train Acc: 0.711842 Loss: 0.952301 | Val Acc: 0.743817 loss: 0.803830\n",
            "saving model with acc 0.744\n",
            "[024/400] Train Acc: 0.714469 Loss: 0.943730 | Val Acc: 0.745020 loss: 0.799222\n",
            "saving model with acc 0.745\n",
            "[025/400] Train Acc: 0.717220 Loss: 0.932560 | Val Acc: 0.747248 loss: 0.793149\n",
            "saving model with acc 0.747\n",
            "[026/400] Train Acc: 0.720016 Loss: 0.922741 | Val Acc: 0.747614 loss: 0.788332\n",
            "saving model with acc 0.748\n",
            "[027/400] Train Acc: 0.721871 Loss: 0.915916 | Val Acc: 0.749687 loss: 0.783810\n",
            "saving model with acc 0.750\n",
            "[028/400] Train Acc: 0.723967 Loss: 0.906390 | Val Acc: 0.749305 loss: 0.785341\n",
            "[029/400] Train Acc: 0.726414 Loss: 0.897970 | Val Acc: 0.751907 loss: 0.777634\n",
            "saving model with acc 0.752\n",
            "[030/400] Train Acc: 0.728269 Loss: 0.891470 | Val Acc: 0.751931 loss: 0.779732\n",
            "saving model with acc 0.752\n",
            "[031/400] Train Acc: 0.730022 Loss: 0.885618 | Val Acc: 0.751321 loss: 0.773914\n",
            "[032/400] Train Acc: 0.731984 Loss: 0.878342 | Val Acc: 0.753947 loss: 0.771382\n",
            "saving model with acc 0.754\n",
            "[033/400] Train Acc: 0.733286 Loss: 0.872886 | Val Acc: 0.755630 loss: 0.765083\n",
            "saving model with acc 0.756\n",
            "[034/400] Train Acc: 0.735152 Loss: 0.865979 | Val Acc: 0.754460 loss: 0.765939\n",
            "[035/400] Train Acc: 0.736768 Loss: 0.860528 | Val Acc: 0.755704 loss: 0.764468\n",
            "saving model with acc 0.756\n",
            "[036/400] Train Acc: 0.738192 Loss: 0.854092 | Val Acc: 0.757655 loss: 0.759014\n",
            "saving model with acc 0.758\n",
            "[037/400] Train Acc: 0.739170 Loss: 0.850403 | Val Acc: 0.756996 loss: 0.755531\n",
            "[038/400] Train Acc: 0.740445 Loss: 0.845114 | Val Acc: 0.757305 loss: 0.753113\n",
            "[039/400] Train Acc: 0.741722 Loss: 0.841008 | Val Acc: 0.755517 loss: 0.761751\n",
            "[040/400] Train Acc: 0.742752 Loss: 0.836902 | Val Acc: 0.758582 loss: 0.749843\n",
            "saving model with acc 0.759\n",
            "[041/400] Train Acc: 0.743855 Loss: 0.832484 | Val Acc: 0.758728 loss: 0.751195\n",
            "saving model with acc 0.759\n",
            "[042/400] Train Acc: 0.745704 Loss: 0.827038 | Val Acc: 0.759338 loss: 0.748283\n",
            "saving model with acc 0.759\n",
            "[043/400] Train Acc: 0.746158 Loss: 0.823654 | Val Acc: 0.760720 loss: 0.745962\n",
            "saving model with acc 0.761\n",
            "[044/400] Train Acc: 0.746802 Loss: 0.820717 | Val Acc: 0.760501 loss: 0.746668\n",
            "[045/400] Train Acc: 0.748295 Loss: 0.816719 | Val Acc: 0.759850 loss: 0.745874\n",
            "[046/400] Train Acc: 0.748700 Loss: 0.813551 | Val Acc: 0.759948 loss: 0.748469\n",
            "[047/400] Train Acc: 0.749292 Loss: 0.811325 | Val Acc: 0.758874 loss: 0.748043\n",
            "[048/400] Train Acc: 0.750850 Loss: 0.806894 | Val Acc: 0.760964 loss: 0.740839\n",
            "saving model with acc 0.761\n",
            "[049/400] Train Acc: 0.751289 Loss: 0.804121 | Val Acc: 0.762427 loss: 0.742166\n",
            "saving model with acc 0.762\n",
            "[050/400] Train Acc: 0.752042 Loss: 0.800858 | Val Acc: 0.761997 loss: 0.737143\n",
            "[051/400] Train Acc: 0.753883 Loss: 0.795124 | Val Acc: 0.760972 loss: 0.741898\n",
            "[052/400] Train Acc: 0.753312 Loss: 0.795779 | Val Acc: 0.761826 loss: 0.741545\n",
            "[053/400] Train Acc: 0.754374 Loss: 0.793107 | Val Acc: 0.761907 loss: 0.739535\n",
            "[054/400] Train Acc: 0.754813 Loss: 0.791146 | Val Acc: 0.762574 loss: 0.739137\n",
            "saving model with acc 0.763\n",
            "[055/400] Train Acc: 0.755665 Loss: 0.787747 | Val Acc: 0.761972 loss: 0.739116\n",
            "[056/400] Train Acc: 0.755866 Loss: 0.786810 | Val Acc: 0.762289 loss: 0.737199\n",
            "[057/400] Train Acc: 0.756546 Loss: 0.783210 | Val Acc: 0.762362 loss: 0.740226\n",
            "[058/400] Train Acc: 0.757783 Loss: 0.781450 | Val Acc: 0.763224 loss: 0.734382\n",
            "saving model with acc 0.763\n",
            "[059/400] Train Acc: 0.758313 Loss: 0.778630 | Val Acc: 0.764850 loss: 0.731051\n",
            "saving model with acc 0.765\n",
            "[060/400] Train Acc: 0.758848 Loss: 0.777180 | Val Acc: 0.764086 loss: 0.732046\n",
            "[061/400] Train Acc: 0.758670 Loss: 0.776089 | Val Acc: 0.763436 loss: 0.732276\n",
            "[062/400] Train Acc: 0.759675 Loss: 0.773672 | Val Acc: 0.762492 loss: 0.732706\n",
            "[063/400] Train Acc: 0.759933 Loss: 0.771781 | Val Acc: 0.763997 loss: 0.731202\n",
            "[064/400] Train Acc: 0.760901 Loss: 0.768168 | Val Acc: 0.766452 loss: 0.727515\n",
            "saving model with acc 0.766\n",
            "[065/400] Train Acc: 0.761194 Loss: 0.767983 | Val Acc: 0.763655 loss: 0.727196\n",
            "[066/400] Train Acc: 0.762142 Loss: 0.764238 | Val Acc: 0.765029 loss: 0.732199\n",
            "[067/400] Train Acc: 0.761806 Loss: 0.763882 | Val Acc: 0.765452 loss: 0.728878\n",
            "[068/400] Train Acc: 0.762040 Loss: 0.762136 | Val Acc: 0.766485 loss: 0.725986\n",
            "saving model with acc 0.766\n",
            "[069/400] Train Acc: 0.762712 Loss: 0.760355 | Val Acc: 0.765793 loss: 0.730183\n",
            "[070/400] Train Acc: 0.763300 Loss: 0.759174 | Val Acc: 0.764054 loss: 0.731880\n",
            "[071/400] Train Acc: 0.763874 Loss: 0.757442 | Val Acc: 0.766078 loss: 0.722127\n",
            "[072/400] Train Acc: 0.764090 Loss: 0.755584 | Val Acc: 0.765802 loss: 0.723726\n",
            "[073/400] Train Acc: 0.764756 Loss: 0.753267 | Val Acc: 0.765509 loss: 0.725246\n",
            "[074/400] Train Acc: 0.764769 Loss: 0.753316 | Val Acc: 0.765850 loss: 0.728805\n",
            "[075/400] Train Acc: 0.766047 Loss: 0.750462 | Val Acc: 0.765501 loss: 0.725624\n",
            "[076/400] Train Acc: 0.766216 Loss: 0.749819 | Val Acc: 0.766119 loss: 0.724428\n",
            "[077/400] Train Acc: 0.766676 Loss: 0.748262 | Val Acc: 0.765322 loss: 0.726351\n",
            "[078/400] Train Acc: 0.766827 Loss: 0.746807 | Val Acc: 0.765476 loss: 0.728161\n",
            "[079/400] Train Acc: 0.766967 Loss: 0.744996 | Val Acc: 0.766151 loss: 0.724147\n",
            "[080/400] Train Acc: 0.767407 Loss: 0.743897 | Val Acc: 0.766086 loss: 0.725992\n",
            "[081/400] Train Acc: 0.767507 Loss: 0.743099 | Val Acc: 0.766159 loss: 0.724648\n",
            "[082/400] Train Acc: 0.768219 Loss: 0.740832 | Val Acc: 0.766550 loss: 0.724983\n",
            "saving model with acc 0.767\n",
            "[083/400] Train Acc: 0.768224 Loss: 0.741035 | Val Acc: 0.768029 loss: 0.722079\n",
            "saving model with acc 0.768\n",
            "[084/400] Train Acc: 0.768999 Loss: 0.739134 | Val Acc: 0.766891 loss: 0.719530\n",
            "[085/400] Train Acc: 0.768988 Loss: 0.739297 | Val Acc: 0.766379 loss: 0.725321\n",
            "[086/400] Train Acc: 0.769378 Loss: 0.736507 | Val Acc: 0.765631 loss: 0.728764\n",
            "[087/400] Train Acc: 0.769214 Loss: 0.736828 | Val Acc: 0.766826 loss: 0.722296\n",
            "[088/400] Train Acc: 0.769726 Loss: 0.734720 | Val Acc: 0.766696 loss: 0.720389\n",
            "[089/400] Train Acc: 0.770056 Loss: 0.732555 | Val Acc: 0.766940 loss: 0.721828\n",
            "[090/400] Train Acc: 0.770031 Loss: 0.733143 | Val Acc: 0.767298 loss: 0.724917\n",
            "[091/400] Train Acc: 0.771080 Loss: 0.730252 | Val Acc: 0.767948 loss: 0.722141\n",
            "[092/400] Train Acc: 0.771302 Loss: 0.730840 | Val Acc: 0.766875 loss: 0.719792\n",
            "[093/400] Train Acc: 0.771533 Loss: 0.729838 | Val Acc: 0.765501 loss: 0.726894\n",
            "[094/400] Train Acc: 0.771488 Loss: 0.727446 | Val Acc: 0.765932 loss: 0.723180\n",
            "[095/400] Train Acc: 0.772321 Loss: 0.726804 | Val Acc: 0.766005 loss: 0.726989\n",
            "[096/400] Train Acc: 0.772033 Loss: 0.727092 | Val Acc: 0.769029 loss: 0.719348\n",
            "saving model with acc 0.769\n",
            "[097/400] Train Acc: 0.772639 Loss: 0.724996 | Val Acc: 0.767257 loss: 0.723605\n",
            "[098/400] Train Acc: 0.772876 Loss: 0.724471 | Val Acc: 0.767013 loss: 0.724828\n",
            "[099/400] Train Acc: 0.773378 Loss: 0.722712 | Val Acc: 0.767346 loss: 0.723362\n",
            "[100/400] Train Acc: 0.773347 Loss: 0.722891 | Val Acc: 0.768363 loss: 0.721716\n",
            "[101/400] Train Acc: 0.773320 Loss: 0.721877 | Val Acc: 0.766452 loss: 0.722545\n",
            "[102/400] Train Acc: 0.774070 Loss: 0.720025 | Val Acc: 0.767420 loss: 0.721919\n",
            "[103/400] Train Acc: 0.773746 Loss: 0.719431 | Val Acc: 0.767298 loss: 0.722720\n",
            "[104/400] Train Acc: 0.774477 Loss: 0.718361 | Val Acc: 0.767940 loss: 0.723414\n",
            "[105/400] Train Acc: 0.774974 Loss: 0.718016 | Val Acc: 0.768810 loss: 0.720910\n",
            "[106/400] Train Acc: 0.774912 Loss: 0.717344 | Val Acc: 0.767647 loss: 0.720980\n",
            "[107/400] Train Acc: 0.774779 Loss: 0.717550 | Val Acc: 0.767208 loss: 0.721988\n",
            "[108/400] Train Acc: 0.775402 Loss: 0.714690 | Val Acc: 0.768298 loss: 0.723487\n",
            "[109/400] Train Acc: 0.775640 Loss: 0.715353 | Val Acc: 0.767151 loss: 0.724714\n",
            "[110/400] Train Acc: 0.775416 Loss: 0.714961 | Val Acc: 0.767785 loss: 0.721334\n",
            "[111/400] Train Acc: 0.775368 Loss: 0.714081 | Val Acc: 0.768265 loss: 0.719253\n",
            "[112/400] Train Acc: 0.776238 Loss: 0.712266 | Val Acc: 0.766550 loss: 0.723501\n",
            "[113/400] Train Acc: 0.776492 Loss: 0.711700 | Val Acc: 0.768371 loss: 0.718190\n",
            "[114/400] Train Acc: 0.776305 Loss: 0.711777 | Val Acc: 0.767509 loss: 0.722765\n",
            "[115/400] Train Acc: 0.776375 Loss: 0.711103 | Val Acc: 0.767444 loss: 0.723710\n",
            "[116/400] Train Acc: 0.776437 Loss: 0.710378 | Val Acc: 0.767224 loss: 0.725230\n",
            "[117/400] Train Acc: 0.777196 Loss: 0.709227 | Val Acc: 0.768883 loss: 0.722984\n",
            "[118/400] Train Acc: 0.777120 Loss: 0.708493 | Val Acc: 0.767997 loss: 0.720391\n",
            "[119/400] Train Acc: 0.777211 Loss: 0.708374 | Val Acc: 0.768444 loss: 0.719361\n",
            "[120/400] Train Acc: 0.777573 Loss: 0.706651 | Val Acc: 0.767151 loss: 0.717507\n",
            "[121/400] Train Acc: 0.777754 Loss: 0.706797 | Val Acc: 0.767224 loss: 0.724288\n",
            "[122/400] Train Acc: 0.777981 Loss: 0.705864 | Val Acc: 0.768794 loss: 0.718585\n",
            "[123/400] Train Acc: 0.778078 Loss: 0.706098 | Val Acc: 0.768200 loss: 0.720693\n",
            "[124/400] Train Acc: 0.778001 Loss: 0.705098 | Val Acc: 0.768712 loss: 0.720333\n",
            "[125/400] Train Acc: 0.778523 Loss: 0.704256 | Val Acc: 0.769029 loss: 0.718866\n",
            "[126/400] Train Acc: 0.778579 Loss: 0.702885 | Val Acc: 0.769477 loss: 0.721014\n",
            "saving model with acc 0.769\n",
            "[127/400] Train Acc: 0.778640 Loss: 0.703475 | Val Acc: 0.768761 loss: 0.719751\n",
            "[128/400] Train Acc: 0.778065 Loss: 0.703647 | Val Acc: 0.769379 loss: 0.721429\n",
            "[129/400] Train Acc: 0.778864 Loss: 0.701451 | Val Acc: 0.769054 loss: 0.723131\n",
            "[130/400] Train Acc: 0.779631 Loss: 0.701517 | Val Acc: 0.769834 loss: 0.718193\n",
            "saving model with acc 0.770\n",
            "[131/400] Train Acc: 0.779789 Loss: 0.699480 | Val Acc: 0.770981 loss: 0.716151\n",
            "saving model with acc 0.771\n",
            "[132/400] Train Acc: 0.779227 Loss: 0.700656 | Val Acc: 0.768493 loss: 0.719615\n",
            "[133/400] Train Acc: 0.779106 Loss: 0.700563 | Val Acc: 0.768672 loss: 0.723339\n",
            "[134/400] Train Acc: 0.779317 Loss: 0.700085 | Val Acc: 0.767940 loss: 0.719869\n",
            "[135/400] Train Acc: 0.780033 Loss: 0.699101 | Val Acc: 0.767777 loss: 0.721105\n",
            "[136/400] Train Acc: 0.779317 Loss: 0.699505 | Val Acc: 0.768086 loss: 0.722606\n",
            "[137/400] Train Acc: 0.779794 Loss: 0.697736 | Val Acc: 0.767964 loss: 0.721911\n",
            "[138/400] Train Acc: 0.779989 Loss: 0.697269 | Val Acc: 0.767151 loss: 0.722416\n",
            "[139/400] Train Acc: 0.779903 Loss: 0.698175 | Val Acc: 0.770111 loss: 0.717737\n",
            "[140/400] Train Acc: 0.780428 Loss: 0.696335 | Val Acc: 0.767997 loss: 0.718160\n",
            "[141/400] Train Acc: 0.780603 Loss: 0.695852 | Val Acc: 0.768607 loss: 0.717597\n",
            "[142/400] Train Acc: 0.780446 Loss: 0.695786 | Val Acc: 0.768802 loss: 0.719518\n",
            "[143/400] Train Acc: 0.781059 Loss: 0.696019 | Val Acc: 0.770485 loss: 0.712933\n",
            "[144/400] Train Acc: 0.781243 Loss: 0.694334 | Val Acc: 0.769233 loss: 0.715269\n",
            "[145/400] Train Acc: 0.780752 Loss: 0.695130 | Val Acc: 0.770688 loss: 0.713468\n",
            "[146/400] Train Acc: 0.781414 Loss: 0.693655 | Val Acc: 0.768720 loss: 0.723024\n",
            "[147/400] Train Acc: 0.780960 Loss: 0.694134 | Val Acc: 0.771916 loss: 0.716886\n",
            "saving model with acc 0.772\n",
            "[148/400] Train Acc: 0.781546 Loss: 0.693239 | Val Acc: 0.770395 loss: 0.715684\n",
            "[149/400] Train Acc: 0.781315 Loss: 0.693229 | Val Acc: 0.768704 loss: 0.721048\n",
            "[150/400] Train Acc: 0.782237 Loss: 0.691544 | Val Acc: 0.768452 loss: 0.721567\n",
            "[151/400] Train Acc: 0.782116 Loss: 0.691485 | Val Acc: 0.769225 loss: 0.719966\n",
            "[152/400] Train Acc: 0.781912 Loss: 0.692086 | Val Acc: 0.771216 loss: 0.718941\n",
            "[153/400] Train Acc: 0.781172 Loss: 0.693681 | Val Acc: 0.768241 loss: 0.718498\n",
            "[154/400] Train Acc: 0.782089 Loss: 0.691202 | Val Acc: 0.768916 loss: 0.718427\n",
            "[155/400] Train Acc: 0.781668 Loss: 0.691032 | Val Acc: 0.768119 loss: 0.721480\n",
            "[156/400] Train Acc: 0.781786 Loss: 0.691470 | Val Acc: 0.768785 loss: 0.718395\n",
            "[157/400] Train Acc: 0.782048 Loss: 0.691263 | Val Acc: 0.771225 loss: 0.718487\n",
            "[158/400] Train Acc: 0.782305 Loss: 0.689559 | Val Acc: 0.770631 loss: 0.715546\n",
            "[159/400] Train Acc: 0.782848 Loss: 0.689254 | Val Acc: 0.769704 loss: 0.717877\n",
            "[160/400] Train Acc: 0.782954 Loss: 0.688990 | Val Acc: 0.768566 loss: 0.718546\n",
            "[161/400] Train Acc: 0.783331 Loss: 0.687922 | Val Acc: 0.769338 loss: 0.715096\n",
            "[162/400] Train Acc: 0.782170 Loss: 0.689126 | Val Acc: 0.768956 loss: 0.717796\n",
            "[163/400] Train Acc: 0.782485 Loss: 0.689014 | Val Acc: 0.768355 loss: 0.724760\n",
            "[164/400] Train Acc: 0.783134 Loss: 0.686973 | Val Acc: 0.767029 loss: 0.722626\n",
            "[165/400] Train Acc: 0.782903 Loss: 0.686071 | Val Acc: 0.769826 loss: 0.719684\n",
            "[166/400] Train Acc: 0.782862 Loss: 0.687751 | Val Acc: 0.770753 loss: 0.715121\n",
            "[167/400] Train Acc: 0.783045 Loss: 0.687693 | Val Acc: 0.770347 loss: 0.719012\n",
            "[168/400] Train Acc: 0.782920 Loss: 0.686507 | Val Acc: 0.769029 loss: 0.720795\n",
            "[169/400] Train Acc: 0.783244 Loss: 0.686566 | Val Acc: 0.770103 loss: 0.718769\n",
            "[170/400] Train Acc: 0.783337 Loss: 0.685920 | Val Acc: 0.769875 loss: 0.719668\n",
            "[171/400] Train Acc: 0.783731 Loss: 0.684937 | Val Acc: 0.768037 loss: 0.721751\n",
            "[172/400] Train Acc: 0.784145 Loss: 0.685319 | Val Acc: 0.768753 loss: 0.716818\n",
            "[173/400] Train Acc: 0.783364 Loss: 0.685749 | Val Acc: 0.768598 loss: 0.719450\n",
            "[174/400] Train Acc: 0.783365 Loss: 0.685245 | Val Acc: 0.769363 loss: 0.717731\n",
            "[175/400] Train Acc: 0.784052 Loss: 0.684037 | Val Acc: 0.769111 loss: 0.716633\n",
            "[176/400] Train Acc: 0.784434 Loss: 0.683806 | Val Acc: 0.769363 loss: 0.719356\n",
            "[177/400] Train Acc: 0.784197 Loss: 0.684136 | Val Acc: 0.770485 loss: 0.717921\n",
            "[178/400] Train Acc: 0.783362 Loss: 0.685132 | Val Acc: 0.770696 loss: 0.717570\n",
            "[179/400] Train Acc: 0.784180 Loss: 0.682724 | Val Acc: 0.770029 loss: 0.719448\n",
            "[180/400] Train Acc: 0.783905 Loss: 0.682796 | Val Acc: 0.769062 loss: 0.720313\n",
            "[181/400] Train Acc: 0.783982 Loss: 0.683066 | Val Acc: 0.769493 loss: 0.719297\n",
            "[182/400] Train Acc: 0.784432 Loss: 0.681616 | Val Acc: 0.769729 loss: 0.719196\n",
            "[183/400] Train Acc: 0.784820 Loss: 0.681676 | Val Acc: 0.768818 loss: 0.721150\n",
            "[184/400] Train Acc: 0.784592 Loss: 0.682013 | Val Acc: 0.770412 loss: 0.717216\n",
            "[185/400] Train Acc: 0.784636 Loss: 0.681262 | Val Acc: 0.768948 loss: 0.719831\n",
            "[186/400] Train Acc: 0.784643 Loss: 0.681500 | Val Acc: 0.769135 loss: 0.720467\n",
            "[187/400] Train Acc: 0.784542 Loss: 0.681389 | Val Acc: 0.768054 loss: 0.719759\n",
            "[188/400] Train Acc: 0.784719 Loss: 0.681396 | Val Acc: 0.768062 loss: 0.720430\n",
            "[189/400] Train Acc: 0.785002 Loss: 0.679511 | Val Acc: 0.769786 loss: 0.721832\n",
            "[190/400] Train Acc: 0.785046 Loss: 0.680917 | Val Acc: 0.768680 loss: 0.717306\n",
            "[191/400] Train Acc: 0.784838 Loss: 0.680558 | Val Acc: 0.769940 loss: 0.718759\n",
            "[192/400] Train Acc: 0.785696 Loss: 0.677525 | Val Acc: 0.769729 loss: 0.719502\n",
            "[193/400] Train Acc: 0.785356 Loss: 0.679796 | Val Acc: 0.770436 loss: 0.721392\n",
            "[194/400] Train Acc: 0.784878 Loss: 0.679377 | Val Acc: 0.769355 loss: 0.718099\n",
            "[195/400] Train Acc: 0.785320 Loss: 0.678802 | Val Acc: 0.770371 loss: 0.720488\n",
            "[196/400] Train Acc: 0.785351 Loss: 0.678380 | Val Acc: 0.770842 loss: 0.717741\n",
            "[197/400] Train Acc: 0.785428 Loss: 0.678758 | Val Acc: 0.770631 loss: 0.719713\n",
            "[198/400] Train Acc: 0.785347 Loss: 0.678807 | Val Acc: 0.769330 loss: 0.721439\n",
            "[199/400] Train Acc: 0.785267 Loss: 0.678381 | Val Acc: 0.769590 loss: 0.718087\n",
            "[200/400] Train Acc: 0.785232 Loss: 0.677976 | Val Acc: 0.769558 loss: 0.716564\n",
            "[201/400] Train Acc: 0.785563 Loss: 0.678084 | Val Acc: 0.769664 loss: 0.720570\n",
            "[202/400] Train Acc: 0.785330 Loss: 0.678918 | Val Acc: 0.768460 loss: 0.722338\n",
            "[203/400] Train Acc: 0.785881 Loss: 0.677398 | Val Acc: 0.768387 loss: 0.721029\n",
            "[204/400] Train Acc: 0.785544 Loss: 0.677681 | Val Acc: 0.770135 loss: 0.718289\n",
            "[205/400] Train Acc: 0.786311 Loss: 0.676227 | Val Acc: 0.769070 loss: 0.722702\n",
            "[206/400] Train Acc: 0.785620 Loss: 0.677894 | Val Acc: 0.769932 loss: 0.719061\n",
            "[207/400] Train Acc: 0.786512 Loss: 0.676567 | Val Acc: 0.768525 loss: 0.720494\n",
            "[208/400] Train Acc: 0.786341 Loss: 0.676286 | Val Acc: 0.769249 loss: 0.716086\n",
            "[209/400] Train Acc: 0.785953 Loss: 0.676061 | Val Acc: 0.769599 loss: 0.720815\n",
            "[210/400] Train Acc: 0.786196 Loss: 0.675898 | Val Acc: 0.769688 loss: 0.718001\n",
            "[211/400] Train Acc: 0.786428 Loss: 0.674856 | Val Acc: 0.770151 loss: 0.717427\n",
            "[212/400] Train Acc: 0.786014 Loss: 0.675968 | Val Acc: 0.770574 loss: 0.719363\n",
            "[213/400] Train Acc: 0.786435 Loss: 0.674411 | Val Acc: 0.770054 loss: 0.719000\n",
            "[214/400] Train Acc: 0.786083 Loss: 0.674880 | Val Acc: 0.767948 loss: 0.721661\n",
            "[215/400] Train Acc: 0.786375 Loss: 0.675649 | Val Acc: 0.768542 loss: 0.720785\n",
            "[216/400] Train Acc: 0.786248 Loss: 0.675354 | Val Acc: 0.769322 loss: 0.719987\n",
            "[217/400] Train Acc: 0.786411 Loss: 0.673575 | Val Acc: 0.768972 loss: 0.722351\n",
            "[218/400] Train Acc: 0.787190 Loss: 0.674003 | Val Acc: 0.770021 loss: 0.721848\n",
            "[219/400] Train Acc: 0.786872 Loss: 0.673359 | Val Acc: 0.771184 loss: 0.720613\n",
            "[220/400] Train Acc: 0.786390 Loss: 0.675047 | Val Acc: 0.771818 loss: 0.716406\n",
            "[221/400] Train Acc: 0.785742 Loss: 0.675505 | Val Acc: 0.770867 loss: 0.716239\n",
            "[222/400] Train Acc: 0.786704 Loss: 0.673122 | Val Acc: 0.768810 loss: 0.721869\n",
            "[223/400] Train Acc: 0.786386 Loss: 0.674543 | Val Acc: 0.770655 loss: 0.717187\n",
            "[224/400] Train Acc: 0.786899 Loss: 0.673498 | Val Acc: 0.770444 loss: 0.722114\n",
            "[225/400] Train Acc: 0.787132 Loss: 0.672884 | Val Acc: 0.769956 loss: 0.716872\n",
            "[226/400] Train Acc: 0.786991 Loss: 0.672843 | Val Acc: 0.768810 loss: 0.717283\n",
            "[227/400] Train Acc: 0.787318 Loss: 0.672308 | Val Acc: 0.768558 loss: 0.721677\n",
            "[228/400] Train Acc: 0.786840 Loss: 0.672865 | Val Acc: 0.769940 loss: 0.720824\n",
            "[229/400] Train Acc: 0.787218 Loss: 0.672587 | Val Acc: 0.767899 loss: 0.723190\n",
            "[230/400] Train Acc: 0.787461 Loss: 0.672496 | Val Acc: 0.770696 loss: 0.719097\n",
            "[231/400] Train Acc: 0.786799 Loss: 0.671897 | Val Acc: 0.769387 loss: 0.717700\n",
            "[232/400] Train Acc: 0.786273 Loss: 0.673870 | Val Acc: 0.770542 loss: 0.718327\n",
            "[233/400] Train Acc: 0.787177 Loss: 0.671461 | Val Acc: 0.770452 loss: 0.714278\n",
            "[234/400] Train Acc: 0.787149 Loss: 0.672297 | Val Acc: 0.769973 loss: 0.717387\n",
            "[235/400] Train Acc: 0.787440 Loss: 0.670206 | Val Acc: 0.771013 loss: 0.717218\n",
            "[236/400] Train Acc: 0.787367 Loss: 0.671357 | Val Acc: 0.768956 loss: 0.717516\n",
            "[237/400] Train Acc: 0.787369 Loss: 0.671199 | Val Acc: 0.770940 loss: 0.717582\n",
            "[238/400] Train Acc: 0.787526 Loss: 0.671542 | Val Acc: 0.769981 loss: 0.719455\n",
            "[239/400] Train Acc: 0.786748 Loss: 0.672339 | Val Acc: 0.770151 loss: 0.716828\n",
            "[240/400] Train Acc: 0.787637 Loss: 0.670999 | Val Acc: 0.769639 loss: 0.715102\n",
            "[241/400] Train Acc: 0.786843 Loss: 0.671620 | Val Acc: 0.769867 loss: 0.721945\n",
            "[242/400] Train Acc: 0.787987 Loss: 0.670214 | Val Acc: 0.769899 loss: 0.718878\n",
            "[243/400] Train Acc: 0.787735 Loss: 0.670503 | Val Acc: 0.770420 loss: 0.717562\n",
            "[244/400] Train Acc: 0.787929 Loss: 0.669581 | Val Acc: 0.770883 loss: 0.717802\n",
            "[245/400] Train Acc: 0.787562 Loss: 0.670103 | Val Acc: 0.769899 loss: 0.718094\n",
            "[246/400] Train Acc: 0.787636 Loss: 0.669382 | Val Acc: 0.769200 loss: 0.720818\n",
            "[247/400] Train Acc: 0.788302 Loss: 0.669375 | Val Acc: 0.770379 loss: 0.721521\n",
            "[248/400] Train Acc: 0.787687 Loss: 0.670447 | Val Acc: 0.769810 loss: 0.719715\n",
            "[249/400] Train Acc: 0.787963 Loss: 0.669082 | Val Acc: 0.769794 loss: 0.719232\n",
            "[250/400] Train Acc: 0.788382 Loss: 0.669709 | Val Acc: 0.769737 loss: 0.717514\n",
            "[251/400] Train Acc: 0.787890 Loss: 0.669513 | Val Acc: 0.770721 loss: 0.718160\n",
            "[252/400] Train Acc: 0.787814 Loss: 0.669458 | Val Acc: 0.770029 loss: 0.714177\n",
            "[253/400] Train Acc: 0.787882 Loss: 0.669295 | Val Acc: 0.772095 loss: 0.711369\n",
            "saving model with acc 0.772\n",
            "[254/400] Train Acc: 0.788163 Loss: 0.667612 | Val Acc: 0.770176 loss: 0.717153\n",
            "[255/400] Train Acc: 0.787867 Loss: 0.668902 | Val Acc: 0.770452 loss: 0.719557\n",
            "[256/400] Train Acc: 0.787841 Loss: 0.668735 | Val Acc: 0.769298 loss: 0.719058\n",
            "[257/400] Train Acc: 0.787924 Loss: 0.668461 | Val Acc: 0.770769 loss: 0.714275\n",
            "[258/400] Train Acc: 0.788319 Loss: 0.667963 | Val Acc: 0.770338 loss: 0.714558\n",
            "[259/400] Train Acc: 0.788246 Loss: 0.668161 | Val Acc: 0.770582 loss: 0.718917\n",
            "[260/400] Train Acc: 0.788240 Loss: 0.668390 | Val Acc: 0.769948 loss: 0.716742\n",
            "[261/400] Train Acc: 0.788529 Loss: 0.667075 | Val Acc: 0.771550 loss: 0.714005\n",
            "[262/400] Train Acc: 0.788616 Loss: 0.667093 | Val Acc: 0.771387 loss: 0.715930\n",
            "[263/400] Train Acc: 0.788591 Loss: 0.667365 | Val Acc: 0.770306 loss: 0.720039\n",
            "[264/400] Train Acc: 0.788741 Loss: 0.667401 | Val Acc: 0.770355 loss: 0.719206\n",
            "[265/400] Train Acc: 0.788316 Loss: 0.667145 | Val Acc: 0.770387 loss: 0.713390\n",
            "[266/400] Train Acc: 0.788893 Loss: 0.665112 | Val Acc: 0.769875 loss: 0.716715\n",
            "[267/400] Train Acc: 0.788568 Loss: 0.666879 | Val Acc: 0.770395 loss: 0.718389\n",
            "[268/400] Train Acc: 0.788863 Loss: 0.666827 | Val Acc: 0.769948 loss: 0.716618\n",
            "[269/400] Train Acc: 0.788853 Loss: 0.665855 | Val Acc: 0.770639 loss: 0.715437\n",
            "[270/400] Train Acc: 0.788404 Loss: 0.666985 | Val Acc: 0.769680 loss: 0.718232\n",
            "[271/400] Train Acc: 0.788321 Loss: 0.666748 | Val Acc: 0.770777 loss: 0.716582\n",
            "[272/400] Train Acc: 0.788567 Loss: 0.666567 | Val Acc: 0.771355 loss: 0.712744\n",
            "[273/400] Train Acc: 0.788718 Loss: 0.667368 | Val Acc: 0.768810 loss: 0.721102\n",
            "[274/400] Train Acc: 0.788720 Loss: 0.665351 | Val Acc: 0.769859 loss: 0.718859\n",
            "[275/400] Train Acc: 0.788641 Loss: 0.665893 | Val Acc: 0.770973 loss: 0.715548\n",
            "[276/400] Train Acc: 0.788651 Loss: 0.665820 | Val Acc: 0.770924 loss: 0.717326\n",
            "[277/400] Train Acc: 0.789369 Loss: 0.664634 | Val Acc: 0.769550 loss: 0.719139\n",
            "[278/400] Train Acc: 0.788621 Loss: 0.666252 | Val Acc: 0.770501 loss: 0.716676\n",
            "[279/400] Train Acc: 0.789079 Loss: 0.666449 | Val Acc: 0.770501 loss: 0.715632\n",
            "[280/400] Train Acc: 0.789608 Loss: 0.664017 | Val Acc: 0.770948 loss: 0.716866\n",
            "[281/400] Train Acc: 0.789240 Loss: 0.665281 | Val Acc: 0.770867 loss: 0.714153\n",
            "[282/400] Train Acc: 0.789120 Loss: 0.665874 | Val Acc: 0.770225 loss: 0.715515\n",
            "[283/400] Train Acc: 0.789149 Loss: 0.664845 | Val Acc: 0.769989 loss: 0.717019\n",
            "[284/400] Train Acc: 0.789234 Loss: 0.664585 | Val Acc: 0.769533 loss: 0.715981\n",
            "[285/400] Train Acc: 0.788763 Loss: 0.665090 | Val Acc: 0.770249 loss: 0.717934\n",
            "[286/400] Train Acc: 0.789694 Loss: 0.664460 | Val Acc: 0.771005 loss: 0.714729\n",
            "[287/400] Train Acc: 0.789237 Loss: 0.663946 | Val Acc: 0.770899 loss: 0.715247\n",
            "[288/400] Train Acc: 0.789760 Loss: 0.663810 | Val Acc: 0.769599 loss: 0.719007\n",
            "[289/400] Train Acc: 0.789249 Loss: 0.664017 | Val Acc: 0.770005 loss: 0.719166\n",
            "[290/400] Train Acc: 0.789696 Loss: 0.664604 | Val Acc: 0.770168 loss: 0.719743\n",
            "[291/400] Train Acc: 0.789490 Loss: 0.664021 | Val Acc: 0.771550 loss: 0.712251\n",
            "[292/400] Train Acc: 0.789645 Loss: 0.663910 | Val Acc: 0.770298 loss: 0.717294\n",
            "[293/400] Train Acc: 0.789178 Loss: 0.664232 | Val Acc: 0.769338 loss: 0.718154\n",
            "[294/400] Train Acc: 0.789122 Loss: 0.663398 | Val Acc: 0.771834 loss: 0.713678\n",
            "[295/400] Train Acc: 0.789090 Loss: 0.663922 | Val Acc: 0.770899 loss: 0.718003\n",
            "[296/400] Train Acc: 0.789498 Loss: 0.663474 | Val Acc: 0.771818 loss: 0.715268\n",
            "[297/400] Train Acc: 0.789522 Loss: 0.663906 | Val Acc: 0.770851 loss: 0.714743\n",
            "[298/400] Train Acc: 0.789630 Loss: 0.663359 | Val Acc: 0.771444 loss: 0.717831\n",
            "[299/400] Train Acc: 0.788880 Loss: 0.664053 | Val Acc: 0.769818 loss: 0.718028\n",
            "[300/400] Train Acc: 0.790047 Loss: 0.661724 | Val Acc: 0.770525 loss: 0.716142\n",
            "[301/400] Train Acc: 0.789461 Loss: 0.663195 | Val Acc: 0.770038 loss: 0.717020\n",
            "[302/400] Train Acc: 0.789944 Loss: 0.662554 | Val Acc: 0.770729 loss: 0.713635\n",
            "[303/400] Train Acc: 0.789668 Loss: 0.661919 | Val Acc: 0.771403 loss: 0.716225\n",
            "[304/400] Train Acc: 0.789349 Loss: 0.663850 | Val Acc: 0.770582 loss: 0.715295\n",
            "[305/400] Train Acc: 0.789906 Loss: 0.662262 | Val Acc: 0.773135 loss: 0.712835\n",
            "saving model with acc 0.773\n",
            "[306/400] Train Acc: 0.790004 Loss: 0.662128 | Val Acc: 0.770623 loss: 0.715304\n",
            "[307/400] Train Acc: 0.789660 Loss: 0.662721 | Val Acc: 0.771395 loss: 0.713038\n",
            "[308/400] Train Acc: 0.789526 Loss: 0.662009 | Val Acc: 0.770794 loss: 0.718378\n",
            "[309/400] Train Acc: 0.789813 Loss: 0.661977 | Val Acc: 0.770273 loss: 0.717596\n",
            "[310/400] Train Acc: 0.789624 Loss: 0.662952 | Val Acc: 0.769916 loss: 0.719678\n",
            "[311/400] Train Acc: 0.789392 Loss: 0.663045 | Val Acc: 0.770338 loss: 0.716302\n",
            "[312/400] Train Acc: 0.790420 Loss: 0.662709 | Val Acc: 0.770891 loss: 0.714852\n",
            "[313/400] Train Acc: 0.789980 Loss: 0.662409 | Val Acc: 0.770655 loss: 0.718297\n",
            "[314/400] Train Acc: 0.789923 Loss: 0.660817 | Val Acc: 0.770534 loss: 0.717564\n",
            "[315/400] Train Acc: 0.790771 Loss: 0.660897 | Val Acc: 0.770460 loss: 0.716708\n",
            "[316/400] Train Acc: 0.789888 Loss: 0.661878 | Val Acc: 0.770290 loss: 0.716132\n",
            "[317/400] Train Acc: 0.790162 Loss: 0.661744 | Val Acc: 0.771111 loss: 0.720969\n",
            "[318/400] Train Acc: 0.790106 Loss: 0.660965 | Val Acc: 0.769493 loss: 0.723032\n",
            "[319/400] Train Acc: 0.790258 Loss: 0.661103 | Val Acc: 0.771184 loss: 0.719222\n",
            "[320/400] Train Acc: 0.790297 Loss: 0.661168 | Val Acc: 0.771769 loss: 0.719203\n",
            "[321/400] Train Acc: 0.790554 Loss: 0.660583 | Val Acc: 0.772192 loss: 0.719483\n",
            "[322/400] Train Acc: 0.789809 Loss: 0.661311 | Val Acc: 0.770696 loss: 0.717534\n",
            "[323/400] Train Acc: 0.790402 Loss: 0.660077 | Val Acc: 0.770493 loss: 0.714502\n",
            "[324/400] Train Acc: 0.789994 Loss: 0.661203 | Val Acc: 0.771176 loss: 0.715681\n",
            "[325/400] Train Acc: 0.790725 Loss: 0.659882 | Val Acc: 0.770517 loss: 0.719689\n",
            "[326/400] Train Acc: 0.790513 Loss: 0.659918 | Val Acc: 0.771135 loss: 0.720413\n",
            "[327/400] Train Acc: 0.790182 Loss: 0.660732 | Val Acc: 0.770192 loss: 0.718787\n",
            "[328/400] Train Acc: 0.790574 Loss: 0.660446 | Val Acc: 0.769485 loss: 0.722392\n",
            "[329/400] Train Acc: 0.790697 Loss: 0.660248 | Val Acc: 0.770810 loss: 0.719156\n",
            "[330/400] Train Acc: 0.790921 Loss: 0.659803 | Val Acc: 0.769989 loss: 0.721019\n",
            "[331/400] Train Acc: 0.790285 Loss: 0.660001 | Val Acc: 0.770672 loss: 0.716636\n",
            "[332/400] Train Acc: 0.791054 Loss: 0.658482 | Val Acc: 0.769582 loss: 0.726729\n",
            "[333/400] Train Acc: 0.790475 Loss: 0.659130 | Val Acc: 0.770826 loss: 0.721984\n",
            "[334/400] Train Acc: 0.790797 Loss: 0.659350 | Val Acc: 0.770777 loss: 0.713627\n",
            "[335/400] Train Acc: 0.790677 Loss: 0.659669 | Val Acc: 0.770704 loss: 0.720360\n",
            "[336/400] Train Acc: 0.791127 Loss: 0.658794 | Val Acc: 0.771078 loss: 0.716920\n",
            "[337/400] Train Acc: 0.790840 Loss: 0.659270 | Val Acc: 0.770753 loss: 0.716944\n",
            "[338/400] Train Acc: 0.790306 Loss: 0.660714 | Val Acc: 0.770599 loss: 0.719349\n",
            "[339/400] Train Acc: 0.790306 Loss: 0.661083 | Val Acc: 0.770013 loss: 0.721431\n",
            "[340/400] Train Acc: 0.790430 Loss: 0.659534 | Val Acc: 0.771127 loss: 0.720745\n",
            "[341/400] Train Acc: 0.790546 Loss: 0.660017 | Val Acc: 0.770834 loss: 0.717218\n",
            "[342/400] Train Acc: 0.790509 Loss: 0.659417 | Val Acc: 0.771574 loss: 0.718134\n",
            "[343/400] Train Acc: 0.791435 Loss: 0.656591 | Val Acc: 0.770338 loss: 0.715054\n",
            "[344/400] Train Acc: 0.790575 Loss: 0.658736 | Val Acc: 0.771387 loss: 0.714464\n",
            "[345/400] Train Acc: 0.791084 Loss: 0.657328 | Val Acc: 0.769940 loss: 0.717580\n",
            "[346/400] Train Acc: 0.790898 Loss: 0.658878 | Val Acc: 0.770794 loss: 0.716763\n",
            "[347/400] Train Acc: 0.790855 Loss: 0.658400 | Val Acc: 0.771843 loss: 0.714407\n",
            "[348/400] Train Acc: 0.791306 Loss: 0.657681 | Val Acc: 0.770184 loss: 0.717803\n",
            "[349/400] Train Acc: 0.790764 Loss: 0.658890 | Val Acc: 0.770786 loss: 0.717768\n",
            "[350/400] Train Acc: 0.791459 Loss: 0.657937 | Val Acc: 0.769891 loss: 0.718051\n",
            "[351/400] Train Acc: 0.790709 Loss: 0.659338 | Val Acc: 0.770021 loss: 0.718692\n",
            "[352/400] Train Acc: 0.790619 Loss: 0.658938 | Val Acc: 0.769981 loss: 0.722529\n",
            "[353/400] Train Acc: 0.791269 Loss: 0.657760 | Val Acc: 0.770777 loss: 0.716192\n",
            "[354/400] Train Acc: 0.790927 Loss: 0.659411 | Val Acc: 0.770038 loss: 0.714779\n",
            "[355/400] Train Acc: 0.790968 Loss: 0.658156 | Val Acc: 0.769599 loss: 0.722048\n",
            "[356/400] Train Acc: 0.791249 Loss: 0.656997 | Val Acc: 0.770200 loss: 0.716853\n",
            "[357/400] Train Acc: 0.790817 Loss: 0.658643 | Val Acc: 0.772160 loss: 0.712729\n",
            "[358/400] Train Acc: 0.791000 Loss: 0.657683 | Val Acc: 0.770412 loss: 0.716778\n",
            "[359/400] Train Acc: 0.790892 Loss: 0.658061 | Val Acc: 0.769363 loss: 0.719775\n",
            "[360/400] Train Acc: 0.791033 Loss: 0.658296 | Val Acc: 0.770281 loss: 0.712900\n",
            "[361/400] Train Acc: 0.790972 Loss: 0.657974 | Val Acc: 0.770151 loss: 0.717022\n",
            "[362/400] Train Acc: 0.790860 Loss: 0.658085 | Val Acc: 0.772135 loss: 0.712723\n",
            "[363/400] Train Acc: 0.791153 Loss: 0.657842 | Val Acc: 0.772241 loss: 0.712771\n",
            "[364/400] Train Acc: 0.791235 Loss: 0.657374 | Val Acc: 0.770428 loss: 0.717446\n",
            "[365/400] Train Acc: 0.791365 Loss: 0.657494 | Val Acc: 0.770249 loss: 0.715194\n",
            "[366/400] Train Acc: 0.791351 Loss: 0.658167 | Val Acc: 0.770062 loss: 0.718146\n",
            "[367/400] Train Acc: 0.791183 Loss: 0.658705 | Val Acc: 0.768737 loss: 0.719849\n",
            "[368/400] Train Acc: 0.791031 Loss: 0.658128 | Val Acc: 0.771070 loss: 0.715820\n",
            "[369/400] Train Acc: 0.791334 Loss: 0.657142 | Val Acc: 0.769590 loss: 0.717194\n",
            "[370/400] Train Acc: 0.791552 Loss: 0.658123 | Val Acc: 0.770899 loss: 0.714915\n",
            "[371/400] Train Acc: 0.791550 Loss: 0.657253 | Val Acc: 0.771119 loss: 0.714464\n",
            "[372/400] Train Acc: 0.791480 Loss: 0.656754 | Val Acc: 0.770566 loss: 0.716846\n",
            "[373/400] Train Acc: 0.792062 Loss: 0.656034 | Val Acc: 0.771241 loss: 0.714486\n",
            "[374/400] Train Acc: 0.791756 Loss: 0.655915 | Val Acc: 0.770314 loss: 0.716730\n",
            "[375/400] Train Acc: 0.791763 Loss: 0.656218 | Val Acc: 0.771208 loss: 0.716668\n",
            "[376/400] Train Acc: 0.791526 Loss: 0.657187 | Val Acc: 0.770981 loss: 0.716080\n",
            "[377/400] Train Acc: 0.791319 Loss: 0.656729 | Val Acc: 0.770070 loss: 0.719675\n",
            "[378/400] Train Acc: 0.791684 Loss: 0.655911 | Val Acc: 0.770672 loss: 0.717652\n",
            "[379/400] Train Acc: 0.791407 Loss: 0.656693 | Val Acc: 0.771436 loss: 0.717027\n",
            "[380/400] Train Acc: 0.791809 Loss: 0.655600 | Val Acc: 0.771395 loss: 0.715389\n",
            "[381/400] Train Acc: 0.791602 Loss: 0.655234 | Val Acc: 0.769973 loss: 0.717213\n",
            "[382/400] Train Acc: 0.791329 Loss: 0.657274 | Val Acc: 0.770281 loss: 0.717697\n",
            "[383/400] Train Acc: 0.791280 Loss: 0.656414 | Val Acc: 0.770054 loss: 0.718524\n",
            "[384/400] Train Acc: 0.791586 Loss: 0.656327 | Val Acc: 0.771631 loss: 0.719963\n",
            "[385/400] Train Acc: 0.791360 Loss: 0.655911 | Val Acc: 0.770647 loss: 0.715705\n",
            "[386/400] Train Acc: 0.791689 Loss: 0.654850 | Val Acc: 0.769948 loss: 0.720439\n",
            "[387/400] Train Acc: 0.792030 Loss: 0.655068 | Val Acc: 0.770501 loss: 0.717944\n",
            "[388/400] Train Acc: 0.792044 Loss: 0.654414 | Val Acc: 0.771485 loss: 0.716301\n",
            "[389/400] Train Acc: 0.791539 Loss: 0.655135 | Val Acc: 0.771257 loss: 0.716918\n",
            "[390/400] Train Acc: 0.792018 Loss: 0.654064 | Val Acc: 0.770623 loss: 0.716894\n",
            "[391/400] Train Acc: 0.791633 Loss: 0.655430 | Val Acc: 0.771493 loss: 0.720924\n",
            "[392/400] Train Acc: 0.791468 Loss: 0.655630 | Val Acc: 0.771908 loss: 0.718100\n",
            "[393/400] Train Acc: 0.791666 Loss: 0.655573 | Val Acc: 0.770721 loss: 0.722953\n",
            "[394/400] Train Acc: 0.791679 Loss: 0.655807 | Val Acc: 0.770403 loss: 0.713284\n",
            "[395/400] Train Acc: 0.791997 Loss: 0.655581 | Val Acc: 0.770005 loss: 0.717982\n",
            "[396/400] Train Acc: 0.792243 Loss: 0.655261 | Val Acc: 0.770176 loss: 0.721741\n",
            "[397/400] Train Acc: 0.791922 Loss: 0.655996 | Val Acc: 0.771501 loss: 0.716600\n",
            "[398/400] Train Acc: 0.791544 Loss: 0.655651 | Val Acc: 0.771477 loss: 0.719206\n",
            "[399/400] Train Acc: 0.791484 Loss: 0.655434 | Val Acc: 0.770216 loss: 0.718197\n",
            "[400/400] Train Acc: 0.791903 Loss: 0.655377 | Val Acc: 0.771469 loss: 0.718084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTr80Panaq8"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiBILuUEncLl",
        "outputId": "3c4a61f4-e7c9-49c9-d74e-3eb6387bf6bb"
      },
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP_tUJeLnd94"
      },
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIjAsNTnhgs"
      },
      "source": [
        "# **Write prediction to a CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "athJ5R7BnlN5",
        "outputId": "66775182-2904-41f3-f55a-721b318e6591"
      },
      "source": [
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))\n",
        "\n",
        "print('Saving results to prediction.csv')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving results to prediction.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV5cHjHLos62"
      },
      "source": [
        "# **Reference**\n",
        "\n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW02/HW02-1.ipynb)"
      ]
    }
  ]
}